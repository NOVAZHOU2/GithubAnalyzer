项目名称,项目URL,提交ID,提交者,提交时间,提交信息,提交链接
redis/redis,https://github.com/redis/redis,cb71dec,Yuan Wang,2025-12-04T01:24:23Z,"Disable RDB compression when diskless replication is used (#14575)  Fixes #14538  If the master uses diskless synchronization and the replica uses diskless load, we can disable RDB compression to reduce full sync time. I tested on AWS and found we could reduce time by 20-40%.  In terms of implementation, when the replica can use diskless load, the replica will send `replconf rdb-no-compress 1` to master to deliver a RDB without compression.  If your network is slow, please disable repl-diskless-load, and maybe even repl-diskless-sync  ---------  Co-authored-by: Ozan Tezcan <ozantezcan@gmail.com>",https://github.com/redis/redis/commit/cb71dec0c3b1202f85e26cbb80b849a24c6bda02
redis/redis,https://github.com/redis/redis,08b63b6,Ozan Tezcan,2025-12-03T09:12:48Z,"Fix flaky ASM tests (#14604)  1. Fix ""Simple slot migration with write load"" by introducing artificial delay to traffic generator to slow down it for tsan builds. Failed test: https://github.com/redis/redis/actions/runs/19720942981/job/56503213650  2. Fix ""Test RM_ClusterCanAccessKeysInSlot returns false for unowned slots"" by waiting config propagation before checking it on a replica. Failed test: https://github.com/redis/redis/actions/runs/19841852142/job/56851802772",https://github.com/redis/redis/commit/08b63b6cebaf18e2df9fbc7c0801be6a03176ef9
redis/redis,https://github.com/redis/redis,3c57a8f,Ozan Tezcan,2025-12-02T10:38:22Z,"Retry an ASM import step when the source node is temporarily not ready (#14599)  The cluster implementation may be temporarily unavailable and return an error to the `ASM_EVENT_MIGRATE_PREP` event to prevent starting a new migration. Although this is most likely a transient condition, the source node has no way to distinguish it from a real error, so it must fail the import attempt and start a new one.  In Redis, failing an attempt is cheap, but in other cluster implementations it may require cleaning up resources and can cause unnecessary disruption.  This PR introduces a new `-NOTREADY` error reply for the `CLUSTER SYNCSLOTS SYNC` command. When the source replies with `-NOTREADY`, the destination can recognize the condition as transient and retry sending `CLUSTER SYNCSLOTS SYNC` step periodically instead of failing the attempt.",https://github.com/redis/redis/commit/3c57a8fc92ab4df9bdec1fbcae9c81c59f0edd90
redis/redis,https://github.com/redis/redis,86c6358,Ozan Tezcan,2025-11-29T19:41:58Z,Refactor some of ASM and slot-stats functions (#14587)  This PR does not introduce any behavioral changes.  - Refactored and moved verifyClusterConfigWithData() into cluster.c. - Refactored and centralized ASM and slot-stats initialization functions.  These changes place shared logic in a common location so it can be reused by different cluster implementations.,https://github.com/redis/redis/commit/86c63588b017b0341600b9ffbc4f2382b207906c
redis/redis,https://github.com/redis/redis,f56e011,Oran Agra,2025-11-27T05:59:11Z,"Fix rare server hang at shutdown (#14581)  In case we have to kill an rdb child at shutdown, we wait for the child process to exit, and then resume with the shutodwn, and we did not clear the child_pid variable, since we're going to terminate anyway. but if the shutdown is then aborted due to another issue further down that function, we will try to kill that child again, and the waitpid will never get released.  Reproduced in the test ""SHUTDOWN can proceed if shutdown command was with nosave""",https://github.com/redis/redis/commit/f56e0115fbce73b012fe4ccce7368a67017b911c
redis/redis,https://github.com/redis/redis,82fbf21,Oran Agra,2025-11-26T07:13:21Z,"fix test tag leakage that can result in skipping tests (#14572)  some error handling paths didn't remove the tags they added, but most importantly, if the start_server proc is given the ""tags"" argument more than once, on exit, it only removed the last one.  this problem exists in start_cluster in list.tcl, and the result was that the ""external:skip cluster modules"" were not removed",https://github.com/redis/redis/commit/82fbf213eb47d2a623decf5c31b29d35459fcd71
redis/redis,https://github.com/redis/redis,8d54243,Ozan Tezcan,2025-11-26T05:50:12Z,"Fix delayed config broadcast after ASM operations (#14573)  An incorrect constant value prevented the configuration from being broadcast immediately. As a result, the config was only broadcast later as part of periodic ping messages, causing other nodes to learn about the configuration change with a delay.  Introduced by https://github.com/redis/redis/pull/14504.",https://github.com/redis/redis/commit/8d542434bd27f935db76258732b14f774e8096af
redis/redis,https://github.com/redis/redis,c8e1e83,Rushabh Mehta,2025-11-25T12:05:33Z,"Validate existence of backup directory for cluster backup in redis-cli (#14448)  Currently, there is a `TODO` regarding adding a check if the backup directory exists in `redis-cli`.  This PR adds a check to improve usability:  - If the backup directory does not exist, the user is informed with an error message. - If the specified path exists but is not a directory, an error is now properly reported.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/c8e1e833cdab067104d7670a6a53c365ce1d2652
redis/redis,https://github.com/redis/redis,3920059,RoyBenMoshe,2025-11-25T07:30:43Z,"SCAN: restore original filter order (#14537)  In #14121, the SCAN filters order was changed, before #14121￼the order was - pattern, expiration and type, after #14121￼pattern became last, this break change broke the original behavior, which will cause scan with pattern also to remove the expired keys. This PR reorders the filters to be consistent with the original behavior and extends a test to cover this scenario.",https://github.com/redis/redis/commit/39200596f48e44067021474adeefeeccf3dc3a13
redis/redis,https://github.com/redis/redis,0288d70,lihp,2025-11-22T03:52:31Z,"Fixes an issue where EXEC checks ACL during AOF loading (#14545)  This PR fixes an issue(#14541) where EXEC’s ACL recheck was still being performed during AOF loading, that may cause AOF loading failed, if ACL rules are changed and don't allow some commands in MULTI-EXEC.",https://github.com/redis/redis/commit/0288d70820095491c0cf7d9d89a7e6062a6418f2
redis/redis,https://github.com/redis/redis,5ee05d8,Ozan Tezcan,2025-11-22T00:38:29Z,"Broadcast config change immediately to the other nodes in cluster (#14504)  Broadcast configuration changes immediately to the other cluster nodes. This ensures that all nodes are aware of configuration updates and the node’s latest epoch immediately, preventing subsequent messages from being incorrectly rejected as stale.  This issue was observed in an Atomic Slot Migration (ASM) scenario: - Node B joins the cluster, but there is a config epoch collision with Node A. - Node A increments its epoch but has not yet broadcast the new configuration. - Meanwhile, Node B starts an import operation. When it finishes, Node B bumps its epoch and broadcasts the new configuration immediately. - However, since Node A already has a higher epoch, it ignores Node B’s update, causing the import operation to fail.",https://github.com/redis/redis/commit/5ee05d8de489fadb30a75f46c8e931888d6c5401
redis/redis,https://github.com/redis/redis,bb6389e,debing.sun,2025-11-21T14:37:17Z,"Fix min_cgroup_last_id cache not updated when destroying consumer group (#14552)  ## Problem  When destroying a consumer group with `XGROUP DESTROY`, the cached `min_cgroup_last_id` was not being invalidated. This caused incorrect behavior when using `XDELEX` with the `ACKED` option, as the cache still referenced the destroyed group's `last_id`.  ## Solution  Invalidate the `min_cgroup_last_id` cache when the destroyed group's `last_id` equals the cached minimum. The cache will be recalculated on the next call to `streamEntryIsReferenced()`.  ---------  Co-authored-by: guybe7 <guy.benoish@redislabs.com>",https://github.com/redis/redis/commit/bb6389e8237723b2c88325a4c4b7853a48db2da2
redis/redis,https://github.com/redis/redis,1102415,Slavomir Kaslev,2025-11-21T10:28:26Z,"Avoid allocation when iterating over hashes, lists, sets and kvstores (#14473)  The PR is follow up on #14200 where we prefer storing iterators on the stack rather than allocating on the heap. Here we continue this for iterators over hashes, lists, sets and kvstores.  Quicklist's iterators are still using heap allocation and will be addressed soon. The reason is that `NULL` is perfectly valid quicklist iterator value and handling this would be better reviewed separately from the mostly mechanical changes here.",https://github.com/redis/redis/commit/1102415f46a8ee95219e78308442ed1243011baa
redis/redis,https://github.com/redis/redis,b632e9d,Ozan Tezcan,2025-11-21T09:18:28Z,Fix flaky ASM write load test (#14551)  Extend write pause timeout to stabilize ASM write load test under TSAN.  Failing test for reference: https://github.com/redis/redis/actions/runs/19520561209/job/55882882951,https://github.com/redis/redis/commit/b632e9df6a46874911242329696b5070cefe2f5e
redis/redis,https://github.com/redis/redis,7a3cb3b,Yuan Wang,2025-11-19T09:10:57Z,"Fix CI flaky tests (#14531)  - https://github.com/redis/redis/actions/runs/19200504999/job/54887625884    avoid calling `start_write_load` before pausing the destination node  - https://github.com/redis/redis/actions/runs/18958533020/job/54140746904 maybe the replica did not sync with master, then the replica did not update the counter",https://github.com/redis/redis/commit/7a3cb3b4b37419b84fa13c51bbaba3f8bea44838
redis/redis,https://github.com/redis/redis,837b14c,Mincho Paskalev,2025-11-19T08:56:18Z,Fix ASan Daily (#14527)  After https://github.com/redis/redis/pull/14226 module tests started running with ASan enabled.  `auth.c` blocks the user on auth and spawns a thread that sleeps for 0.5s before unblocking the client and returning.  A tcl tests unloads the module which may happen just after the spawned thread unblocks the client. In that case if the unloading finishes fast enough the spawned thread may try to execute code from the module's dynamic library that is already unloaded resulting in sefault.  Fix: just wait on the thread during module's OnUnload method.,https://github.com/redis/redis/commit/837b14c89a4d3b48ac04e3b7154eeae4af21414b
redis/redis,https://github.com/redis/redis,42f3675,alonre24,2025-11-17T18:48:32Z,Update RediSearch to v8.4.2 (#14542)  [8.4] Validate CLUSTER SHARDS Reply - [MOD-12432] (https://github.com/RediSearch/RediSearch/commit/b6301f5c7b02cca98dfed000878c39d6a4833406) [8.4] Fix TLS config check - [MOD-12408] (https://github.com/RediSearch/RediSearch/commit/6606e8112bb34265c8bafd6575ff77a67b612d80)  [MOD-12432]: https://redislabs.atlassian.net/browse/MOD-12432?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ [MOD-12408]: https://redislabs.atlassian.net/browse/MOD-12408?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ,https://github.com/redis/redis/commit/42f36755ed1a00df9c584cbdc57b9ecddfe4f8bd
redis/redis,https://github.com/redis/redis,6d3e1fd,alonre24,2025-11-13T08:16:53Z,Bump Redisearch module version to v8.4.1 (#14536)  Fix FT.CREATE failure with LeanVec parameters on non-Intel architectures(https://github.com/RediSearch/RediSearch/commit/4b9dceb2356758a1fa156edf392c970165f43181),https://github.com/redis/redis/commit/6d3e1fd4acd5455f0517fec8996ec25b5532ae1c
redis/redis,https://github.com/redis/redis,0a6eacf,Oran Agra,2025-11-12T09:36:10Z,"Add variable key-spec flags to SET IF* and DELEX (#14529)  These commands behave as DEL and SET (blindly Remove or Overwrite) when they don't get IF* flags, and require the value of the key when they do run with these flags.  Making sure they have the VARIABLE_FLAGS flag, and getKeysProc that can provide the right flags depending on the arguments used. (the plain flags when arguments are unknown are the common denominator ones)  Move lookupKey call in DELEX to avoid double lookup, which also means (some, namely arity) syntax errors are checked (and reported) before checking the existence of the key.",https://github.com/redis/redis/commit/0a6eacff1fb6184b5e026c8b3c45b785832fac37
redis/redis,https://github.com/redis/redis,fafbf53,alonre24,2025-11-12T08:30:27Z,Update RediSearch version to 8.4.0 (8.4 GA) (#14530),https://github.com/redis/redis/commit/fafbf53e98a220e5b3f5c92c9e9b8c4d00ae833e
redis/redis,https://github.com/redis/redis,a760d17,Slavomir Kaslev,2025-11-12T08:02:23Z,Fix FreeBSD daily runs (#14532)  Daily CI runs on FreeBSD started failing due to package rename tclx -> tclX. Also fix macro redefinition warning on FreeBSD while here.,https://github.com/redis/redis/commit/a760d1769a0f9797e1ff85c39da95d9e7ce77c6e
redis/redis,https://github.com/redis/redis,52942ee,Slavomir Kaslev,2025-11-11T15:43:22Z,Document per slot memory tracking config behavior in redis.conf (#14526)  Per slot memory statistics are collected only if 'cluster-slot-stats-enabled' is enabled on strartup. Document this behavior in 'redis.conf'.  ---------  Co-authored-by: Oran Agra <oran@redislabs.com>,https://github.com/redis/redis/commit/52942eebeb48bd316ac7c599311e3602c1fdfde2
redis/redis,https://github.com/redis/redis,90ba7ba,Sergei Georgiev,2025-11-11T11:05:22Z,"Fix XREADGROUP CLAIM to return delivery metadata as integers (#14524)  ### Problem The XREADGROUP command with CLAIM parameter incorrectly returns delivery metadata (idle time and delivery count) as strings instead of integers, contradicting the Redis specification.  ### Solution Updated the XREADGROUP CLAIM implementation to return delivery metadata fields as integers, aligning with the documented specification and maintaining consistency with Redis response conventions.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/90ba7ba4dc92c9b4edd69a0d68b4561c0324fb99
redis/redis,https://github.com/redis/redis,d25e582,Moti Cohen,2025-11-10T15:15:37Z,Fix flaky test of hfe persist rdb reload (#14525)  So far occured once on daily in the test-sanitizer-address job,https://github.com/redis/redis/commit/d25e582a17989634d02e8490ebf0ed1ab79465dc
redis/redis,https://github.com/redis/redis,1fddc44,Tom Gabsow,2025-11-10T12:39:38Z,update modules to 8.4.0 (#14519),https://github.com/redis/redis/commit/1fddc44c6150b42eaf8ade02f31fd197067a1b04
redis/redis,https://github.com/redis/redis,189b760,Moti Cohen,2025-11-09T07:49:54Z,Add hfe rdb load test (#14511)  Verify that following RDB load fields keep their expiration time. Verify that hashes that had HFEs not counted following rdb load in subexpiry (by command `info keyspace`),https://github.com/redis/redis/commit/189b7609f53efab454b44fe1a5d883bd614a0c09
redis/redis,https://github.com/redis/redis,a64e725,Filipe Oliveira (Redis),2025-11-07T11:55:30Z,"Introduce copy-free version streamPropagateXCLAIM to reduce allocation overhead in stream propagation (#14516)  As seen in the following flamegraph, even after PR #14480, there a lot of redundant work when propagating multiple XCLAIMs withing a XREADGROUP.  This PR refactors streamPropagateXCLAIM to add a new static inline variant, `streamPropagateXCLAIMCopyFree()`, which accepts pre-created `robj*` arguments. This enables reusing argument objects across multiple XCLAIM propagations, reducing repeated creation and destruction costs during high-throughput consumer group operations.",https://github.com/redis/redis/commit/a64e725034a5c91a7ed31cd47c1bc858d0db686e
redis/redis,https://github.com/redis/redis,dc94d36,Yuan Wang,2025-11-07T07:35:23Z,"Add CLUSTER MIGRATION subcommand in CLUSTER HELP (#14517)  I tried to use `cluster help` to learn how to migrate a slot, but I found that we missed adding the `cluster migration` subcommand to `cluster help`.",https://github.com/redis/redis/commit/dc94d362952cf14a9a5c819f2793e4959ddc3a16
redis/redis,https://github.com/redis/redis,b9ad4f6,Filipe Oliveira (Redis),2025-11-07T00:26:22Z,"Optimize stream ID comparison and endian conversion hot paths (#14480)  The added logic from https://github.com/redis/redis/pull/14402 introduced overhead to the XREADGROUP even when the added feature is not used.  This PR tries to mitigate it, by removing unnecessary streamEncodeID() calls and redundant byte-swapping operations from the stream iterator hot path. By comparing stream IDs directly in native-endian form, we eliminate repeated encoding and memcmp() calls that were responsible for a significant portion of total CPU time during stream iteration.  Additionally, endian conversion helpers are modernized to leverage compiler-provided intrinsics (__builtin_bswap*) for single-instruction byte-swaps on supported compilers.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/b9ad4f6782670faf5422fcf97d8e58f1e1f2fa70
redis/redis,https://github.com/redis/redis,4f9bab7,Slavomir Kaslev,2025-11-06T12:05:03Z,Revert getKeySlot() changes from #14039 (#14510)  Pull request #14039 introduced `CLUSTER SLOT-STATS` command based on valkey's implementation but unintentionally picked up unnecessary changes to getKeySlot() caching behavior.  Reported-by: ran.tidhar@redis.com,https://github.com/redis/redis/commit/4f9bab773c631ef75bb86ece21af6ac28a0abba7
redis/redis,https://github.com/redis/redis,7f1bafc,debing.sun,2025-09-14T11:32:46Z,"Fix XACKDEL stack overflow when IDs exceed STREAMID_STATIC_VECTOR_LEN (CVE-2025-62507)  This issue was introduced by redis/redis#14130. The problem is that when the number of IDs exceeds STREAMID_STATIC_VECTOR_LEN (8), the code forgot to reallocate memory for the IDs array, which causes a stack overflow.",https://github.com/redis/redis/commit/7f1bafc9225f210317070d8716756032a53f6d20
redis/redis,https://github.com/redis/redis,3e2003e,sggeorgiev,2025-08-24T18:20:52Z,"Fix HGETEX out-of-bounds read when FIELDS option missing numfields argument  When the HGETEX command is used with the FIELDS option but without the required numfields argument, the server would attempt to access an out-of-bounds argv index.  This PR adds a check to ensure numfields is present before accessing it, returning an error if it is missing. Also includes a test case to cover this scenario.",https://github.com/redis/redis/commit/3e2003ee0f7c8ce78a9a87b3483143f8d6784d4d
redis/redis,https://github.com/redis/redis,32497c0,debing.sun,2025-08-13T08:09:44Z,"Fix MurmurHash64A overflow in HyperLogLog with 2GB+ entries  The MurmurHash64A function in hyperloglog.c used an int parameter for length, causing integer overflow when processing PFADD entries larger than 2GB. This could lead to server crashes.  Changed the len parameter from int to size_t to properly handle large inputs up to SIZE_MAX in HyperLogLog operations. Refer to the implementation in facebook/mcrouter@2dbee3d/mcrouter/lib/fbi/hash.c#L54",https://github.com/redis/redis/commit/32497c0a5ffa0fa2618222e566c180a0b4c807ca
redis/redis,https://github.com/redis/redis,d4307af,Ozan Tezcan,2025-11-05T06:12:54Z,Fix schema of CLUSTER MIGRATION command (#14509)  CLUSTER MIGRATION STATUS accepts one of ID <task_id> or ALL arguments.,https://github.com/redis/redis/commit/d4307af631686c3c9a77036f55481409306a3b9b
redis/redis,https://github.com/redis/redis,e436a0e,debing.sun,2025-11-03T08:59:50Z,"Enforce 16-char hex digest length and case-insensitive comparison for IFDEQ/IFDNE (#14502)  Fix https://github.com/redis/redis/issues/14496  This PR makes the following changes: - DIGEST: Always return 16 hex characters with leading zeros   Example: ""00006c38adf31777"" instead of ""6c38adf31777""  - IFDEQ/IFDNE: Validate the digest must be exactly 16 characters  - IFDEQ/IFDNE: Use strcasecmp for case-insensitive hex comparison   Both uppercase and lowercase hex digits now work identically  ---------  Co-authored-by: Marc Gravell <marc.gravell@gmail.com> Co-authored-by: Yuan Wang <yuan.wang@redis.com>",https://github.com/redis/redis/commit/e436a0e54849e91cba8c2e2aadd14e8e46b6b703
redis/redis,https://github.com/redis/redis,82a2968,Slavomir Kaslev,2025-11-02T06:43:46Z,Fix cluster_asm.c use of cluster_legacy API (#14497)  Fix few spots in `cluster_asm.c` using cluster_legacy API. This is a problem for builds where cluster_legacy is not linked in.  ---------  Co-authored-by: Ozan Tezcan <ozantezcan@gmail.com>,https://github.com/redis/redis/commit/82a296893f3d8219db9a84734d8214d80e1200b8
redis/redis,https://github.com/redis/redis,bf0ba1d,Yuan Wang,2025-11-01T12:49:46Z,"Fix loading old version node.conf without shard_id (#14495)  - We will generate a random shard id when creating a cluster node, so `auxFieldHandlers[af_shard_id].isPresent(n) == 0` never meet, so it means we never add master nodes into `cluster.shards` when loading, this bug is introduced in https://github.com/redis/redis/pull/10536 that supports `shard-id` concept. BTW, #13468 can make replicas add into `cluster.shards`  - Replica shard_id may be different with master, so before we add again, we should remove it, otherwise, `cluster.shards` will have dirty shards, introduced in #13468  These bugs causes the output of the `cluster shards` is corrupt, the temporary `slot_info` may not be cleaned up, we may see the duplicated slot ranges, and a shard info without master.",https://github.com/redis/redis/commit/bf0ba1dcc488cec5d318dac70f50ab417c854698
redis/redis,https://github.com/redis/redis,b5f1061,Alexey Weyl,2025-10-31T06:03:40Z,"Add rdb_consecutive_write_failures metric (#14493)  Currently, it is difficult to determine whether an RDB persistence failure is caused by a persistent issue in the process or by a temporary error. Tracking the number of consecutive RDB persistence failures would help clarify this distinction. Multiple failures in a row could indicate an ongoing problem, while isolated cases may point to transient or environmental issues.",https://github.com/redis/redis/commit/b5f106110a53ed86925323ca5d8686b1ce8e0e70
redis/redis,https://github.com/redis/redis,88cbd3e,Slavomir Kaslev,2025-10-30T07:46:45Z,Fix outdated raxNewWithMetadata() comment (#14483)  Fix `raxNewWithMetadata()`'s comment that is out of date with the code.,https://github.com/redis/redis/commit/88cbd3e0889bfc3bc7b90aa8f63272d9f763202e
redis/redis,https://github.com/redis/redis,3bb9fc7,Ozan Tezcan,2025-06-23T10:33:00Z,Lua script may lead to integer overflow and potential RCE (CVE-2025-46817),https://github.com/redis/redis/commit/3bb9fc73089b3c13ae37f26cadeeb9ea973c36be
redis/redis,https://github.com/redis/redis,671953d,Ozan Tezcan,2025-06-23T09:11:31Z,LUA out-of-bound read (CVE-2025-46819),https://github.com/redis/redis/commit/671953d021608c54315c3bfb600c9b6fdc9fcd3d
redis/redis,https://github.com/redis/redis,a3ae6cb,Ozan Tezcan,2025-06-23T09:10:12Z,Lua script can be executed in the context of another user (CVE-2025-46818),https://github.com/redis/redis/commit/a3ae6cb0d6e7acf9b5f4c9a1f0943913140d7527
redis/redis,https://github.com/redis/redis,e304356,Mincho Paskalev,2025-06-23T08:41:37Z,Lua script may lead to remote code execution (CVE-2025-49844),https://github.com/redis/redis/commit/e304356f597ae959abe71ff0d4e68d9ede6375fc
redis/redis,https://github.com/redis/redis,060c690,Slavomir Kaslev,2025-10-29T06:00:15Z,"Refine condition when CLUSTER SLOT-STATS reports memory-bytes field (#14481)  Make CLUSTER SLOT-STATS command report memory-bytes if accounting is turned on disregarding cluster-slot-stats-enabled.  There is currently slight disparity between cluster-slot-stats-enabled and whether per slot memory accounting is on:    Per slot memory accounting is turned on only iff cluster-slot-stats-enabled   was enabled on startup.  This PR reconciles this in CLUSTER SLOT-STATS output.  New behavior enables scenario where if one wants per slot memory accounting but no other per slot stats, then redis can start with cluster-slot-stats-enabled and then have it turned off through CONFIG SET avoiding stats collection overhead other than memory-bytes per slot.",https://github.com/redis/redis/commit/060c6901a3e2fa549424848515a30e959e0de872
redis/redis,https://github.com/redis/redis,3b74c5c,Slavomir Kaslev,2025-10-28T15:08:12Z,"Fix list commands per slot memory accounting condition (#14479)  Fix list commands per slot memory accounting condition introduced in #14451.  While `cluster-slot-stats-enabled` config can be dynamically changed, currently we decide on startup whether to collect per slot memory stats (based on its initial value) and stick with this choice disregarding future config changes.  The reason behind this behavior is that we want to avoid situation where we would need to catch up or iterate over slots.",https://github.com/redis/redis/commit/3b74c5ce023fc73d5004bed9981e87a62615ec5d
redis/redis,https://github.com/redis/redis,6ea4e2c,debing.sun,2025-10-28T13:35:03Z,"Fix race condition for lookupCommand (#14466)  Fixes data race where main thread modifies pauserehash in dictNext while IO thread reads `useStoredKeyApi()` in `lookupCommand()->dictFind()` path. ASAN detected overlapping memory access at same byte offset causing race condition. When bit fields are adjacent in a struct, modifying one bit field requires a read-modify-write operation on the entire memory unit, which can cause race conditions with concurrent access to other bit fields in the same unit. This was introduced by https://github.com/redis/redis/pull/13696, although it was changed by https://github.com/redis/redis/pull/14440, but this issue still exist.  The fix moves `useStoredKeyApi` to share a memory word with `pauseAutoResize` instead. Since `pauseAutoResize` is never modified for `server.commands`, this eliminates the race condition while maintaining memory efficiency through bit field packing.  Reproduce step: ``` make SANITIZER=thread ./runtest --tsan --config io-threads 4 --accurate --verbose --dump-logs --single unit/obuf-limits --loop --stop ```  failed CI: https://github.com/redis/redis/actions/runs/18803219305/job/53653572710  ---------  Co-authored-by: Moti Cohen <moti.cohen@redis.com>",https://github.com/redis/redis/commit/6ea4e2c949c7fa50b85ff59647ee5e3f95e97037
redis/redis,https://github.com/redis/redis,91b5808,Mincho Paskalev,2025-10-28T11:40:44Z,"Fix numeric config boundry check (#14286)  Revert a breaking change introduced in #14051 described in this comment https://github.com/redis/redis/pull/14051#discussion_r2281765769  The non-negative check inside `checkNumericBoundaries` was ignoring that passing a big unsigned long long value (> 2^63-1) will be passed as negative value and will never reach the lower/upper boundary check.  The check is removed, reverting the breaking change.  This allows for RedisModule_ConfigSetNumeric to pass big unsigned number, albeit via `long long` parameter. Added comments about this behaviour.  Added tests for https://github.com/redis/redis/pull/14051#discussion_r2281765769",https://github.com/redis/redis/commit/91b5808fd6c5e697de276a67f360b454d551347e
redis/redis,https://github.com/redis/redis,0cbdc8e,Yuan Wang,2025-10-28T07:13:16Z,"Fix flaky tests for ASM (#14478)  `mem_cluster_slot_migration_output_buffer` and `mem_cluster_slot_migration_input_buffer` is transient, it will be reset on disconnect when ASM task is failed or done. Actually for these conditions, we just want to verify the metrics are accessible.  Failed CI job: https://github.com/redis/redis/actions/runs/18859697064/job/53815311358",https://github.com/redis/redis/commit/0cbdc8eb37f4b6f9a683d40c8266eef00febbfc6
redis/redis,https://github.com/redis/redis,f74b04f,Slavomir Kaslev,2025-10-27T11:16:54Z,"Build with -DREDIS_TEST in PR/push CI sanitizer run (#14471)  Also fix test ""SLOT-ALLOCSIZE - Test DEBUG ALLOCSIZE-SLOTS-ASSERT command"" which needs `cluster-enabled` and `cluster-slot-stats-enabled` configs set to be effective.",https://github.com/redis/redis/commit/f74b04f563bf43bce8020bc1fe8ae9ca65d6ac69
redis/redis,https://github.com/redis/redis,a14ac5e,alonre24,2025-10-27T09:56:03Z,"Update RediSearch to 8.4 RC1 (v8.3.90) (#14467)  (https://github.com/RediSearch/RediSearch/pull/7076, https://github.com/RediSearch/RediSearch/pull/6857) - Introducing `FT.HYBRID`, a new command that enables hybrid queries combining both text and vector search, with support for **RRF** and **LINEAR** result combination. This update enhances performance and reliability through a more efficient networking layer, smoother query execution, and improved overall stability. (https://github.com/RediSearch/RediSearch/pull/7065) - Add `search-default-scorer` configuration to set the default text scorer across queries (by default it is BM25). https://github.com/RediSearch/RediSearch/pull/7022 - Handle Atomic Slot Migration events upon moving slots from one node to another in a cluster mode. (https://github.com/RediSearch/RediSearch/pull/6769, https://github.com/RediSearch/RediSearch/pull/6828, https://github.com/RediSearch/RediSearch/pull/6877, https://github.com/RediSearch/RediSearch/pull/6921) - Introduce query time memory guardrails by adding a new `search-on-oom` configuration which defines the query engine behavior when OOM (Out Of Memory) is reached. OOM checks are applied to `FT.SEARCH`, `FT.AGGREGATE`, and `FT.HYBRID` commands. The behavior on OOM can be configured to one of three modes: `IGNORE`, `FAIL`, or `RETURN`. `IGNORE` - The default behavior, run queries anyway (not recommended for heavy queries returning a large result set). `FAIL` - Fail query execution immediately if any of the nodes are in OOM state when query execution starts. `RETURN` - A best effort appraoch to return partial results when OOM is detected in only some of the nodes in a cluster mode.",https://github.com/redis/redis/commit/a14ac5e731b0bed695a3a78493b56cf3953148cf
redis/redis,https://github.com/redis/redis,815276f,Tom Gabsow,2025-10-27T09:54:59Z,Update Json version 8.3.90 (#14453)  8.4 RC1,https://github.com/redis/redis/commit/815276f945275c618a526d3b51ef4f5a2680d300
redis/redis,https://github.com/redis/redis,0286257,Tom Gabsow,2025-10-27T09:54:35Z,Update RedisBloom module version to v8.3.90 (#14454),https://github.com/redis/redis/commit/0286257e593dd5aaba02a649711d6534c1657433
redis/redis,https://github.com/redis/redis,efb2aa2,Tom Gabsow,2025-10-27T09:54:13Z,Update time series version to v8.3.90 (#14468),https://github.com/redis/redis/commit/efb2aa2cb22188e7942b589081425d432b60666c
redis/redis,https://github.com/redis/redis,379fec1,debing.sun,2025-10-27T09:20:29Z,"Use fixed position keys parameter for MSETEX command (#14470)  In PR https://github.com/redis/redis/pull/14434, we made the keys parameter flexible, meaning it could appear anywhere among the command arguments. However, this also made key parsing more complex, since we could no longer determine the fixed position of key arguments. Therefore, in this PR, we reverted it back to using fixed positions for the keys.  And also fix this [comment](https://github.com/redis/redis/pull/14434#discussion_r2459282563).  ---------  Co-authored-by: Yuan Wang <yuan.wang@redis.com>",https://github.com/redis/redis/commit/379fec1426cf2711a3b327cdf2921f1461f56df0
redis/redis,https://github.com/redis/redis,f2e2dff,Yuan Wang,2025-10-25T13:14:15Z,"Fix estoreEmpty not clearing buckets sizes (#14465)  `TEST(""Empty estore"")` can not find this bug because we don't create buckets_sizes if `num_buckets_bits` == 1, two bugs make us not find issues.  ```     TEST(""Empty estore"") {         estore *es = estoreCreate(&testEbucketsType, 1); /* 2 buckets */         ...         assert(estoreGetNextNonEmptyBucket(es, 0) == -1); ``` this bug may cause active expiration cycle for hash-fields wrongly searches bucket in cluster mode after we execute `flushall` or `empty db on replicas`.  introduced in #14294",https://github.com/redis/redis/commit/f2e2dff563e707622eda43a3d5d22f52022d8909
redis/redis,https://github.com/redis/redis,414758d,Mincho Paskalev,2025-10-25T12:32:35Z,Guard calls to vectorDistanceFloat SIMD calls behind ifdefs (#14458),https://github.com/redis/redis/commit/414758dd9822c8a122667f306812d7e1474a063e
redis/redis,https://github.com/redis/redis,6a145b2,Slavomir Kaslev,2025-10-25T12:17:08Z,Add per slot memory accounting (#14451),https://github.com/redis/redis/commit/6a145b2bc914ba742eaab17adae7ca85721348fa
redis/redis,https://github.com/redis/redis,70861be,Yuan Wang,2025-10-25T01:00:33Z,Fix daily CI for atomic slot migration (#14459)  Mainly fix the usage of `start_write_load`.,https://github.com/redis/redis/commit/70861be389247fb1696435767994a21960806f2c
redis/redis,https://github.com/redis/redis,078019c,Mincho Paskalev,2025-10-24T11:38:30Z,"Fix flags of DELEX command (#14452)  In #14435 the `RM` flag  was incorrect for the DELEX command as the Redis command code accesses and uses the value of the key, it needs to be RW (just like `GETDEL` command).",https://github.com/redis/redis/commit/078019cd849417e1b3e9dda7eba7865420f8193f
redis/redis,https://github.com/redis/redis,4b6935b,meiravgri,2025-10-24T10:18:43Z,"Fix re-entrant deadlock in bugReportStart() when called from signal handlers (#14457)  When a crash occurs during `bugReportStart()` execution, the signal handler re-enters the function on the same thread. The signal handler calls `bugReportStart()` again on the same thread, causing a **mutex deadlock**.  ## Solution This PR implements a two-part fix:  1. **Use PTHREAD_MUTEX_RECURSIVE**: Changed `bug_report_start_mutex` to use `PTHREAD_MUTEX_RECURSIVE` type, which allows the same thread to re-acquire the lock multiple times. This prevents deadlock when a signal handler interrupts code that's already holding the lock.  2. **Set flag before printing**: Moved the `bug_report_start = 1` assignment **before** calling `serverLogRaw()` to prevent infinite recursion if a crash occurs during printing.",https://github.com/redis/redis/commit/4b6935b5f8f68071033da38579de11f0ed60c315
redis/redis,https://github.com/redis/redis,d43bf1d,Yuan Wang,2025-10-24T09:07:53Z,Skip CRC check for slot importing (#14456)  - skip CRC check for slot importing - verify the destination node is known and is a master when asking slot sync  ---------  Co-authored-by: Ozan Tezcan <ozantezcan@gmail.com>,https://github.com/redis/redis/commit/d43bf1d6e26f5320dd881a25e68e543b410deb1e
redis/redis,https://github.com/redis/redis,976041c,Martin Dimitrov,2025-10-24T04:46:18Z,"Vectorize vectors_distance_float in vector-sets (#14222)  Conducted tests on IceLake server with LAION 512 1M vectors dataset. We can see in the perf profile, that function ‘vector_distance_float’ is consuming majority of the computation time (this profile is captured during upload). Vectorizing this function, leads to about 2.38X speedup in upload time and 1.45X speedup in RPS during search.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com> Co-authored-by: Omer Shadmi <76992134+oshadmi@users.noreply.github.com>",https://github.com/redis/redis/commit/976041cbebd36696c441b0dcd22b9e26393706de
redis/redis,https://github.com/redis/redis,52ea47b,Stav-Levi,2025-10-23T16:12:02Z,"Add MSETEX command  (#14434)  Introduce a new command MSETEX to set multiple string keys with a shared expiration in a single atomic operation. Also with flexible argument parsing.  Syntax: MSETEX KEYS numkeys key value [key value …] [XX | NX] [EX seconds | PX milliseconds | EXAT unix-time-seconds | PXAT unix-time-milliseconds | KEEPTTL]  Sets the given keys to their respective values. This command is an extension of the MSETNX that adds expiration and XX options.  Options:  EX seconds - Set the specified expiration time, in seconds PX milliseconds - Set the specified expiration time, in milliseconds EXAT timestamp-seconds - Set the specified Unix time at which the keys will expire, in seconds PXAT timestamp-milliseconds - Set the specified Unix time at which the keys will expire, in milliseconds KEEPTTL - Retain the time to live associated with the keys XX - Only set the keys and their expiration if all already exist NX - Only set the keys and their expiration if none exist  Flexible Argument Parsing examples:   - MSETEX EX 10 KEYS 2 k1 v1 k2 v2   - MSETEX KEYS 2 k1 v1 k2 v2 NX PX 5000   - MSETEX NX EX 10 KEYS 2 k1 v1 k2 v2    Return Values: Integer reply: 1 - All keys were set successfully Integer reply: 0 - No keys were set (due to NX/XX conditions) Error reply - Syntax error or invalid arguments",https://github.com/redis/redis/commit/52ea47b792be0a6b381a284b3b73d717759e466e
redis/redis,https://github.com/redis/redis,b043ac5,GuyAv46,2025-10-23T12:44:01Z,"Fix RedisModule_GetClusterNodeInfo API (#14443)  `RedisModule_GetClusterNodeInfo` is [documented](https://redis.io/docs/latest/develop/reference/modules/modules-api-ref/#redismodule_getclusternodeslist) to be used with `RedisModule_GetClusterNodesList`, which states (and does) that the returned strings are not null-terminated.  Therefore, it is unsafe to call `strlen` on the `const char *id` input of `RedisModule_GetClusterNodeInfo`, and the API should assume the string is of the correct length",https://github.com/redis/redis/commit/b043ac5666902415f36dbab7b75ce748c6ae65a7
redis/redis,https://github.com/redis/redis,d4f2bf2,debing.sun,2025-10-23T12:42:11Z,"Disable io-threads for `Pending command pool expansion and shrinking` test (#14449)  We only enable the pending command pool when disabling iothreads, so make this test only runs without iothreads.",https://github.com/redis/redis/commit/d4f2bf2597844e3725453ae91d161bcb0da4f598
redis/redis,https://github.com/redis/redis,04f738f,Slavomir Kaslev,2025-10-23T09:36:29Z,"Add per key memory accounting (#14363)  Add O(1) memory accounting per key for all robj types.  This is part 1 of adding slot level memory accounting #10472 (discussed in #10411 and #14119) for redis.  Currently the tracked allocation sizes are used in the `MEMORY USAGE` command and replace previous approximation in `kvobjComputeSize()`. In part 2, this is extended to provide per slot memory accounting.  Note that we're accounting for usable memory size (not requested memory size). This is consistent with current `MEMORY USAGE` semantics and is also provides more realistic metric than requested memory size.  Since asking for usable size of an allocation with `zmalloc_usable_size()` is not free, care is taken to avoid it when possible and rely on usable sizes reported from `zmalloc_usable()`/`zmalloc_free_usable()` et al.  For efficiency reasons for listpacks and small strings we currently track requested memory. Since listpacks and small strings don't store allocation size, we need `zmalloc_usable_size()` to get it and in previous iterations of this PR the added overhead for some commands was around 2-3%.  Quicklists, skiplists and streams track allocated size in `alloc_size` field costing additional 8 bytes per instance.  Rax users that want memory accounting can provide `size_t *` pointer where this should be done on rax creation.  Since dicts are generic data structures where most uses don't care about memory accounting, having additional 8 bytes per instance would be overkill and instead allocated size is tracked in metadata only where it's necessary (currently only for hashes and sets with hashtable encoding).  Dict's allocation size is easily approximated by `dictMemUsage()` and the metadata `alloc_size` accumulates allocation sizes of associated keys/values only.  Correctness is enforced in unit tests that compare O(1) accounted allocations sizes with the result of O(n) memory accounted version (e.g. `_rax_verify_alloc_size()`, `_ql_verify_alloc_size()`). Additionally for REDIS_TEST builds correct `alloc_size` is asserted on robj deallocation.",https://github.com/redis/redis/commit/04f738fb5e47432a7d9a859f0f2afb7acd9b4f92
redis/redis,https://github.com/redis/redis,6431de3,Stav-Levi,2025-10-23T08:33:59Z,"Refactor and change config for broken AOF tail on startup (#14349)  This PR follows https://github.com/redis/redis/pull/14058  When Redis is shut down uncleanly (e.g., due to power loss or system crashes), invalid bytes may remain at the end of AOF files. This PR refactors the AOF corruption handling system and introduces automatic recovery capabilities to improve resilience and reduce downtime. The feature only handles corruption at the end of files, respects the configured size limit to prevent excessive data loss, maintains all valid commands before the corruption point.  Removed aof-load-broken configuration option Rename aof-load-broken-max-size to aof-load-corrupt-tail-max-size. New Config: aof-load-corrupt-tail-max-size - enables recovery for corruption up to specified size",https://github.com/redis/redis/commit/6431de3c3869dd32d4f10be3e98ad6b82f978775
redis/redis,https://github.com/redis/redis,235e688,debing.sun,2025-10-22T16:16:32Z,"RED-135816: Lookahead pre-fetching (#14440)  ## Problem and Motivation Currently, the client only parses one command, then executes it, then parses new commands until the querybuf is consumed. Doing it this way means we cannot perform memory prefetch when IO threads are not enabled, and when IO threads are enabled, we can only parse the first command in the IO thread, while the remaining command parsing still needs to be done in the main thread.  This describes a limitation in the current Redis command processing pipeline where:  Without IO threads: Commands are parsed and executed one by one sequentially, preventing memory prefetching optimizations With IO threads: Only the first command gets parsed in the IO thread, but subsequent commands from the same client's query buffer must still be parsed in the main thread  ## Solution Overview  **Core Innovation**: Parse multiple user commands in advance through a lookahead pipeline.  **Key Insight**: Since Redis already parses commands to extract keys, we can do this parsing earlier and memory prefetch operations before the command reaches execution, allowing multiple I/O operations to run in parallel.  The bulk of the PR is a redesign of the command processing flow for both standalone commands and transactional commands.  ### High Level Command Processing Flow  #### Before This PR (processInputBuffer())  - While there is data in the client's query buffer: - Read the data and try to parse a complete command (processInlineBuffer() or processMultibulkBuffer()).   - If the command is incomplete, exit and wait for more data. - The Command is complete. Process and potentially execute it (processCommandAndResetClient(), processCommand()):     - Prepare for the next command (commandProcessed()).  ### Major Changes in the Client's Structure  To support the new command processing flow:  - **New pendingCommand structure**: Since the previous flow processed commands one at a time, it used the client structure to hold the current (and only) parsed command arguments (argv/argc) and other metadata. In the new design, multiple commands are processed, waiting for execution. So, a new pendingCommand structure is introduced to hold a parsed command's arguments and its metadata. - **New pendingCommandList structure (pending_cmds)** that contains all the pending commands with maintained order and includes a ready_len counter that tracks the number of fully parsed commands ready for execution. All commands are fully parsed except possibly the last one (client's command order is maintained). - **New pendingCommandPool structure (cmd_pool)** that manages a shared pool for reusing pendingCommand objects to reduce memory allocation overhead.  There is a configurable lookahead limit (server.lookahead) that controls how many fully parsed commands (ready pending commands) to process ahead of time.  #### New High Level Flow for Standalone Commands (processInputBuffer())  - While there is data in the client's query buffer or there are ready pending commands: - While there is data in the client's query buffer and we haven't reached the lookahead limit: - Read the data and try to parse a complete command (processInlineBuffer() or processMultibulkBuffer()). Allocate a new pending command if needed, store the command's metadata in the pending command, and add the pending command to the client's pending commands list.     - If the command is incomplete, exit and wait for more data. - The command is complete, we have a new ready pending command, preprocess it (preprocessCommand()): - Extract the keys of the command and store the results in the pending command (extractKeysAndSlot()). - If there are pending commands, continue executing them until the queue is empty.  ## Transaction Support  ### Major Changes in Structures  - The multiState structure now contains an array of pendingCommand pointers instead of multiCmd pointers. - The multiCmd structure was deleted (no longer needed).  ### New Transaction Support  - queueMultiCommand(): - The pending commands are moved from the client's pending_cmds list to the multiState's commands array.  ## Detailed Changes  ### Additional Client Structure Changes  - Replaced argv_len_sum with all_argv_len_sum to reflect the total memory consumed by all pending commands.  ### Clients and Pending Commands Management  - Clients using pending commands now manage the command arguments via the pendingCommand. Specifically, the memory occupied by argv. - **Pending commands management functions**: - `initPendingCommand()` initializes a newly allocated pending command. - `freeClientPendingCommand()` frees a pending command of a client and its associated resources. - `freeClientPendingCommands()` receives the number of pending commands to free and calls freeClientPendingCommand() to free them.  ### Buffer Processing Changes  - `processInlineBuffer()`, once a full command is read, used to populate the client's command fields (argc, argv, etc.). Now it creates and populates a pendingCommand, and adds it to the client's pending_cmds list. - `processMultibulkBuffer()`: Similar changes to processInlineBuffer(). The difference is that a pending command may already exist from a previous call to the function, so parsing will continue populating it instead of creating a new one. - `resetClientInternal()` used to receive a free_argv parameter and pass it to freeClientArgvInternal(), which freed the client's argv if set, and also reset client's command fields. It now receives the number of pending commands to free and handles two cases: - The client uses pending commands so they are freed by calling freeClientPendingCommands(). - The client doesn't use pending commands (e.g., LUA client) so the client's argv is freed by calling freeClientArgvInternal(). It then frees the client's command fields that freeClientArgvInternal() doesn't free now.  ### Other Changes  - Simulate lookahead command preprocessing when loading an AOF and queuing transaction commands; This is necessary since queueMultiCommand() now requires a pending command. - The INVALID_CLUSTER_SLOT constant was defined to indicate an invalid cluster slot. It is used to signal a cross-slot error in preprocessCommand(). - getNodeByQuery() no longer performs cross-slot checks, relying instead on the checks already performed in preprocessCommand(). It also no longer calls getKeysFromCommand() as this was also done in preprocessCommand().  ### Debugging  - Added ""debug lookahead"" command to print the size of the lookahead pipeline for each client.  ## New Configuration  - **lookahead**: Runtime-configurable lookahead depth (default: 16)  ## Security  - **Limit lookahead for unauthenticated clients to 1**. This is both to reduce memory overhead, and to prevent errors; AUTH can affect the handling of succeeding commands.   ---------  Co-authored-by: Slava Koyfman <slava.koyfman@redis.com> Co-authored-by: Oran Agra <oran@redis.com> Co-authored-by: Udi Ron <udi.ron@redis.com> Co-authored-by: moticless <moticless@github.com> Co-authored-by: Yuan Wang <yuan.wang@redis.com>",https://github.com/redis/redis/commit/235e688b010b38496ea1de06b0bfc2786b1ebc63
redis/redis,https://github.com/redis/redis,3406152,GuyAv46,2025-10-22T15:28:30Z,"Add API RedisModule_ClusterKeySlotC (#14442)  Enhancing #13069, by adding a more direct API for getting the slot of a string  Introduces new module API:   ```c /* Like `RM_ClusterKeySlot`, but gets a char pointer and a length.  * Returns the cluster slot of a key, similar to the `CLUSTER KEYSLOT` command.  * This function works even if cluster mode is not enabled. */  unsigned int RM_ClusterKeySlotC(const char *keystr, size_t keylen) ```",https://github.com/redis/redis/commit/340615255b6f1f448f1b8a39572e6df9d8f09b80
redis/redis,https://github.com/redis/redis,2bc4e02,Ozan Tezcan,2025-10-22T12:56:20Z,"Add Atomic Slot Migration (ASM) support (#14414)  ## <a name=""overview""></a> Overview  This PR is a joint effort with @ShooterIT . I’m just opening it on behalf of both of us.  This PR introduces Atomic Slot Migration (ASM) for Redis Cluster — a new mechanism for safely and efficiently migrating hash slots between nodes.  Redis Cluster distributes data across nodes using 16384 hash slots, each owned by a specific node. Sometimes slots need to be moved — for example, to rebalance after adding or removing nodes, or to mitigate a hot shard that’s overloaded. Before ASM, slot migration was non-atomic and client-dependent, relying on CLUSTER SETSLOT, GETKEYSINSLOT, MIGRATE commands, and client-side handling of ASK/ASKING replies. This process was complex, error-prone, slow and could leave clusters in inconsistent states after failures. Clients had to implement redirect logic, multi-key commands could fail mid-migration, and errors often resulted in orphaned keys or required manual cleanup. Several related discussions can be found in the issue list, some examples: https://github.com/redis/redis/issues/14300 , https://github.com/redis/redis/issues/4937 , https://github.com/redis/redis/issues/10370 , https://github.com/redis/redis/issues/4333 , https://github.com/redis/redis/issues/13122, https://github.com/redis/redis/issues/11312  Atomic Slot Migration (ASM) makes slot rebalancing safe, transparent, and reliable, addressing many of the limitations of the legacy migration method. Instead of moving keys one by one, ASM replicates the entire slot’s data plus live updates to the target node, then performs a single atomic handoff. Clients keep working without handling ASK/ASKING replies, multi-key operations remain consistent, failures don’t leave partial states, and replicas stay in sync. The migration process also completes significantly faster. Operators gain new commands (CLUSTER MIGRATION IMPORT, STATUS, CANCEL) for monitoring and control, while modules can hook into migration events for deeper integration.  ### The problems of legacy method in detail  Operators and developers ran into multiple issues with the legacy method, some of these issues in detail:  1. **Redirects and Client Complexity:** While a slot was being migrated, some keys were already moved while others were not. Clients had to handle `-ASK` and `-ASKING` responses, reissuing requests to the target node. Not all client libraries implemented this correctly, leading to failed commands or subtle bugs. Even when implemented, it increased latency and broke naive pipelines. 2. **Multi-Key Operations Became Unreliable:** Commands like `MGET key1 key2` could fail with `TRYAGAIN` if part of the slot was already migrated. This made application logic unpredictable during resharding. 3. **Risk of failure:** Keys were moved one-by-one (with MIGRATE command). If the source crashed, or the destination ran out of memory, the system could be left in an inconsistent state: some keys moved, others lost, slots partially migrated. Manual intervention was often needed, sometimes resulting in data loss. 4. **Replica and Failover Issues:** Replicas weren’t aware of migrations in progress. If a failover occurred mid-migration, manual intervention was required to clean up or resume the process safely. 5. **Operational Overhead:** Operators had to coordinate multiple commands (CLUSTER SETSLOT, MIGRATE, GETKEYSINSLOT, etc.) with little visibility into progress or errors, making rebalancing slow and error-prone. 6. **Poor performance:** Key-by-key migration was inherently slow and inefficient for large slot ranges. 7. **Large keys:** Large keys could fail to migrate or cause latency spikes on the destination node.  ### How Atomic Slot Migration Fixes This  Atomic Slot Migration (ASM) eliminates all of these issues by:  1. **Clients:** Clients no longer need to handle ASK/ASKING; the migration is fully transparent. 2. **Atomic ownership transfer:** The entire slot’s data (snapshot + live updates) is replicated and handed off in a single atomic step. 3. **Performance**: ASM completes migrations significantly faster by streaming slot data in parallel (snapshot + incremental updates) and eliminating key-by-key operations. 4. **Consistency guarantees:** Multi-key operations and pipelines continue to work reliably throughout migration. 5. **Resilience:** Failures no longer leave orphaned keys or partial states; migration tasks can be retried or safely cancelled. 6. **Replica awareness:** Replicas remain consistent during migration, and failovers will no longer leave partially imported keys. 7. **Operator visibility:** New CLUSTER MIGRATION subcommands (IMPORT, STATUS, CANCEL) provide clear observability and management for operators.   ### ASM Diagram and Migration Steps  ```       ┌─────────────┐               ┌────────────┐     ┌───────────┐      ┌───────────┐ ┌───────┐               │             │               │Destination │     │Destination│      │ Source    │ │Source │               │  Operator   │               │   master   │     │ replica   │      │ master    │ │ Fork  │               │             │               │            │     │           │      │           │ │       │               └──────┬──────┘               └─────┬──────┘     └─────┬─────┘      └─────┬─────┘ └───┬───┘                      │                            │                  │                  │           │                          │                            │                  │                  │           │                          │CLUSTER MIGRATION IMPORT    │                  │                  │           │                          │   <start-slot> <end-slot>..│                  │                  │           │                          ├───────────────────────────►│                  │                  │           │                          │                            │                  │                  │           │                          │   Reply with <task-id>     │                  │                  │           │                          │◄───────────────────────────┤                  │                  │           │                          │                            │                  │                  │           │                          │                            │                  │                  │           │                          │                            │ CLUSTER SYNCSLOTS│SYNC              │           │                          │ CLUSTER MIGRATION STATUS   │   <task-id> <start-slot> <end-slot>.│           │             Monitor      │   ID <task-id>             ├────────────────────────────────────►│           │             task      ┌─►├───────────────────────────►│                  │                  │           │             state     │  │                            │                  │                  │           │             till      │  │      Reply status          │  Negotiation with multiple channels │           │             completed └─ │◄───────────────────────────┤      (i.e rdbchannel repl)          │           │                          │                            │◄───────────────────────────────────►│           │                          │                            │                  │                  │  Fork     │                          │                            │                  │                  ├──────────►│ ─┐                                                    │                  │                  │           │  │                                                    │   Slot snapshot as RESTORE commands │           │  │                                                    │◄────────────────────────────────────────────────┤  │                                                    │   Propagate      │                  │           │  │                ┌─────────────┐                     ├─────────────────►│                  │           │  │                │             │                     │                  │                  │           │  │ Snapshot       │   Client    │                     │                  │                  │           │  │ delivery       │             │                     │   Replication stream for slot range │           │  │ duration       └──────┬──────┘                     │◄────────────────────────────────────┤           │  │                       │                            │   Propagate      │                  │           │  │                       │                            ├─────────────────►│                  │           │  │                       │                            │                  │                  │           │  │                       │    SET key value1          │                  │                  │           │  │                       ├─────────────────────────────────────────────────────────────────►│           │  │                       │         +OK                │                  │                  │           │ ─┘                       │◄─────────────────────────────────────────────────────────────────┤           │                          │                            │                  │                  │           │                          │                            │    Drain repl stream                │ ──┐       │                          │                            │◄────────────────────────────────────┤   │       │                          │    SET key value2          │                  │                  │   │       │                          ├─────────────────────────────────────────────────────────────────►│   │Write  │                          │                            │                  │                  │   │pause  │                          │                            │                  │                  │   │       │                          │                            │  Publish new config via cluster bus │   │       │                          │       +MOVED               ├────────────────────────────────────►│ ──┘       │                          │◄─────────────────────────────────────────────────────────────────┤ ──┐       │                          │                            │                  │                  │   │       │                          │                            │                  │                  │   │Trim   │                          │                            │                  │                  │ ──┘       │                          │     SET key value2         │                  │                  │           │                          ├───────────────────────────►│                  │                  │           │                          │         +OK                │                  │                  │           │                          │◄───────────────────────────┤                  │                  │           │                          │                            │                  │                  │           │                          │                            │                  │                  │           │              ```  ### New commands introduced  There are two new commands:  1. A command to start, monitor and cancel the migration operation:  `CLUSTER MIGRATION <arg>` 2. An internal command to manage slot transfer between source and destination:  `CLUSTER SYNCSLOTS <arg>` For more details, please refer to the [New Commands](#new-commands) section. Internal command messaging is mostly omitted in the diagram for simplicity.   ### Steps 1. Slot migration begins when the operator sends `CLUSTER MIGRATION IMPORT <start-slot> <end-slot> ...` to the destination master. The process is initiated from the destination node, similar to REPLICAOF. This approach allows us to reuse the same logic and share code with the new replication mechanism (see https://github.com/redis/redis/pull/13732). The command can include multiple slot ranges. The destination node creates one migration task per source node, regardless of how many slot ranges are specified. Upon successfully creating the task, the destination node replies IMPORT command with the assigned task ID. The operator can then monitor progress using `CLUSTER MIGRATION STATUS ID <task-id>` . When the task’s state field changes to `completed`, the migration has finished successfully. Please see [New Commands](#new-commands) section for the output sample.  2. After creating the migration task,  the destination node will request replication of slots by using the internal command `CLUSTER SYNCSLOTS`. 3. Once the source node accepts the request, the destination node establishes another separate connection(similar to rdbchannel replication) so snapshot data and incremental changes can be transmitted in parallel. 4. Source node forks, starts delivering snapshot content (as per-key RESTORE commands) from one connection and incremental changes from the other connection. The destination master starts applying commands from the snapshot connection and accumulates incremental changes. Applied commands are also propagated to the destination replicas via replication backlog.      Note: Only commands of related slots are delivered to the destination node. This is done by writing them to the migration client’s output buffer, which serves as the replication stream for the migration operation. 5. Once the source node finishes delivering the snapshot and determines that the destination node has caught up (remaining repl stream to consume went under a configured limit), it pauses write traffic for the entire server. After pausing the writes, the source node forwards any remaining write commands to the destination node.  6. Once the destination consumes all the writes, it bumps up cluster config epoch and changes the configuration. New config is published via cluster bus. 7. When the source node receives the new configuration, it can redirect clients and it begins trimming the migrated slots, while also resuming write traffic on the server.  ### Internal slots synchronization state machine ![asm state machine](https://github.com/user-attachments/assets/b7db353c-969e-4bde-b77f-c6abe5aa13d3)  1. The destination node performs authentication using the cluster secret introduced in #13763 , and transmits its node ID information. 2. The destination node sends `CLUSTER SYNCSLOTS SYNC <task-id> <start-slot> <end-slot>` to initiate a slot synchronization request and establish the main channel. The source node responds with `+RDBCHANNELSYNCSLOTS`, indicating that the destination node should establish an RDB channel. 3. The destination node then sends `CLUSTER SYNCSLOTS RDBCHANNEL <task-id>` to establish the RDB channel, using the same task-id as in the previous step to associate the two connections as part of the same ASM task. The source node replies with `+SLOTSSNAPSHOT`, and `fork` a child process to transfer slot snapshot. 4. The destination node applies the slot snapshot data received over the RDB channel, while proxying the command stream to replicas. At the same time, the main channel continues to read and buffer incremental commands in memory. 5. Once the source node finishes sending the slot snapshot, it notifies the destination node using the `CLUSTER SYNCSLOTS SNAPSHOT-EOF` command. The destination node then starts streaming the buffered commands while continuing to read and buffer incremental commands sent from the source. 6. The destination node periodically sends `CLUSTER SYNCSLOTS ACK <offset>` to inform the source of the applied data offset. When the offset gap meets the threshold, the source node pauses write operations. After all buffered data has been drained, it sends `CLUSTER SYNCSLOTS STREAM-EOF` to the destination node to hand off slots. 7. Finally, the destination node takes over slot ownership, updates the slot configuration and bumps the epoch, then broadcasts the updates via cluster bus. Once the source node detects the updated slot configuration, the slot migration process is complete.   ### Error handling - If the connection between the source and destination is lost (due to disconnection, output buffer overflow, OOM, or timeout), the destination node automatically restarts the migration from the beginning. The destination node will retry the operation until it is explicitly cancelled using the CLUSTER MIGRATION CANCEL <task-id> command. - If a replica connection drops during migration, it can later resume with PSYNC, since the imported slot data is also written to the replication backlog. - During the write pause phase, the source node sets a timeout. If the destination node fails to drain remaining replication data and update the config during that time, the source node assumes the destination has failed and automatically resumes normal writes for the migrating slots. - On any error, the destination node triggers a trim operation to discard any partially imported slot data. - If node crashes during importing, unowned keys are deleted on start up.    ### <a name=""slot-snapshot-format-considerations""></a> Slot Snapshot Format Considerations   When the source node forks to deliver slot content, in theory, there are several possible formats for transmitting the snapshot data:  **Mini RDB**:A compact RDB file containing only the keys from the migrating slots. This format is efficient for transmission, but it cannot be easily forwarded to destination-side replicas. **AOF format**: The source node can generate commands in AOF form (e.g., SET x y, HSET h f v) and stream them. Individual commands are easily appended to the replication stream and propagated to replicas. Large keys can also be split into multiple commands (incrementally reconstructing the value), similar to the AOF rewrite process. **RESTORE commands**: Each key is serialized and sent as a `RESTORE` command. These can be appended directly to the destination’s replication stream, though very large keys may make serialization and transmission less efficient.  We chose the `RESTORE` command as default approach for the following reasons: - It can be easily propagated to replicas. - It is more efficient than AOF for most cases, and some module keys do not support the AOF format. - For large **non-module** keys that are not string, ASM automatically switches to the AOF-based key encoding as an optimization when the key’s cardinality exceeds 512. This approach allows the key to be transferred in chunks rather than as a single large payload, reducing memory pressure and improving migration efficiency. In future versions, the RESTORE command may be enhanced to handle large keys more efficiently.  Some details: - For RESTORE commands, normally by default Redis compresses keys. We disable compression while delivering RESTORE commands as compression comes with a performance hit. Without compression, replication is several times faster.  - For string keys, we still prefer AOF format, e.g. SET commands as it is currently more efficient than RESTORE, especially for big keys.  ### <a name=""trimming-the-keys""></a> Trimming the keys   When a migration completes successfully, the source node deletes the migrated keys from its local database. Since the migrated slots may contain a large number of keys, this trimming process must be efficient and non-blocking.  In cluster mode, Redis maintains per-slot data structures for keys, expires, and subexpires. This organization makes it possible to efficiently detach all data associated with a given slot in a single step. During trimming, these slot-specific data structures are handed off to a background I/O (BIO) thread for asynchronous cleanup—similar to how FLUSHALL or FLUSHDB operate. This mechanism is referred to as background trimming, and it is the preferred and default method for ASM, ensuring that the main thread remains unblocked.  However, unlike Redis itself, some modules may not maintain per-slot data structures and therefore cannot drop related slots data in a single operation. To support these cases, Redis introduces active trimming, where key deletion occurs in the main thread instead. This is not a blocking operation, trimming runs concurrently in the main thread, periodically removing keys during the cron loop. Each deletion triggers a keyspace notification so that modules can react to individual key removals. While active trim is less efficient, it ensures backward compatibility for modules during the transition period.  Before starting the trim, Redis checks whether any module is subscribed to newly added  `REDISMODULE_NOTIFY_KEY_TRIMMED` keyspace event. If such subscribers exist, active trimming is used; otherwise, background trimming is triggered. Going forward, modules are expected to adopt background trimming to take advantage of its performance and scalability benefits, and active trimming will be phased out once modules migrate to the new model.  Redis also prefers active trimming if there is any client that is using client tracking feature (see [client-side caching](https://redis.io/docs/latest/develop/reference/client-side-caching/)). In the current client tracking protocol, when a database is flushed (e.g., via the FLUSHDB command), a null value is sent to tracking clients to indicate that they should invalidate all locally cached keys. However, there is currently no mechanism to signal that only specific slots have been flushed. Iterating over all keys in the slots to be trimmed would be a blocking operation. To avoid this, if there is any client that is using client tracking feature, Redis automatically switches to active trimming mode. In the future, the client tracking protocol can be extended to support slot-based invalidation, allowing background trimming to be used in these cases as well.  Finally, trimming may also be triggered after a migration failure. In such cases, the operation ensures that any partially imported or inconsistent slot data is cleaned up, maintaining cluster consistency and preventing stale keys from remaining in the source or destination nodes.  Note about active trim: Subsequent migrations can complete while a prior trim is still running. In that case, the new migration’s trim job is queued and will start automatically after the current trim finishes. This does not affect slot ownership or client traffic—it only serializes the background cleanup.  ### <a name=""replica-handling""></a> Replica handling   - During importing, new keys are propagated to destination side replica. Replica will check slot ownership before replying commands like SCAN, KEYS, DBSIZE not to include these unowned keys in the reply.     Also, when an import operation begins, the master now propagates an internal command through the replication stream, allowing replicas to recognize that an ASM operation is in progress. This is done by the internal `CLUSTER SYNCSLOTS CONF ASM-TASK` command in the replication stream. This enables replicas to trigger the relevant module events so that modules can adapt their behavior — for example, filtering out unowned keys from read-only requests during ASM operations. To be able to support full sync with RDB delivery scenarios, a new AUX field is also added to the RDB: `cluster-asm-task`. It's value is a string in the format of `task_id:source_node:dest_node:operation:state:slot_ranges`.   - After a successful migration or on a failed import, master will trim the keys. In that case, master will propagate a new command to the replica: `TRIMSLOTS RANGES <numranges> <start-slot> <end-slot> ... ` . So, the replica will start trimming once this command is received.   ### <a name=""propagating-data-outside-the-keyspace""></a> Propagating data outside the keyspace  When the destination node is newly added to the cluster, certain data outside the keyspace may need to be propagated first. A common example is functions. Previously, redis-cli handled this by transferring functions when a new node was added. With ASM, Redis now automatically dumps and sends functions to the destination node using `FUNCTION RESTORE ..REPLACE` command — done purely for convenience to simplify setup.  Additionally, modules may also need to propagate their own data outside the keyspace. To support this, a new API has been introduced: `RM_ClusterPropagateForSlotMigration()`. See the [Module Support](#module-support) section for implementation details.  ### Limitations  1. Single migration at a time: Only one ASM migration operation is allowed at a time. This limitation simplifies the current design but can be extended in the future.  2. Large key handling: For large keys, ASM switches to AOF encoding to deliver key data in chunks. This mechanism currently applies only to non-module keys. In the future, the RESTORE command may be extended to support chunked delivery, providing a unified solution for all key types. See [Slot Snapshot Format Considerations](#slot-snapshot-format-considerations) for details.  3. There are several cases that may cause an Atomic Slot Migration (ASM) to be aborted (can be retried afterwards):     - FLUSHALL / FLUSHDB: These commands introduce complexity during ASM. For example, if executed on the migrating node, they must be propagated only for the migrating slots. However, when combined with active trimming, their execution may need to be deferred until it is safe to proceed, adding further complexity to the process.     - FAILOVER: The replica cannot resume the migration process. Migration should start from the beginning.     - Module propagates cross-slot command during ASM via RM_Replicate(): If this occurs on the migrating node, Redis cannot split the command to propagate only the relevant slots to the ASM destination. To keep the logic simple and consistent, ASM is cancelled in this case. Modules should avoid propagating cross-slot commands during migration.     - CLIENT PAUSE: The import task cannot progress during a write pause, as doing so would violate the guarantee that no writes occur during migration. To keep things simple, the ASM task is aborted when CLIENT PAUSE is active.     - Manual Slot Configuration Changes: If slot configuration is modified manually during ASM (for example, when legacy migration methods are mixed with ASM), the process is aborted. Note: This situation is highly unexpected — users should not combine ASM with legacy migration methods.      4. When active trimming is enabled, a node must not re-import the same slots while trimming for those slots is still in progress. Otherwise, it can’t distinguish newly imported keys from pre-existing ones, and the trim cron might delete the incoming keys by mistake. In this state, the node rejects IMPORT operation for those slots until trimming completes. If the master has finished trimming but a replica is still trimming, master may still start the import operation for those slots. So, the replica checks whether the master is sending commands for those slots; if so, it blocks the master’s client connection until trimming finishes. This is a corner case, but we believe the behavior is reasonable for now. In the worst case, the master may drop the replica (e.g., buffer overrun), triggering a new full sync.  # API Changes  ## <a name=""new-commands""></a> New Commands   ### Public commands 1. **Syntax:**  `CLUSTER MIGRATION IMPORT <start-slot> <end-slot> [<start-slot> <end-slot>]...`   **Args:** Slot ranges   **Reply:**      - String task ID     - -ERR <message> on failure (e.g. invalid slot range)       **Description:** Executes on the destination master. Accepts multiple slot ranges and triggers atomic migration for the specified ranges. Returns a task ID that can be used to monitor the status of the task. In CLUSTER MIGRATION STATUS output, “state” field will be `completed` on a successful operation.  2. **Syntax:**  `CLUSTER MIGRATION CANCEL [ID <id> | ALL]`   **Args:** Task ID or ALL   **Reply:** Number of cancelled tasks      **Description:** Cancels an ongoing migration task by its ID or cancels all tasks if ALL is specified. Note: Cancelling a task on the source node does not stop the migration on the destination node, which will continue retrying until it is also cancelled there.   3. **Syntax:**  `CLUSTER MIGRATION STATUS [ID <id> | ALL]`   **Args:** Task ID or ALL     - **ID:** If provided, returns the status of the specified migration task.     - **ALL:** Lists the status of all migration tasks.      **Reply:**       - A list of migration task details (both ongoing and completed ones).       - Empty list if the given task ID does not exist.      **Description:** Displays the status of all current and completed atomic slot migration tasks. If a specific task ID is provided, it returns detailed information for that task only.          **Sample output:** ``` 127.0.0.1:5001> cluster migration status all 1)  1) ""id""     2) ""24cf41718b20f7f05901743dffc40bc9b15db339""     3) ""slots""     4) ""0-1000""     5) ""source""     6) ""1098d90d9ba2d1f12965442daf501ef0b6667bec""     7) ""dest""     8) ""b3b5b426e7ea6166d1548b2a26e1d5adeb1213ac""     9) ""operation""    10) ""migrate""    11) ""state""    12) ""completed""    13) ""last_error""    14) """"    15) ""retries""    16) ""0""    17) ""create_time""    18) ""1759694528449""    19) ""start_time""    20) ""1759694528449""    21) ""end_time""    22) ""1759694528464""    23) ""write_pause_ms""    24) ""10"" ```  ### Internal commands  1. **Syntax:**  `CLUSTER SYNCSLOTS <arg> ...`   **Args:** Internal messaging operations   **Reply:**  +OK or -ERR <message> on failure (e.g. invalid slot range)       **Description:** Used for internal communication between source and destination nodes. e.g. handshaking, establishing multiple channels, triggering handoff.      2. **Syntax:**  `TRIMSLOTS RANGES <numranges> <start-slot> <end-slot> ...`   **Args:** Slot ranges to trim   **Reply:**  +OK       **Description:** Master propagates it to replica so that replica can trim unowned keys after a successful migration or on a failed import.   ## New configs  - `cluster-slot-migration-max-archived-tasks`: To list in `CLUSTER MIGRATION STATUS ALL` output, Redis keeps last n migration tasks in memory. This config controls maximum number of archived ASM tasks. Default value: 32, used as a hidden config - `cluster-slot-migration-handoff-max-lag-bytes`: After the slot snapshot is completed, if the remaining replication stream size falls below this threshold, the source node pauses writes to hand off slot ownership. A higher value may trigger the handoff earlier but can lead to a longer write pause, since more data remains to be replicated. A lower value can result in a shorter write pause, but it may be harder to reach the threshold if there is a steady flow of incoming writes. Default value: 1MB - `cluster-slot-migration-write-pause-timeout`: The maximum duration (in milliseconds) that the source node pauses writes during ASM handoff. After pausing writes, if the destination node fails to take over the slots within this timeout (for example, due to a cluster configuration update failure), the source node assumes the migration has failed and resumes writes to prevent indefinite blocking. Default value: 10 seconds - `cluster-slot-migration-sync-buffer-drain-timeout`: Timeout in milliseconds for sync buffer to be drained during ASM.  After the destination applies the accumulated buffer, the source continues sending commands for migrating slots. The destination keeps applying them, but if the gap remains above the acceptable limit (see `slot-migration-handoff-max-lag-bytes`), which may cause endless synchronization. A timeout check is required to handle this case. The timeout is calculated as **the maximum of two values**:    - A configurable timeout (slot-migration-sync-buffer-drain-timeout) to avoid false positives.    - A dynamic timeout based on the time that the destination took to apply the slot snapshot and the accumulated buffer during slot snapshot delivery. The destination should be able to drain the remaining sync buffer in less time than this. We multiply it by 2 to be more conservative.       Default value: 60000 millliseconds, used as a hidden config  ## New flag in CLIENT LIST - the client responsible for importing slots is marked with the `o` flag. - the client responsible for migrating slots is marked with the `g` flag.  ## New INFO fields  - `mem_cluster_slot_migration_output_buffer`: Memory usage of the migration client’s output buffer. Redis writes incoming changes to this buffer during the migration process. - `mem_cluster_slot_migration_input_buffer`: Memory usage of the accumulated replication stream buffer on the importing node. - `mem_cluster_slot_migration_input_buffer_peak`: Peak accumulated repl buffer size on the importing side  ## New CLUSTER INFO fields  - `cluster_slot_migration_active_tasks`: Number of in-progress ASM tasks. Currently, it will be 1 or 0.  - `cluster_slot_migration_active_trim_running`: Number of active trim jobs in progress and scheduled - `cluster_slot_migration_active_trim_current_job_keys`: Number of keys scheduled for deletion in the current trim job. - `cluster_slot_migration_active_trim_current_job_trimmed`: Number of keys already deleted in the current trim job. - `cluster_slot_migration_stats_active_trim_started`: Total number of trim jobs that have started since the process began. - `cluster_slot_migration_stats_active_trim_completed`: Total number of trim jobs completed since the process began. - `cluster_slot_migration_stats_active_trim_cancelled`: Total number of trim jobs cancelled since the process began.   ## Changes in RDB format  A new aux field is added to RDB: `cluster-asm-task`. When an import operation begins, the master now propagates an internal command through the replication stream, allowing replicas to recognize that an ASM operation is in progress. This enables replicas to trigger the relevant module events so that modules can adapt their behavior — for example, filtering out unowned keys from read-only requests during ASM operations. To be able to support RDB delivery scenarios, a new field is added to the RDB. See [replica handling](#replica-handling)  ## Bug fix - Fix memory leak when processing forgetting node type message - Fix data race of writing reply to replica client directly when enabling multi-threading We don't plan to back point them into old versions, since they are very rare cases.  ## Keys visibility When performing atomic slot migration, during key importing on the destination node or key trimming on the source/destination, these keys will be filtered out in the following commands: - KEYS - SCAN - RANDOMKEY - CLUSTER GETKEYSINSLOT - DBSIZE - CLUSTER COUNTKEYSINSLOT  The only command that will reflect the increasing number of keys is: - INFO KEYSPACE  ## <a name=""module-support""></a> Module Support   **NOTE:** Please read [trimming](#trimming-the-keys) section and see how does ASM decide about trimming method when there are modules in use.   ### New notification: ```c #define REDISMODULE_NOTIFY_KEY_TRIMMED (1<<17)  ``` When a key is deleted by the active trim operation, this notification will be sent to subscribed modules. Also, ASM will automatically choose the trimming method depending on whether there are any subscribers to this new event. Please see the further details here: [trimming](#trimming-the-keys)   ### New struct in the API: ```c typedef struct RedisModuleSlotRange {     uint16_t start;     uint16_t end; } RedisModuleSlotRange;  typedef struct RedisModuleSlotRangeArray {     int32_t num_ranges;     RedisModuleSlotRange ranges[]; } RedisModuleSlotRangeArray; ```  ### New Events #### 1. REDISMODULE_EVENT_CLUSTER_SLOT_MIGRATION (RedisModuleEvent_ClusterSlotMigration)  These events notify modules about different stages of Active Slot Migration (ASM) operations such as when import or migration starts, fails, or completes. Modules can use these notifications to track cluster slot movements or perform custom logic during ASM transitions.  ```c #define REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_IMPORT_STARTED 0 #define REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_IMPORT_FAILED 1 #define REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_IMPORT_COMPLETED 2 #define REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_MIGRATE_STARTED 3 #define REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_MIGRATE_FAILED 4 #define REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_MIGRATE_COMPLETED 5 #define REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_MIGRATE_MODULE_PROPAGATE 6 ```  Parameter to these events: ```c typedef struct RedisModuleClusterSlotMigrationInfo { uint64_t version; /* Not used since this structure is never passed from the module to the core right now. Here                                for future compatibility. */     char source_node_id[REDISMODULE_NODE_ID_LEN + 1];     char destination_node_id[REDISMODULE_NODE_ID_LEN + 1];     const char *task_id;     RedisModuleSlotRangeArray* slots; } RedisModuleClusterSlotMigrationInfoV1;  #define RedisModuleClusterSlotMigrationInfo RedisModuleClusterSlotMigrationInfoV1 ```   #### 2. REDISMODULE_EVENT_CLUSTER_SLOT_MIGRATION_TRIM (RedisModuleEvent_ClusterSlotMigrationTrim)  These events inform modules about the lifecycle of ASM key trimming operations. Modules can use them to detect when trimming starts, completes, or is performed asynchronously in the background.  ```c #define REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_TRIM_STARTED     0 #define REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_TRIM_COMPLETED   1 #define REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_TRIM_BACKGROUND  2 ```  Parameter to these events: ```c typedef struct RedisModuleClusterSlotMigrationTrimInfo { uint64_t version; /* Not used since this structure is never passed from the module to the core right now. Here                                for future compatibility. */     RedisModuleSlotRangeArray* slots; } RedisModuleClusterSlotMigrationTrimInfoV1;  #define RedisModuleClusterSlotMigrationTrimInfo RedisModuleClusterSlotMigrationTrimInfoV1 ```  ### New functions  ```c /* Returns 1 if keys in the specified slot can be accessed by this node, 0 otherwise.  *  * This function returns 1 in the following cases: * - The slot is owned by this node or by its master if this node is a replica * - The slot is being imported under the old slot migration approach (CLUSTER SETSLOT <slot> IMPORTING ..)  * - Not in cluster mode (all slots are accessible)  *  * Returns 0 for:  * - Invalid slot numbers (< 0 or >= 16384)  * - Slots owned by other nodes  */ int RM_ClusterCanAccessKeysInSlot(int slot);  /* Propagate commands along with slot migration.  *  * This function allows modules to add commands that will be sent to the * destination node before the actual slot migration begins. It should only be * called during the REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_MIGRATE_MODULE_PROPAGATE event.  *  * This function can be called multiple times within the same event to  * replicate multiple commands. All commands will be sent before the  * actual slot data migration begins.  * * Note: This function is only available in the fork child process just before  *       slot snapshot delivery begins.  *  * On success REDISMODULE_OK is returned, otherwise  * REDISMODULE_ERR is returned and errno is set to the following values:  *  * * EINVAL: function arguments or format specifiers are invalid. * * EBADF: not called in the correct context, e.g. not called in the REDISMODULE_SUBEVENT_CLUSTER_SLOT_MIGRATION_MIGRATE_MODULE_PROPAGATE event.  * * ENOENT: command does not exist.  * * ENOTSUP: command is cross-slot. * * ERANGE: command contains keys that are not within the migrating slot range.  */ int RM_ClusterPropagateForSlotMigration(RedisModuleCtx *ctx,                                         const char *cmdname,                                         const char *fmt, ...);  /* Returns the locally owned slot ranges for the node.  *  * An optional `ctx` can be provided to enable auto-memory management. * If cluster mode is disabled, the array will include all slots (0–16383).  * If the node is a replica, the slot ranges of its master are returned.  *  * The returned array must be freed with RM_ClusterFreeSlotRanges().  */                                 RedisModuleSlotRangeArray *RM_ClusterGetLocalSlotRanges(RedisModuleCtx *ctx);  /* Frees a slot range array returned by RM_ClusterGetLocalSlotRanges(). * Pass the `ctx` pointer only if the array was created with a context. */ void RM_ClusterFreeSlotRanges(RedisModuleCtx *ctx, RedisModuleSlotRangeArray *slots); ```  ## ASM API for alternative cluster implementations  Following https://github.com/redis/redis/pull/12742, Redis cluster code was restructured to support alternative cluster implementations. Redis uses cluster_legacy.c implementation by default. This PR adds a generic ASM API so alternative implementations can initiate and coordinate Atomic Slot Migration (ASM) while Redis executes the data movement and emits state changes.  Documentation rests in `cluster.h`:  ```c There are two new functions:  /* Called by cluster implementation to request an ASM operation. (cluster impl --> redis) */ int clusterAsmProcess(const char *task_id, int event, void *arg, char **err);  /* Called when an ASM event occurs to notify the cluster implementation. (redis --> cluster impl) */ int clusterAsmOnEvent(const char *task_id, int event, void *arg); ```  ```c /* API for alternative cluster implementations to start and coordinate  * Atomic Slot Migration (ASM).  * * These two functions drive ASM for alternative cluster implementations. * - clusterAsmProcess(...) impl -> redis: initiates/advances/cancels ASM operations  * - clusterAsmOnEvent(...) redis -> impl: notifies state changes  *  * Generic steps for an alternative implementation: * - On destination side, implementation calls clusterAsmProcess(ASM_EVENT_IMPORT_START)  *   to start an import operation.  * - Redis calls clusterAsmOnEvent() when an ASM event occurs. * - On the source side, Redis will call clusterAsmOnEvent(ASM_EVENT_HANDOFF_PREP) * when slots are ready to be handed off and the write pause is needed. * - Implementation stops the traffic to the slots and calls clusterAsmProcess(ASM_EVENT_HANDOFF) * - On the destination side, Redis calls clusterAsmOnEvent(ASM_EVENT_TAKEOVER) * when destination node is ready to take over the slot, waiting for ownership change. * - Cluster implementation updates the config and calls clusterAsmProcess(ASM_EVENT_DONE)  *   to notify Redis that the slots ownership has changed.  *  * Sequence diagram for import: * - Note: shows only the events that cluster implementation needs to react.  * * ┌───────────────┐ ┌───────────────┐ ┌───────────────┐ ┌───────────────┐ * │ Destination │ │ Destination │ │ Source │ │ Source │ * │ Cluster impl │ │ Master │ │ Master │ │ Cluster impl │ * └───────┬───────┘ └───────┬───────┘ └───────┬───────┘ └───────┬───────┘ * │ │ │ │ * │ ASM_EVENT_IMPORT_START │ │ │ * ├─────────────────────────────►│ │ │ * │ │ CLUSTER SYNCSLOTS <arg> │ │ * │ ├────────────────────────►│ │ * │ │ │ │ * │ │ SNAPSHOT(restore cmds) │ │ * │ │◄────────────────────────┤ │ * │ │ Repl stream │ │ * │ │◄────────────────────────┤ │ * │ │ │ ASM_EVENT_HANDOFF_PREP │ * │ │ ├────────────────────────────►│ * │ │ │ ASM_EVENT_HANDOFF │ * │ │ │◄────────────────────────────┤ * │ │ Drain repl stream │ │ * │ │◄────────────────────────┤ │ * │ ASM_EVENT_TAKEOVER │ │ │ * │◄─────────────────────────────┤ │ │ * │ │ │ │ * │ ASM_EVENT_DONE │ │ │ * ├─────────────────────────────►│ │ ASM_EVENT_DONE │ * │ │ │◄────────────────────────────┤ * │ │ │ │  */  #define ASM_EVENT_IMPORT_START 1 /* Start a new import operation (destination side) */ #define ASM_EVENT_CANCEL 2 /* Cancel an ongoing import/migrate operation (source and destination side) */ #define ASM_EVENT_HANDOFF_PREP 3 /* Slot is ready to be handed off to the destination shard (source side) */ #define ASM_EVENT_HANDOFF 4 /* Notify that the slot can be handed off (source side) */ #define ASM_EVENT_TAKEOVER 5 /* Ready to take over the slot, waiting for config change (destination side) */ #define ASM_EVENT_DONE 6 /* Notify that import/migrate is completed, config is updated (source and destination side) */  #define ASM_EVENT_IMPORT_PREP 7 /* Import is about to start, the implementation may reject by returning C_ERR */ #define ASM_EVENT_IMPORT_STARTED    8  /* Import started */ #define ASM_EVENT_IMPORT_FAILED     9  /* Import failed */ #define ASM_EVENT_IMPORT_COMPLETED 10 /* Import completed (config updated) */ #define ASM_EVENT_MIGRATE_PREP 11 /* Migrate is about to start, the implementation may reject by returning C_ERR */ #define ASM_EVENT_MIGRATE_STARTED   12 /* Migrate started */ #define ASM_EVENT_MIGRATE_FAILED    13 /* Migrate failed */ #define ASM_EVENT_MIGRATE_COMPLETED 14 /* Migrate completed (config updated) */ ```  ------ Co-authored-by: Yuan Wang <yuan.wang@redis.com>  ---------  Co-authored-by: Yuan Wang <yuan.wang@redis.com>",https://github.com/redis/redis/commit/2bc4e0299d4d23cf8a0cb32b5cbf82e6ecb2b75a
redis/redis,https://github.com/redis/redis,090ca80,sggeorgiev,2025-10-21T12:35:43Z,"Add CLAIM parameter to XREADGROUP for automatic pending entry claiming (#14402)  ## Overview This PR enhances Redis Streams consumer groups by adding an optional CLAIM parameter to the `XREADGROUP` command, enabling automatic claiming of idle pending entries alongside normal message consumption in a single operation. ## Problem Statement Current Redis Streams consumer group implementations require developers to manually orchestrate multiple commands to handle both pending and new entries:  - `XPENDING` to discover idle pending entries - `XCLAIM/XAUTOCLAIM` to claim idle entries - `XREADGROUP` to consume new entries  This multi-command approach creates: - **Performance overhead** from multiple round trips to Redis - **Implementation complexity**, particularly when working with multiple streams - **Code duplication** across consumer implementations  ## Solution Extends XREADGROUP with a new optional CLAIM parameter: `XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] [CLAIM min-idle-time] STREAMS key [key ...] id [id ...]`  When CLAIM min-idle-time is specified, the command operates in two phases:  1. **Claim Phase:** Automatically claims pending entries idle for ≥ min-idle-time milliseconds 2. **Read Phase:** Processes new entries if the COUNT limit hasn't been reached  ## Response Format Changes When the CLAIM option is used, the response format is extended to include delivery metadata for each entry: **Standard XREADGROUP response (without CLAIM):** ``` 127.0.0.1:6379> XREADGROUP GROUP mygroup consumer1 STREAMS mystream > 1) 1) ""mystream""    2) 1) 1) ""1609459200000-0""          2) 1) ""field1""             2) ""value1"" ``` **XREADGROUP response with CLAIM:** ``` 127.0.0.1:6379> XREADGROUP GROUP mygroup consumer1 CLAIM 30000 STREAMS mystream > 1) 1) ""mystream""    2) 1) 1) ""1609459200000-0""          2) 1) ""field1""             2) ""value1""          3) 15000          4) 3 ``` **Response structure with CLAIM:**  - **Field 1:** Stream entry ID (unchanged) - **Field 2:** Field-value pairs (unchanged) - **Field 3:** Idle time in milliseconds - the number of milliseconds elapsed since this entry was last delivered to a consumer - **Field 4:** Delivery count - the number of times this entry has been delivered:      - `0` for new messages that haven't been delivered before   - `1+` for claimed messages (previously unacknowledged entries)  **Purpose of the new fields:** These fields enable intelligent client-side processing decisions:  - Idle time enables time-based escalation strategies, detection of stuck messages, and priority processing for critically delayed work - Delivery count enables retry limits, dead-letter queue logic, poison message detection, and alternative processing strategies based on failure history  Together, these fields provide the visibility needed to build robust, self-healing consumer systems without requiring additional XPENDING queries.  **Note:** If the ID parameter is not `>`, the command returns entries that are pending for the consumer, and the CLAIM option is ignored. In this case, the response follows the standard format without the additional delivery metadata fields. ## Key Benefits  - **Reduced Complexity:** Eliminates manual PEL management and multi-command orchestration - **Improved Performance:** Reduces round trips by 50-70% for workloads processing both pending and new entries - **Backward Compatibility:** Fully optional parameter with zero breaking changes to existing behavior - **Multi-Stream Support:** Works seamlessly across multiple streams in a single command - **Flexible Consumer Patterns:** Enables mixed consumer types within the same group:   -   Consumers without CLAIM that only handle new messages   -   Consumers with CLAIM that process both pending and new entries  ## Impact on Existing Commands The XCLAIM and XAUTOCLAIM commands may potentially benefit from the new pel_by_time index for improved performance, such optimizations require further investigation and testing. Enhancements to XCLAIM and XAUTOCLAIM are postponed for future work.  ## Performance Benchmarks ### Latency Performance Comprehensive performance testing demonstrates significant improvements over the traditional XAUTOCLAIM approach: **Test Methodology** Two identical test scenarios were executed to compare XAUTOCLAIM against XREADGROUP with CLAIM: **Test Setup:** 1. Insert 20,000 messages into a stream 2. Read all messages with XREADGROUP to populate the pending entries list (PEL) 3. Set IDLE time to 1100ms on 1,000 randomly selected pending messages using XCLAIM 4. Set IDLE time to 50ms on all remaining 19,000 pending messages using XCLAIM 5. Execute the target command with min-idle-time=1000ms and COUNT=1000 to claim the eligible messages 6. Repeat steps 3-5 for 1,000 iterations  **Test 1 - XAUTOCLAIM (Traditional Approach):** ``` XAUTOCLAIM Performance:   Average:    54.671ms   Median:     53.582ms   Min:        3.738ms   Max:        71.596ms   P95:        62.536ms   P99:        68.800ms ``` **Test 2 - XREADGROUP with CLAIM (New Approach):** ``` XREADGROUP CLAIM Performance:   Average:    2.426ms   Median:     2.571ms   Min:        1.287ms   Max:        4.653ms   P95:        3.370ms   P99:        4.212ms ``` **Performance Analysis** The new XREADGROUP CLAIM implementation delivers **22.5x faster average performance** compared to XAUTOCLAIM: - **Average latency reduction:** 95.6% (54.671ms → 2.426ms) - **Median latency reduction:** 95.2% (53.582ms → 2.571ms) - **P95 latency reduction:** 94.6% (62.536ms → 3.370ms) - **P99 latency reduction:** 93.9% (68.800ms → 4.212ms)  This performance improvement is achieved through the time-ordered PEL index (pel_by_time), which enables O(log n + k) retrieval of idle entries versus XAUTOCLAIM's less efficient scanning approach.  ### Memory Performance To evaluate the memory overhead of the pel_by_time index, comprehensive memory testing was conducted comparing Redis with and without the index under realistic workload conditions.  **Test Methodology:** - Insert 200,000 new messages into a stream - Read messages in blocks of 100 using XREADGROUP (populating the PEL with 200,000 pending entries) - Wait 5ms after each read block (simulating realistic processing delays that affect rax tree compression) - Measure memory usage before and after the reading phase  **Test Results - Without pel_by_time Index:** ``` Initial memory (used):                                   926.10 KB After insertion (used):                                    6.80 MB After reading (used):                                     41.53 MB Memory increase from data:                                 5.90 MB Memory increase from reading:                             34.72 MB Total memory increase:                                    40.62 MB ``` **Test Results - With pel_by_time Index:** ``` Initial memory (used):                                   927.44 KB After insertion (used):                                    6.81 MB After reading (used):                                     45.07 MB Memory increase from data:                                 5.90 MB Memory increase from reading:                             38.27 MB Total memory increase:                                    44.17 MB ``` **Memory Performance Analysis:** The pel_by_time index introduces a measurable but reasonable memory overhead:  **Used Memory Impact:** - Memory increase from pel_by_time index: **3.55 MB** (38.27 MB - 34.72 MB) - Per-entry overhead: **18.6 bytes** (3.55 MB / 200,000 entries) - Percentage overhead: **8.7%** increase in total memory usage  **Per-Entry Memory Breakdown:** The theoretical minimum for the pel_by_time index is 32 bytes per entry (composite key only, no node values). The observed 18.6 bytes per entry overhead is lower than the theoretical maximum, suggesting effective rax tree compression is occurring despite the 5ms delays between reads.  ## Technical Implementation  ### New Data Structure: Time-Ordered PEL Index (`pel_by_time`) To efficiently identify and claim idle pending entries, this PR introduces a new rax tree structure to the consumer group implementation: **Structure Design:**  - Tree Type: Rax tree named pel_by_time added to each consumer group - Key Composition: 32-byte composite key consisting of:   - `delivery_time` (timestamp when entry was last delivered)   - `streamId` (stream entry ID)  **Key Format:** `delivery_time` + `streamId` (concatenated) **Node Value:** None - all necessary information is encoded in the key itself for memory efficiency **Key Properties:** _Uniqueness Guarantee:_ While multiple pending entries may share the same `delivery_time`, the `streamId` component ensures each key is globally unique within the tree. _Lexicographical Ordering:_ The rax tree naturally orders nodes lexicographically by key. Since `delivery_time` forms the prefix of each key, entries are automatically sorted by delivery time, with oldest entries appearing first in the tree. _Efficient Range Operations:_ This time-based ordering enables highly efficient range searches. To find all entries idle for at least `min-idle-time` milliseconds, we simply perform a range query from the tree's beginning up to `current_time - min-idle-time`. **Fast Retrieval:**  Once idle entries are identified via the `pel_by_time` index, the embedded `streamId` in each key is used to quickly retrieve the full pending message data structure for the subsequent `XREADGROUP` claim operation. **Performance Characteristics:** - **Insertion:** O(log n) when adding entries to PEL - **Range Search:** O(log n + k) where k is the number of idle entries found - **Memory Overhead:** 32 bytes per pending entry for the index key (no additional node values stored)  This dual-index approach (existing PEL structures plus the new time-ordered index) allows XREADGROUP with CLAIM to efficiently identify claimable entries without scanning the entire PEL, making the operation suitable for consumer groups with large pending entry lists.  ### COUNT Behavior with CLAIM When the `COUNT` option is used in conjunction with `CLAIM`, the command follows a two-phase execution strategy to maximize the specified count limit: **Phase 1:** Claim Idle Pending Entries  - Retrieve claimable pending entries (idle for ≥ min-idle-time) up to the COUNT limit - These entries are claimed and returned to the consumer  **Phase 2:** Fetch New Messages (if needed)  - If the `COUNT` limit has not been satisfied by claimed pending entries, the command proceeds to read new messages from the stream - New messages are fetched up to the remaining available count: `remaining_count = COUNT - claimed_entries`  This prioritization ensures that idle pending entries are always processed first, preventing indefinite message stalling while still allowing consumers to process new messages efficiently when pending entries are scarce.   ### BLOCK Behavior with CLAIM When the CLAIM option is used in conjunction with the BLOCK option, the command exhibits sophisticated blocking behavior that responds to both new messages and pending entries becoming claimable:  **Blocking State Management:** If there are no immediately claimable pending entries and no new messages available in the stream, the `XREADGROUP` command enters a blocking state for the specified duration. However, the implementation must handle a critical scenario: pending entries that become idle (and thus claimable) while the command is blocked must trigger an early wakeup to serve those entries.  **Implementation: `stream_claim_pending_keys` Dictionary** To enable this reactive blocking behavior, a new `stream_claim_pending_keys` dictionary is introduced to the `redisDb` structure: - **Key:** Stream key being watched - **Value:** The minimum timestamp when the next pending entry in this stream will become claimable (i.e., will satisfy the min-idle-time requirement)  **Multi-Client Coordination:** When multiple XREADGROUP commands with BLOCK and CLAIM are executed concurrently on the same stream, the dictionary value stores the shortest claimable time across all waiting clients. This ensures the earliest possible wakeup when any pending entry becomes available for claiming.  **Wakeup Mechanism: `handleClaimableStreamEntries`** The `handleClaimableStreamEntries` function is invoked regularly from `blockedBeforeSleep` to monitor and react to claimable entries: 1. **Scan Phase:** Iterates through all entries in the `stream_claim_pending_keys` dictionary 2. **Time Check:** Compares each entry's claimable timestamp against the current time 3. **Signal Phase:** When `claimable_time ≤ current_time`, calls `signalKeyAsReady` to wake up all clients blocked on that stream 4. **Client Processing:** Awakened clients attempt to claim and process the newly available pending entries  **Resource Contention Handling:** When the number of claimable entries is insufficient to satisfy all awakened clients:  - Clients that successfully claim entries complete their operations - Remaining clients recalculate the next minimum claimable time based on remaining pending entries - These clients update the `stream_claim_pending_keys` dictionary with the new timestamp - They re-enter the blocking state to wait for the next batch of claimable entries  This design ensures fair resource distribution and prevents busy-waiting while maintaining responsiveness to both new messages and aging pending entries.",https://github.com/redis/redis/commit/090ca801eab073376af0acb8750a1fc49604cbae
redis/redis,https://github.com/redis/redis,aed879a,Mincho Paskalev,2025-10-21T07:32:49Z,"Optimistic locking for string objects - compare-and-set and compare-and-delete (#14435)  # Description  Add optimistic locking for string objects via compare-and-set and compare-and-delete mechanism.  ## What's changed  Introduction of new DIGEST command for string objects calculated via XXH3 hash.  Extend SET command with new parameters supporting optimistic locking. The new value is set only if checks against a given (old) value or a given string digest pass.  Introduction of new DELEX command to support conditionally deleting a key. Conditions are also checks against string value or string digest.  ## Motivation  For developers who need to to implement a compare-and-set and compare-and-delete single-key optimistic concurrency control this PR provides single-command based implementation.  Compare-and-set and compare-and-delete are mostly used for [Optimistic concurrency control](https://en.wikipedia.org/wiki/Optimistic_concurrency_control): a client (1) fetches the value, keeps the old value (or its digest, for a large string) in memory, (2) manipulates a local copy of the value, (3) applies the local changes to the server, but only if the server’s value hasn’t been changed (still equal to the old value).  Note that compare-and-set [can also be implemented](https://redis.io/docs/latest/develop/using-commands/transactions/#optimistic-locking-using-check-and-set) with WATCH … MULTI … EXEC and Lua scripts. The new SET optional arguments and the DELEX command do not enable new functionality, however, they are much simpler and faster to use for the very common use case of single-key optimistic concurrency control.  ## Related issues and PRs  https://github.com/redis/redis/issues/12485 https://github.com/redis/redis/pull/8361 https://github.com/redis/redis/pull/4258  ## Description of the new commands  ### DIGEST  ``` DIGEST key ```  Get the hash digest of the value stored in key, as an hex string.  Reply: - Null if key does not exist - error if key exists but holds a value which is not a string - (bulk string) the XXH3 digest of the value stored in key, as an hex string  ### SET  ``` SET key value [NX | XX | IFEQ match-value | IFNE match-value | IFDEQ match-digest | IFDNE match-digest] [GET] [EX seconds | PX milliseconds | EXAT unix-time-seconds | PXAT unix-time-milliseconds | KEEPTTL] ```  `IFEQ match-value` - Set the key’s value and expiration only if its current value is equal to match-value. If key doesn’t exist - it won’t be created. `IFNE match-value` - Set the key’s value and expiration only if its current value is not equal to match-value. If key doesn’t exist - it will be created. `IFDEQ match-digest` - Set the key’s value and expiration only if the digest of its current value is equal to match-digest. If key doesn’t exist - it won’t be created. `IFDNE match-digest` - Set the key’s value and expiration only if the digest of its current value is not equal to match-digest. If key doesn’t exist - it will be created.  Reply update: - If GET was not specified:    - Nil reply if either - the key doesn’t exist and XX/IFEQ/IFDEQ was specified. The key was not created. - the key exists, and NX was specified or a specified IFEQ/IFNE/IFDEQ/IFDNE condition is false. The key was not set.    - Simple string reply: OK: The key was set. - If GET was specified, any of the following: - Nil reply: The key didn't exist before this command (whether the key was created or not). - Bulk string reply: The previous value of the key (whether the key was set or not).  ### DELEX  ``` DELEX key [IFEQ match-value | IFNE match-value | IFDEQ match-digest | IFDNE match-digest] ```  Conditionally removes the specified key. A key is ignored if it does not exist.  `IFEQ match-value` - Delete the key only if its value is equal to match-value `IFNE match-value` - Delete the key only if its value is not equal to match-value `IFDEQ match-digest` - Delete the key only if the digest of its value is equal to match-digest `IFDNE match-digest` - Delete the key only if the digest of its value is not equal to match-digest  Reply:  - error if key exists but holds a value that is not a string and IFEQ/IFNE/IFDEQ/IFDNE is specified. - (integer) 0 if not deleted (the key does not exist or a specified IFEQ/IFNE/IFDEQ/IFDNE condition is false), or 1 if deleted.  ### Notes  Added copy of xxhash repo to deps - [version](https://github.com/Cyan4973/xxHash/commit/c961fbe61ad1ee1e430b9c304735a0534fda1c6d)  ---------  Co-authored-by: debing.sun <debing.sun@redis.com> Co-authored-by: Yuan Wang <wangyuancode@163.com>",https://github.com/redis/redis/commit/aed879ad0ab4b127b2273cf54b0f309cdb987d19
redis/redis,https://github.com/redis/redis,a01c061,Paulo Sousa,2025-10-18T04:20:08Z,"Optimize HyperLogLog Operations with Branchless Comparisons and ARM NEON SIMD Vectorization (#14428)  This PR introduces a comprehensive performance optimization for HyperLogLog operations through three improvements: branchless comparisons, ARM NEON-optimized merge operations, and ARM NEON-optimized compression operations.  These changes significantly accelerate both `PFCOUNT` and `PFMERGE` commands on AArch64 architectures, while also improving scalar performance on AMD64 through branchless maximum computation.  ## TL;DR Branchless comparisons and ARM NEON vectorization improve HyperLogLog `PFCOUNT` and `PFMERGE` performance by up to **8.5x** on AArch64, reducing latency by up to **88%**, with full backward compatibility and no behavioral changes.  ## Motivation HyperLogLog operations like `PFCOUNT` and `PFMERGE` are CPU-bound on large cardinality estimations due to conditional branches and scalar data processing. This PR eliminates branch mispredictions and leverages SIMD parallelism to maximize throughput on modern ARM processors.  ## Changes Overview 1. **Branchless maximum** using `MAX` macro instead of explicit conditional branches 2. **ARM NEON SIMD optimization** for `hllMergeDense()` (`PFCOUNT`)   3. **ARM NEON SIMD optimization** for `hllDenseCompress()` (`PFMERGE`)    ### Reference For more details on the original vectorized design, see Nugine’s [redis-hyperloglog repository](https://github.com/Nugine/redis-hyperloglog).",https://github.com/redis/redis/commit/a01c0618668213a69caf40e8e6ff0ce943d708d6
redis/redis,https://github.com/redis/redis,c66fbda,adrianoamaral,2025-10-16T11:43:49Z,Document Redis Query Engine and Intel SVS-VAMANA usage (#14374)  Added extra section to instruct the developers to leverage the proprietary Intel SVS binaries as an optional way during the build and with a clear license disclaimer  ---------  Co-authored-by: meiravgri <109056284+meiravgri@users.noreply.github.com>,https://github.com/redis/redis/commit/c66fbda23fa294a7710b89ad70c1aa168abcf84c
redis/redis,https://github.com/redis/redis,91b150b,Lior Kogan,2025-10-15T19:20:41Z,Update SECURITY.md (#14355)  Extended the support of 7.2 and align it with 7.4,https://github.com/redis/redis/commit/91b150bdb8977f91a7b9cc9d9546849097512a0a
redis/redis,https://github.com/redis/redis,fdf2659,Lior Kogan,2025-10-15T19:20:25Z,Ubuntu instructions fix (#14356),https://github.com/redis/redis/commit/fdf2659011c655adcc06a06254b5ec42cf01fe00
redis/redis,https://github.com/redis/redis,166ae60,debing.sun,2025-10-15T06:45:55Z,"Fix infinite loop during reverse iteration due to corrupted stream (#14423)  When the `lp_count` of a stream entry is incorrect, and we are performing a reverse iteration, we can normally move backward by `lp_count` to locate the previous entry. However, if `lp_count` is wrong, during forward iteration, after obtaining the flag, ID, and other fields, we may step beyond the current entry into the start position of the next entry(where we came from), which could cause an infinite loop.  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>",https://github.com/redis/redis/commit/166ae607346495410975da31c39280d9987095f2
redis/redis,https://github.com/redis/redis,6d89370,debing.sun,2025-10-15T06:13:40Z,"Fix shutdown blocked client not being properly reset after shutdown cancellation (#14420)  This issue was introduced by https://github.com/redis/redis/pull/10440 In that PR, we avoided resetting the current user during processCommand, but overlooked the fact that this client might not be the current one, it could be a client that was previously blocked due to shutdown. If we don’t reset these clients, and the shutdown is canceled, then when these clients continue executing other commands, they will trigger an assertion.  This PR delays the operation of resetting the client to processUnblockedClients and no longer skips SHUTDOWN_BLOCKED clients.",https://github.com/redis/redis/commit/6d89370cd67a32edbeaf048811910a90636c5bcc
redis/redis,https://github.com/redis/redis,0f39801,Filipe Oliveira (Redis),2025-10-14T10:24:12Z,Fix nested function on bitopsTest (#14430),https://github.com/redis/redis/commit/0f39801756b21464b2fade19dbe213c67b293790
redis/redis,https://github.com/redis/redis,119c83d,Filipe Oliveira (Redis),2025-10-14T00:28:18Z,"Optimize `BITCOUNT` with AVX2 and AVX512 popcount implementations.  (#14309)  This PR introduces **vectorized implementations of `BITCOUNT`** for x86_64 targets with AVX2 and AVX512 support.  - **AVX2 path**: processes 32B at a time, using unrolled POPCNT on 64-bit lanes with independent accumulators to reduce data dependencies. - **AVX512 path**: leverages `VPOPCNTDQ` on 64B chunks with `_mm512_reduce_add_epi64` to efficiently aggregate results across 512-bit vectors. - Both paths include cache prefetching hints to better overlap memory fetches with computation. This was proved to matter in https://github.com/redis/redis/pull/14103. - Fallbacks to the scalar implementation if hardware support is unavailable.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/119c83d7763f45645bfe87c2f2f73b1482bb0d45
redis/redis,https://github.com/redis/redis,5b49119,Moti Cohen,2025-10-13T09:12:38Z,"Fix crash in lookupKey() when `executing_client` is NULL (#14415)  This PR is based on: https://github.com/valkey-io/valkey/pull/2347  This was introduced in https://github.com/redis/redis/pull/13512  The server crashes with a null pointer dereference when lookupKey() is called from handleClientsBlockedOnKey(). The crash occurs because server.executing_client is NULL, but the code attempts to access server.executing_client->cmd->proc without checking.  **Crash scenario:** Client 1 enables CLIENT NO-TOUCH Client 2 blocks on BRPOP mylist 0 Client 1 executes RPUSH mylist elem When unblocking Client 2, lookupKey() dereferences NULL server.executing_client → crash  **Solution** Added proper null checks before dereferencing server.executing_client: Check if LOOKUP_NOTOUCH flag is already set before attempting to modify it Verify both server.current_client and server.executing_client are not NULL before accessing their members Maintain the TOUCH command exception for scripts  **Testing** Added regression test in tests/unit/type/list.tcl that reproduces and verifies the fix for this crash scenario.  This fix is based on valkey-io/valkey#2347  Co-authored-by: Uri Yagelnik <uriy@amazon.com> Co-authored-by: Ran Shidlansik <ranshid@amazon.com>",https://github.com/redis/redis/commit/5b49119236b51f52030b1a3ed4891fa4854ab1cf
redis/redis,https://github.com/redis/redis,083f38e,张宇杭,2025-10-12T03:02:03Z,"Fix issues with server.allow_access_expired (#14262)  Close https://github.com/redis/redis/issues/14214  1. When the server.allow_access_expired flag is set to 1, it allows access to expired keys that have not yet been evicted. All places involving access to expired keys should consider the impact of this parameter. 2. The modifications involve five methods: hfieldIsExpired, hashTypeNext, hashTypeLength, keyIsExpired, and hashTypeIsExpired. When the server.allow_access_expired flag is set to 1, these methods will not skip expired keys, otherwise they follow the normal logic execution.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/083f38ef5a2620b8ee947b5d9b2357c86d44498b
redis/redis,https://github.com/redis/redis,791a5a7,Polaris-911,2025-10-11T07:40:58Z,"enable unaligned access optimizations for RISC-V with Zicclsm​ (#14342)  While reviewing the Redis code, we discovered that optimizations for unaligned memory accesses are not enabled on RISC-V. After testing siphash separately, we found some performance improvements in this area and hope to add them.  The zicclsm extension, which includes unaligned memory accesses, is required on RISC-V, as specified in the [RVA20U64 specification](https://github.com/riscv/riscv-profiles/blob/5879c13c924ec5636995c5883f40337e83f6049a/src/profiles.adoc#L658). GCC also provides the macro [zicclsm](https://gcc.gnu.org/pipermail/gcc-patches/2024-February/644655.html) to detect the zicclsm extension.  Supported versions: GCC 14.1.0 and above",https://github.com/redis/redis/commit/791a5a75970b5c501f1152e0ea447c1b0c37698f
redis/redis,https://github.com/redis/redis,df28ad3,debing.sun,2025-10-10T11:27:07Z,Add VRANGE to ignored commands for schemas validator (#14426)  In https://github.com/redis/redis/pull/14235 we forgot to skip vset module command `VRANGE` for schema validator.,https://github.com/redis/redis/commit/df28ad35e8b8b3ec5647cf30b05ea0a5cb64825c
redis/redis,https://github.com/redis/redis,133989e,Slavomir Kaslev,2025-10-09T16:59:53Z,"Fix test ""CLUSTER FORGET with invalid node ID"" (#14421)  Fix typo in test ""CLUSTER FORGET with invalid node ID"".",https://github.com/redis/redis/commit/133989ef0fae2c1541bd36a3b08f8bdee2ba0926
redis/redis,https://github.com/redis/redis,48d0aa9,Moti Cohen,2025-10-09T10:22:42Z,"Fix heap-buffer-overflow in CLUSTER FORGET with invalid node ID length (#14417)  Fix a heap-buffer-overflow vulnerability in the `CLUSTER FORGET` command when provided with a node ID shorter than the expected 40 bytes. When `CLUSTER FORGET` is called with a node ID that has a length smaller than `CLUSTER_NAMELEN` (40 bytes), the `clusterBlacklistExists()` function would read beyond the allocated string buffer. This occurs because the function always attempted to read exactly 40 bytes via `sdsnewlen(nodeid, CLUSTER_NAMELEN)`.  Changes: - Added a `size_t len` parameter to `clusterBlacklistExists()` to use the actual string length - Updated all call sites to pass the appropriate length - Added a test case to verify the fix - Test added to verify that `CLUSTER FORGET` with an invalid short node ID returns an appropriate error.  This PR is based on: valkey-io/valkey#2108 Co-authored-by: Ran Shidlansik <ranshid@amazon.com>",https://github.com/redis/redis/commit/48d0aa9d0aa098a11aab21d27161da32e3612216
redis/redis,https://github.com/redis/redis,1f2c285,Filipe Oliveira (Redis),2025-10-09T07:35:54Z,"Optimize arm `BITCOUNT` popcount loop using 4-lane NEON accumulation (#14277)  This PR optimizes the AArch64 implementation of `BITCOUNT` by replacing the scalar byte-by-byte accumulation with a **4-lane NEON vectorized loop** that amortizes reductions over 128B chunks. This reduces instruction count, improves cache utilization, and better balances ILP (instruction-level parallelism) without creating excessive register pressure.  We evaluated **2-lane**, **4-lane**, and **8-lane** variants. While 8-lane achieved marginally higher IPC, the gains in ops/sec were negligible and frontend stalls increased. 4-lane provided the best balance between throughput, latency, and hardware utilization.  ---  ## Key Observations from `perf` Profiles  1. **Baseline** was *not* memory-bound — very low L1/L2 refills, extremely high instruction count, and low stalls. Performance was dominated by scalar bit-counting overhead. 2. **Optimized variants** reduced instruction count by up to ~60%, exposing the memory subsystem as the bottleneck:    - L1D cache refills: +100× increase vs baseline   - Backend stalls: ~57% of total cycles (typical signature of memory-bound workloads). - we've confirmed this by the profile data, in which the majority of the cycles are spent on loads. <img width=""359"" height=""489"" alt=""image"" src=""https://github.com/user-attachments/assets/ae2729cd-dc95-4300-b5aa-b232cbeaa6b3"" /> 3. Why 4-lane Wins? strikes the right balance — enough parallelism to hide some latency, but not so wide that we incur excessive frontend stalls or instruction bloat. - Significant throughput gain over 2-lane (+8.2% ops/sec, −7% latency). - Marginal gain from 8-lane (<0.4% ops/sec) does not justify added complexity and increased frontend stalls. - Lower combined cache refill overhead compared to 8-lane for similar throughput.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/1f2c28501c1f4cc4500b48092d11a739a85cb508
redis/redis,https://github.com/redis/redis,3de2fda,Salvatore Sanfilippo,2025-10-09T02:14:14Z,"[vector sets] VRANGE implementation (#14235)  This is basically the Vector Set iteration primitive. It exploits the underlying radix tree implementation. The usage pattern is strongly reminiscent of other Redis commands doing similar things.  The command usage is straightforward:  ``` > VRANGE word_embeddings_int8 [Redis + 10  1) ""Redis""  2) ""Rediscover""  3) ""Rediscover_Ashland""  4) ""Rediscover_Northern_Ireland""  5) ""Rediscovered""  6) ""Rediscovered_Bookshop""  7) ""Rediscovering""  8) ""Rediscovering_God""  9) ""Rediscovering_Lost"" 10) ""Rediscovers"" ```  The above command returns 10 (or less, if less are available in the specified range) elements from ""Redis"" (inclusive) to the maximum possible element. The comparison is performed byte by byte, as `memcmp()` would do, in this way the elements have a total order. The start and end range can be either a string, prefixed by `[` or `(` (the prefix is mandatory) to tell the command if the range is inclusive or exclusive, or can be the special symbols `-` and `+` that means the maximum and minimum element.  More info can be found in the implementation itself and in the README file change.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/3de2fdad58cf332fa4050b3b72c25eacd22b2b5c
redis/redis,https://github.com/redis/redis,3c1a759,Slavomir Kaslev,2025-10-08T07:36:28Z,Move cluster stats state out of cluster_legacy.c (#14409)  In certain build configurations cluster_legacy.c is not being linked in.  To handle this cluster slots stats state is referenced from the global `server` state instead.,https://github.com/redis/redis/commit/3c1a7599542ee0598a2f803f2399999f6ce9b975
redis/redis,https://github.com/redis/redis,97df5b5,Joan Fontanals,2025-10-05T08:15:24Z,[RED-170693] Limit efSearch parameter to avoid heap overflow (#14373)  This PR aims to avoid the situation of a potential crash when efSearch is too large (and therefore the memory allocated could lead to a server crash or an integer overflow (where less memory is allocated than expected).  - Limit the accepted EF in the request o 100_000 as in VADD - Limit the ef search to the number of nodes in the HNSW graph,https://github.com/redis/redis/commit/97df5b5411d1db2f7eb2d5394f9097c8f6abaf8b
redis/redis,https://github.com/redis/redis,161130f,Tom Gabsow,2025-09-30T17:06:17Z,Update json version 8.3.80 (#14399),https://github.com/redis/redis/commit/161130f412c38d43ca64050460f93918afdd485f
redis/redis,https://github.com/redis/redis,2cd4afb,Tom Gabsow,2025-09-30T17:05:40Z,Update bloom v8.3.80 (#14398),https://github.com/redis/redis/commit/2cd4afb39aa5c7e8b607dab3f9d82250a7e97575
redis/redis,https://github.com/redis/redis,77c995d,Tom Gabsow,2025-09-30T17:05:04Z,Update time series v8.3.80 (#14397),https://github.com/redis/redis/commit/77c995dcbdd33ad1a695fed3dab75de26ec4e9ea
redis/redis,https://github.com/redis/redis,e0bb2a3,Stav-Levi,2025-09-28T14:09:50Z,Remove unnecessary skip:external for module tests (#14354)  This PR follows: https://github.com/redis/redis/pull/14226 this pr goal is to remove unnecessary skip:external,https://github.com/redis/redis/commit/e0bb2a31d4de5b8ac9798df84af87113425db8fb
redis/redis,https://github.com/redis/redis,b1eb9ba,h.o.t. neglected,2025-09-28T05:28:22Z,Change `ps` command options to work on Solaris (#14351)  Fix https://github.com/redis/redis/issues/14304 `ps` command options in tcl tests are adjusted to work on both Linux and Solaris.,https://github.com/redis/redis/commit/b1eb9ba861591463bbf37b12021dbe6d8382b622
redis/redis,https://github.com/redis/redis,acbcaae,Rushabh Mehta,2025-09-19T03:54:34Z,"Fix potential division by zero in LRU test mode in redis-cli (#14369)  This PR resolves a potential division-by-zero issue in the `redis-cli` LRU test mode (`--lru-test`), as reported by the Linux Verification Center.  Fixes #14361",https://github.com/redis/redis/commit/acbcaae530035aab5480756e8e29e075398ea931
redis/redis,https://github.com/redis/redis,e8726d1,Rushabh Mehta,2025-09-15T01:33:18Z,"[vector sets] Add --ollama-url option to configure embedding server (#14317)  This PR adds a `--ollama-url` option to `cli.py`, the lightweight redis-cli-like tool that expands !""text"" arguments into embeddings via Ollama.  Previously, the embedding call was hardcoded to http://localhost:11434/api/embeddings. With this change, users can specify a custom Ollama server URL when starting the tool.  If no URL is provided, the tool defaults to what it was before.",https://github.com/redis/redis/commit/e8726d18e5bab24cbfcb0a0c36f21ce5a1140471
redis/redis,https://github.com/redis/redis,9b63e99,Moti Cohen,2025-09-11T13:45:17Z,"Refactor HFE: Introduce Per-Slot Expiration Store (estore) (#14294)  Hash field expiration is managed with two levels of data structures.  1. At the DB level, an ebuckets structure maintains the set of all hashes that contain fields with expiration. 2. At the per-hash level, an ebuckets structure tracks fields with expiration.  This pull request refactors the 1st level to operate per slot instead, and introduces a new API called estore (expiration store). Its design aligns closely with the existing kvstore API, ensuring consistency and simplifying usage. The terminology at that level has been updated from “HFE” or “hexpire” to “subexpiry”, reflecting a broader scope that can later support other data types.",https://github.com/redis/redis/commit/9b63e99d05118a337b701becf5e0e688e833515e
redis/redis,https://github.com/redis/redis,fb32174,debing.sun,2025-09-11T12:59:47Z,"Fix Active Defrag HFE with large_ebrax test (#14344)  From the malloc-stats reports of both failures and successes, we can see that the additional fragments mainly come from bin24. By analyzing the fragments mainly from the entries of the dict, since `large_ebrax` test uses a dictionary with 1600 elements, it will move a large number of entries during the rehashing process, and we will not perform defragmentation on the dict entries.  In https://github.com/redis/redis/pull/13842 we changed to use two dicts alternately to generate frag. Normally, the entries should also alternate, but rehashing disrupted this, which resulted in bin24 frag that can't be defragged.  ## Solution In this PR, the length of a single dictionary was reduced from 1600 to 500 to avoid excessive rehashing, and the threshold was also lowered.  ---------  Co-authored-by: oranagra <oran@redislabs.com>",https://github.com/redis/redis/commit/fb32174b0d6bc4962fed4cc1dfd69fd25d0943e2
redis/redis,https://github.com/redis/redis,64f63bf,h.o.t. neglected,2025-09-11T03:49:42Z,Fix help docs of vector-set commands (#14192)  Fix https://github.com/redis/redis/issues/14026 Use `RedisModule_SetCommandInfo` to add help info for vector-set commands.,https://github.com/redis/redis/commit/64f63bfcb017aa06dd54bec9f8dec3522e06b381
redis/redis,https://github.com/redis/redis,60adba4,debing.sun,2025-09-10T04:52:20Z,"Introduce DEBUG_DEFRAG compilation option to allow run test with activedefrag when allocator is not jemalloc (#14326)  This PR is based on https://github.com/valkey-io/valkey/pull/1303  This PR introduces a DEBUG_DEFRAG compilation option that enables activedefrag functionality even when the allocator is not jemalloc, and always forces defragmentation regardless of the amount or ratio of fragmentation.  ## Using ``` make SANITIZER=address DEBUG_DEFRAG=<force|fully> ./runtest --debug-defrag ```  * DEBUG_DEFRAG=force    * Ignore the threshold for defragmentation to ensure that defragmentation is always triggered.    * Always reallocate pointers to probe for correctness issues in pointer reallocation.  * DEBUG_DEFRAG=fully    * Includes everything in the option `force`.    * Additionally performs a full defrag on every defrag cycle, which is significantly slower but more accurate.  ---------  Co-authored-by: Ran Shidlansik <ranshid@amazon.com> Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com> Co-authored-by: oranagra <oran@redislabs.com>",https://github.com/redis/redis/commit/60adba48aa122427dc7ae50e981cff3c84b46954
redis/redis,https://github.com/redis/redis,d649637,debing.sun,2025-09-08T09:52:30Z,"Fix defrag issues for stream defrag and HFE (#14323)  This PR fixes three defrag issues.  1. Fix the issue that forget to update cgroup_ref_node when the consume group was reallocated. This crash was introduced by https://github.com/redis/redis/issues/14130 In this PR, when performing defragmentation on `s->cgroups` using `defragRadixTree()`, we no longer rely on the automatic data defragmentation of `defragRadixTree()`. Instead, we manually defragment the consumer group and then update its reference in `s->cgroups`.  2. Fix a use-after-free issue caused by updating dictionary keys after HFE key is reallocated. This issue was introduced by https://github.com/redis/redis/issues/13842  3. Fix the issue that forgot to be updated NextSegHdr->firstSeg when the first segment was reallocated. This issue was introduced by https://github.com/redis/redis/issues/13842  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>",https://github.com/redis/redis/commit/d649637ebd1b7b7792a4224a42e1ef8824743378
redis/redis,https://github.com/redis/redis,dee0d11,Oran Agra,2025-09-07T13:56:38Z,"Avoid OOM error on very big args due to allocation of very big query buffer (#14327)  We have code to assume that if we're facing a big argument, then the next argument is likely to be very big too, so we allocate another huge query buffer. This will backfire and return OOM error on any command with an argument that's larger than half the memory limit (even if the db completely empty). To mitigate that, we reserve query buffer for another big argument, only if that big argument is less than 1/30 of the memory limit.",https://github.com/redis/redis/commit/dee0d11a74dbb8efcec8148bd2ff43d203b48931
redis/redis,https://github.com/redis/redis,d339fe7,debing.sun,2025-09-07T09:07:48Z,Fix defrag issues for pubsub and lua (#14330)  This PR fixes two defrag issues.  1. Fix a use-after-free issue caused by updating dictionary keys after a pubsub channel is reallocated. This issue was introduced by https://github.com/redis/redis/pull/13058  1. Fix potential use-after-free for lua during AOF loading with defrag This issue was introduced by https://github.com/redis/redis/issues/13058    This fix follows https://github.com/redis/redis/pull/14319 This PR updates the LuaScript LRU list before script execution to prevent accessing a potentially invalidated pointer after long-running scripts.,https://github.com/redis/redis/commit/d339fe70adc335dcf8b0e03cbb52bcd0263afcd5
redis/redis,https://github.com/redis/redis,64546d2,debing.sun,2025-09-05T06:49:19Z,"Fix timing issue for sentinel master-reboot test (#14312)  From the following logs, if we are in a slow environment, the election process of sentinels may become very slow. Even if the master instance that was restarted and is slowly loading RDB has already been loaded, the election just gets started.  This PR makes the master load the RDB more slowly, and fixes the missing of reseting reset `key-load-delay` for the master node.",https://github.com/redis/redis/commit/64546d20093b585143593e3728727164855fd64a
redis/redis,https://github.com/redis/redis,34da5b7,Stav-Levi,2025-09-04T10:22:13Z,Add modules to make test (#14333)  This PR follows https://github.com/redis/redis/pull/14226. make test fails on fresh checkout because test modules are not built by default when running tests. error:  [exception]: Executing test client: ERR Error loading the extension. Please check the server logs.. ERR Error loading the extension. Please check the server logs. solution: Add module_tests to the test target dependencies:,https://github.com/redis/redis/commit/34da5b786027b7c03d87eb6f4e8e7329a7643220
redis/redis,https://github.com/redis/redis,d816a65,debing.sun,2025-09-03T12:48:23Z,"Fix module_tests dependency to resolve parallel build issues (#14328)  This PR follows https://github.com/redis/redis/issues/14226. When using parallel compilation with `make -j`, the `module_tests` target and `REDIS_SERVER_NAME` may compile concurrently, leading to build failures. This appears to be caused by both targets having a shared dependency on `redismodule.h`, creating a race condition during parallel execution.  Solution: Add explicit dependency of `module_tests` on `$(REDIS_SERVER_NAME)` to enforce build order.",https://github.com/redis/redis/commit/d816a6591b43181f5bedc88b9e918ddce64aadb8
redis/redis,https://github.com/redis/redis,8ad5421,debing.sun,2025-09-02T02:51:45Z,"Fix crash during lua script defrag (#14319)  This PR fixes two crashes due to the defragmentation of the Lua script, which were by https://github.com/redis/redis/pull/13108  1. During long-running Lua script execution, active defragmentation may be triggered, causing the luaScript structure to be reallocated to a new memory location, then we access `l->node`(may be reallocatedd) after script execution to update the Lua LRU list. In this PR, we don't defrag during blocked scripts, so we don't mess up the LRU update when the script ends.    Note that defrag is now only permitted during loading. This PR also reverts the changes made by https://github.com/redis/redis/pull/14274.  2. Forgot to update the Lua LUR list node's value. Since `lua_scripts_lru_list` node stores a pointer to the `lua_script`'s key, we also need to update `node->value` when the key is reallocated. In this PR, after performing defragmentation on a Lua script, if the script is in the LRU list, its reference in the LRU list will be unconditionally updated.",https://github.com/redis/redis/commit/8ad5421502241d6088b701bb4a4262124343345a
redis/redis,https://github.com/redis/redis,5f8e785,Giuseppe Coco,2025-09-01T00:36:38Z,"Fix: Validate ENTRIESREAD in XGROUP command (#14259)  Fixes #14257  The XGROUP CREATE and SETID subcommands allowed setting an ENTRIESREAD value greater than the stream's total `entries_added` counter. This could lead to a logically inconsistent state.  This commit adds a check to ensure the provided ENTRIESREAD value is not greater than the number of entries ever added to the stream. If ENTRIESREAD is too large, it gets set to the total number of entries in the stream, i.e. `s->entries_added`.",https://github.com/redis/redis/commit/5f8e7852f4e66c744122041cfe292ea4896909fc
redis/redis,https://github.com/redis/redis,38d16a8,Vitah Lin,2025-08-28T13:17:55Z,"Fix missing prev update in hnsw_cursor_free causing unlink failure (#14050)  This PR fixes a bug in the `hnsw_cursor_free` function where the prev pointer was never updated during cursor list traversal. As a result, if the cursor being freed was not the head of the list, it would not be correctly unlinked, potentially causing memory leaks or corruption of the cursor list.  Note that since `hnsw_cursor_free()` is never used for now, this PR does not actually fix any bug.",https://github.com/redis/redis/commit/38d16a82eb4a8b0e393b51cc3e9144dc7b413fd1
redis/redis,https://github.com/redis/redis,e2b8f8f,debing.sun,2025-08-27T14:25:32Z,"Fix timing issue for module defrag test (#14305)  These two tests often fail in the slow environment. 1. `Module defrag: late defrag with cursor works` test `defragtest_datatype_resumes` in a defrag cycle does not always reach 10 times, so increase the threshold and move the assertion of `defragtest_datatype_resumes` to `wait_for_condition`.  2. `Module defrag: global defrag works` test      Increase the waiting time for this test.",https://github.com/redis/redis/commit/e2b8f8ff6d00f6aab52cd2e18e71405db2e03f68
redis/redis,https://github.com/redis/redis,35aacdf,debing.sun,2025-08-27T03:11:59Z,"Fix some flakiness tests (#14303)  This PR mainly fixes two flakiness tests. 1. Fix the failure of `Active Defrag HFE with ebrax` test in `memefficiency.tcl` When `redisObject` structure size changes, the current test design becomes flakiness: In the current test, we will create 1 hash key + N string keys. When we delete this string key, these hash keys may be evenly distributed in robj 's slabs, resulting in the inability to perform defragmentation.  2. Fix `bulk reply protocol` test in `protocol.tcl` introduced by https://github.com/redis/redis/pull/13711 When `OBJ_ENCODING_EMBSTR_SIZE_LIMIT` (currently 44) changes, it can cause this test to fail. This isn't necessarily a problem, but the main issue is that we use `rawread` to verify encoding correctness. If the reply length doesn't match exactly, it can cause the test to hang and become difficult to debug.",https://github.com/redis/redis/commit/35aacdf80a0871c933047fc46655b98a73a9374e
redis/redis,https://github.com/redis/redis,81df8de,Stav-Levi,2025-08-26T11:49:05Z,Run module tests as part of the base redis testsuit (#14226)  integrate module API tests into default test suite - Add module_tests target to main Makefile to build test modules - Include unit/moduleapi in test_dirs to run module tests with ./runtest - Module API tests now run by default instead of requiring runtest-moduleapi  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>,https://github.com/redis/redis/commit/81df8deca614e46cb113974dbac0b9ec3e6724e7
redis/redis,https://github.com/redis/redis,c333dbd,Moti Cohen,2025-08-25T12:58:02Z,"dictEntry and dictEntryNoValue common layout (#14264)  Refactor `dictEntryNoValue` to remove the need for a per-entry bit (`ENTRY_PTR_NO_VALUE`) indicating the absence of a value. By aligning `dictEntry` and `dictEntryNoValue` so that the key and next fields share the same layout, we can reuse common code paths. This precious bit can be leveraged for other stuff.  The trade-off is that callers must now be more careful not to call `dictSetVal()` or `dictGetVal()` on dictionaries that only store keys, since we can no longer catch this with an assertion. However, this limitation is manageable.",https://github.com/redis/redis/commit/c333dbd9a307e95aed132b75daad2c856268d063
redis/redis,https://github.com/redis/redis,823de3f,debing.sun,2025-08-25T07:13:11Z,"Add support to decode json empty array as lua array (#14296)  ## Summary This PR adds a new configuration option `decode_array_with_array_mt` to lua_cjson that allows users to control how empty JSON arrays are handled during encoding/decoding.  ## Problem Currently, lua_cjson has an ambiguity when handling empty tables: - When decoding an empty JSON array `[]`, it becomes an empty Lua table. This is mainly because both {} (object) and [] (array) are represented as tables. The Lua cjson library then decides whether to encode a table as a JSON object or as a JSON array, depending on its length. If the length is not 0, it becomes a JSON array; otherwise, it is treated as a JSON object.  ## Solution Added a new configuration option `decode_array_with_array_mt` (default: `false` for backward compatibility):  - **When `false` (default)**: Maintains current behavior - empty arrays decode to Lua table - **When `true`**: Empty JSON arrays decode to tables with a special metatable marker `__is_cjson_array`  ```lua -- Usage Example -- Default behavior without decode_array_with_array_mt  (backward compatible) local arr = cjson.decode(""[]"")  -- plain table {} cjson.encode(arr)  -- produces ""{}""  -- Default behavior (backward compatible) cjson.decode_array_with_array_mt(false) local arr = cjson.decode(""[]"")  -- plain table {} cjson.encode(arr)  -- produces ""{}""  -- New behavior cjson.decode_array_with_array_mt(true) local arr = cjson.decode(""[]"")  -- table with __is_cjson_array metatable cjson.encode(arr)  -- produces ""[]"" ```  ## Note this new Lua cjson API(decode_array_with_array_mt) references from https://github.com/openresty/lua-cjson  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com> Co-authored-by: Ozan Tezcan <ozantezcan@gmail.com>",https://github.com/redis/redis/commit/823de3f191e3768848940daf6131da64289d18ba
redis/redis,https://github.com/redis/redis,67fa237,debing.sun,2025-08-25T02:33:52Z,Reduce the overhead for calculating jemalloc small bins frag (#14299)  ## Summary This PR optimizes the performance of `zmalloc_get_frag_smallbins_by_arena()` by eliminating the overhead of `snprintf()` and string parsing in the hot loop when calculating jemalloc small bins frag bytes.  ## Solution  Replaced je_mallctl() calls with je_mallctlbymib(). This approach has two benefits: 1. Avoid the overhead from `snprintf`. 2. Reduces the overhead that jemalloc needs to parse parameters from the parameter.,https://github.com/redis/redis/commit/67fa23741053a3c57cf9d66a36c591e8e20fbab3
redis/redis,https://github.com/redis/redis,027d98e,Ted Lyngmo,2025-08-21T14:31:28Z,Fix assumptions that pthread functions set errno (#14284)  pthread functions return the errno instead of setting errno.  Signed-off-by: Ted Lyngmo <ted@lyncon.se>,https://github.com/redis/redis/commit/027d98e9bd36e2855da1ad6e60c7098d3679e60d
redis/redis,https://github.com/redis/redis,e6c261f,Moti Cohen,2025-08-20T10:54:45Z,"Fix MEMORY USAGE command (#14288)  After the key-value unification (kvobj), the MEMORY USAGE command may no longer account for the embedded key length stored within the kvobj. To fix this, replace sizeof(*o) with zmalloc_size((void *)o) to ensure the full allocated size is measured.  In this context, the function objectComputeSize() was renamed and modified to kvobjComputeSize(). From computing only the value size to compute the key and its value.",https://github.com/redis/redis/commit/e6c261f3fb77fe6325f232a96c4bef87671d3a3a
redis/redis,https://github.com/redis/redis,ca9ede6,guybe7,2025-08-19T23:32:24Z,"Expose ""touches-arbitrary-keys"" flag to Redis modules (#14290)  This commit adds support for the ""touches-arbitrary-keys"" command flag in Redis modules, allowing module commands to be properly marked when they modify keys not explicitly provided as arguments, to avoid wrapping replicated commands with MULTI/EXEC.  Changes: - Added ""touches-arbitrary-keys"" flag parsing in commandFlagsFromString() - Updated module command documentation to describe the new flag - Added test implementation in zset module with zset.delall command to demonstrate and verify the flag functionality  The zset.delall command serves as a test case that scans the keyspace and deletes all zset-type keys, properly using the new flag since it modifies keys not provided via argv.  This commit adds a new `zset.delall` command to the zset test module that iterates through the keyspace and deletes all keys of type ""zset"".  Key changes: - Added zset_delall() function that uses RedisModule_Scan to iterate through all keys in the keyspace - Added zset_delall_callback() that checks each key's type and deletes zset keys using RedisModule_Call with ""DEL"" command - Registered the new command with ""write touches-arbitrary-keys"" flags since it modifies arbitrary keys not provided via argv - Added support for ""touches-arbitrary-keys"" flag in module command parsing - Added comprehensive tests for the new functionality  The command returns the number of deleted zset keys and properly handles replication by using the ""s!"" format specifier with RedisModule_Call to ensure DEL commands are replicated to slaves and AOF.  Usage: ZSET.DELALL Returns: Integer count of deleted zset keys",https://github.com/redis/redis/commit/ca9ede696857369503be76b7bdbdcec50b9c7af5
redis/redis,https://github.com/redis/redis,b9d9d40,debing.sun,2025-08-15T07:15:16Z,"Prevent crash when cgroups_ref is null in streamEntryIsReferenced() after reload (#14276)  This bug was introduced by https://github.com/redis/redis/pull/14130 found by @oranagra  ### Summary  Because `s->cgroup_ref` is created at runtime the first time a consumer group is linked with a message, but it is not released when all references are removed.  However, after `debug reload` or restart, if the PEL is empty (meaning no consumer group is referencing any message), `s->cgroup_ref` will not be recreated.  As a result, when executing XADD or XTRIM with `ACKED` option and checking whether a message that is being read but has not been ACKed can be deleted, the cgroup_ref being NULL will cause a crash.  ### Code Path ``` xaddCommand -> streamTrim -> streamEntryIsReferenced ```  ### Solution  Check if `s->cgroup_ref` is NULL in streamEntryIsReferenced().",https://github.com/redis/redis/commit/b9d9d4000b0b45b87c7c6fea23ec9fd8fcac107e
redis/redis,https://github.com/redis/redis,46a3efa,debing.sun,2025-08-15T03:25:03Z,"Prevent active defrag from triggering during replicaof db flush (#14274)  Fix https://github.com/redis/redis/issues/14267 This bug was introduced by https://github.com/redis/redis/pull/13495  ### Summary  When a replica clears a large database, it periodically calls processEventsWhileBlocked() in the replicationEmptyDbCallback() callback during the key deletion process. If defragmentation is enabled, this means that active defrag can be triggered while the database is being deleted. The defragmentation process may also modify the database at this time, which could lead to crashes when the database is accessed after defragmentation.  Code Path: ``` replicationEmptyDbCallback() -> processEventsWhileBlocked() -> whileBlockedCron() -> defragWhileBlocked() ```  ### Solution  This PR temporarily disables active defrag before emptying the database, then restores the active defrag setting after the empty is complete.  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>",https://github.com/redis/redis/commit/46a3efa75065c909053c969519098cff3e16a022
redis/redis,https://github.com/redis/redis,51f364f,mengxun,2025-08-14T03:34:36Z,Fix typo in thread status file path comment (#14273),https://github.com/redis/redis/commit/51f364f70035deb3734df15aca317ffc0da11594
redis/redis,https://github.com/redis/redis,2bf275f,huangzheng,2025-08-14T01:45:25Z,"Add USE_PROCESSOR_CLOCK for RISC-V to improve monotonic clock support (#14251)  ### Description This PR introduces support for using the processor clock (USE_PROCESSOR_CLOCK) for the RISC-V architecture in the `src/monotonic.c`. The change adds conditional compilation code enabling Redis to utilize the RISC-V processor clock for monotonic timing, improving time measurement accuracy and consistency on RISC-V platforms.  ### Motivation Currently, Redis's monotonic clock implementation lacks explicit handling for RISC-V processor clocks, adding USE_PROCESSOR_CLOCK support helps Redis better leverage hardware capabilities on RISC-V, enhancing portability and performance.  ### Changes  - Added `USE_PROCESSOR_CLOCK` macro and related support code guarded by RISC-V specific macros in `src/monotonic.c`. - No existing functionality is changed for other architectures.  ### Testing - Build success, redis-server and redis-benchmark run all well on **Sophgo SG2042 RISC-V CPU**. - It is not easy to test the performance improvement brought by `USE_PROCESSOR_CLOCK` using redis-benchmark, so we wrote a micro-benchmark `monotonic_bench.c` test on RISC-V.  From `monotonic_bench` on **Sophgo SG2042 RISC-V CPU**, we can see that the RISC-V processor clock implementation is approximately **2.78 times faster** than the POSIX monotonic clock method on this platform.  ### Notes - No impact on existing Redis platforms. Enables improved timing for RISC-V users which is critical for latency measurements, timeouts, and other internal Redis timing logic. - This change aligns with Redis’s strategy of supporting diverse hardware platforms with minimal footprint, and it preserves backward compatibility with existing code. - To use `USE_PROCESSOR_CLOCK` on RISC-V, complie redis with `make CFLAGS=""-DUSE_PROCESSOR_CLOCK""`.  Signed-off-by: Huang Zheng <huang.zheng@sanechips.com.cn>",https://github.com/redis/redis/commit/2bf275f9485f1d7c58d0154f621b7a3a3a37f685
redis/redis,https://github.com/redis/redis,b99ca67,alonre24,2025-08-13T19:26:02Z,"RediSearch v8.2.1 (#14269)  * Protect cursors that are running in the background for FT.AGGREGATE command while running FLUSHDB and avoid server crash - https://github.com/RediSearch/RediSearch/pull/6601 * Fix performance regression in `info` command upon computing search indexes memory due to a change in Trie data structure implementation, and having it in O(1) again - https://github.com/RediSearch/RediSearch/pull/6621",https://github.com/redis/redis/commit/b99ca67ec3942dfd06e24375787fc04dba9771fb
redis/redis,https://github.com/redis/redis,eac4827,Stav-Levi,2025-08-12T07:55:16Z,"Add auto-repair options for broken AOF tail on startup (#14058)  When Redis is shut down uncleanly (e.g., due to power loss), invalid bytes may remain at the end of the AOF file. Currently, Redis detects such corruption only after parsing most of the AOF, leading to delayed error detection and increased downtime. Manual recovery via `redis-check-aof --fix` is also time-consuming.  This fix introduces two new options to improve resilience and reduce downtime:  - `aof-load-broken`: Enables automatic detection and repair of broken AOF tails. - `aof-load-broken-max-size`: Sets a maximum threshold (in bytes) for the corrupted tail size that Redis will attempt to fix automatically without requiring user intervention.",https://github.com/redis/redis/commit/eac48279ad21b8612038953fefa0dcf926773efc
redis/redis,https://github.com/redis/redis,674b829,Salvatore Sanfilippo,2025-08-12T03:45:35Z,"VSIM EPSILON fixes (#14223)  Hi, this PR implements the following changes:  1. The EPSILON option of VSIM is now documented. 2. The EPSILON behavior was fixed: the score was incorrectly divided by two in the meaning, with a 0-2 interval provided by the underlying cosine similarity, instead of the 0-1 interval. So an EPSILON of 0.2 only returned elements with a distance between 1 and 0.9 instead of 1 and 0.8. This is a *breaking change* but the command was not documented so far, and it is a fix, as the user sees the similarity score so was a total mismatch. I believe this fix should definitely be back ported as soon as possible. 3. There are now tests.  Thanks for checking, Salvatore",https://github.com/redis/redis/commit/674b829981c0b8ad15a670a32df503e0e4514e96
redis/redis,https://github.com/redis/redis,e03a458,Ozan Tezcan,2025-08-11T06:26:05Z,"Fix flaky repl test by increasing key count to capture LOADING reply (#14265)  Increased replica key count to make flush take longer, ensuring the test catches the `-LOADING` reply reliably.",https://github.com/redis/redis/commit/e03a4584fab0c90b6b1a3be536a72e3683d2abba
redis/redis,https://github.com/redis/redis,3fa7a65,h.o.t. neglected,2025-08-06T07:09:08Z,"Fix memory leak in RM_GetCommandKeysWithFlags (#14243)  Fix https://github.com/redis/redis/issues/14208 As mentioned in the above issue, RM_GetCommandKeysWithFlags could have memory leak when the number of keys is larger than MAX_KEYS_BUFFER. This PR fixes it by calling getKeysFreeResult before the function's return. A TCL testcase is created to verify the fix.",https://github.com/redis/redis/commit/3fa7a656f19d909a11dfba2bce0f507c2a73cca1
redis/redis,https://github.com/redis/redis,7b40dba,Stav-Levi,2025-08-06T07:07:30Z,Fix timing issue for correct replication disconnection time counters behavior test (#14221)   Co-authored-by: debing.sun <debing.sun@redis.com>,https://github.com/redis/redis/commit/7b40dbacdf96e302fec482588bf20a535733ab2b
redis/redis,https://github.com/redis/redis,0ac04bb,Chris Lamb,2025-08-06T07:04:47Z,"Add ""Redis ver. $REDIS_VERSION"" to LOLWUT 8 output (#14195)  eg. python-redis (https://github.com/redis/redis-py/blob/master/tests/test_commands.py#L1092)  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/0ac04bbd1426c5cb491b54b0f1a426423815edd3
redis/redis,https://github.com/redis/redis,ecc3cbf,Slavomir Kaslev,2025-08-04T11:42:30Z,Move getSlotOrReply() to cluster.c (#14253)  getSlotOrReply() is used by the `CLUSTER SLOT-STATS` command but is defined in cluster_legacy.c which might not be present in all build configurations.,https://github.com/redis/redis/commit/ecc3cbfde8c8c6e865b897d37fc1fd481397f069
redis/redis,https://github.com/redis/redis,ff2f0b0,kei-nan,2025-08-01T03:53:56Z,Avoid Accessing Arguments Out Of Bounds In handleDebugClusterCommand (#14242)  Noticed we assume there are at least 3 arguments since we access to index 2 in the if and only later check the argc. Moved the argc check to the start of the if so the code will be a bit safer.,https://github.com/redis/redis/commit/ff2f0b092c24d5cc590ff1eb596fc0865e0fb721
redis/redis,https://github.com/redis/redis,c55e33a,Moti Cohen,2025-07-31T12:44:25Z,"KEYSIZES - Fix resolving key slot on modules (#14240)  In cluster mode with modules, for a given key, the slot resolution for the KEYSIZES histogram update was incorrect. As a result, the histogram might gracefully ignored those keys instead or update the wrong slot histogram.",https://github.com/redis/redis/commit/c55e33a99fa2efd5a9583964c45425ef0855182b
redis/redis,https://github.com/redis/redis,c636d80,Eran Hadad,2025-07-31T09:00:44Z,"Update Bloom, Json, and Timeserise to 8.2.0 (#14230)",https://github.com/redis/redis/commit/c636d80a77b9f6f4263d537b51bc982bb0bb66f1
redis/redis,https://github.com/redis/redis,96ff42b,alonre24,2025-07-31T08:55:31Z,"Bump RediSearch to 8.2.0 (#14232)  * Expose more compression variants for the new SVS-VAMANA vector index - https://github.com/RediSearch/RediSearch/pull/6430 * Add the optional `SHARD_K_RATIO` parameter for KNN vector query in a cluster environment to favor network latency reduction at the expense of accuracy (under unsatble features) - https://github.com/RediSearch/RediSearch/pull/6531, https://github.com/RediSearch/RediSearch/pull/6535",https://github.com/redis/redis/commit/96ff42b20ebef0875071282e74d09746297bd98a
redis/redis,https://github.com/redis/redis,4604407,YaacovHazan,2025-07-31T08:51:26Z,Use safe Iterator for modules (#14239),https://github.com/redis/redis/commit/4604407fdd0614d2db63204a0f5e3f5f8ca8bfd3
redis/redis,https://github.com/redis/redis,9b6fb47,YaacovHazan,2025-07-31T08:51:04Z,Remove string cat usage in tcl tests in order to support tcl8.5 (#14238)  Co-authored-by: Mincho Paskalev <minchopaskal@gmail.com>,https://github.com/redis/redis/commit/9b6fb4760396c7cd3fbe354839da0d73005de341
redis/redis,https://github.com/redis/redis,bec644a,debing.sun,2025-07-30T14:24:56Z,"Fix missing kvobj reassignment after reallocation in MOVE command (#14233)  Introduced by https://github.com/redis/redis/issues/13806  Fixed a crash in the MOVE command when moving hash objects that have both key expiration and field expiration.  The issue occurred in the following scenario: 1. A hash has both key expiration and field expiration. 2. During MOVE command, `setExpireByLink()` is called to set the expiration time for the target hash, which may reallocate the kvobj of hash. 3. Since the hash has field expiration, `hashTypeAddToExpires()` is called to update the minimum field expiration time  Issue: However, the kvobj pointer wasn't updated with the return value from `setExpireByLink()`, causing `hashTypeAddToExpires()` to use freed memory.",https://github.com/redis/redis/commit/bec644aab198049eaa5583631c419b4574b137e1
redis/redis,https://github.com/redis/redis,333f679,debing.sun,2025-07-30T13:47:51Z,Fix timeout issues in memefficiency.tcl (#14231)  Follow https://github.com/redis/redis/pull/14217 Fix https://github.com/redis/redis/issues/14196  Fix two other issues that might cause timeouts due to command writing via pipe.,https://github.com/redis/redis/commit/333f679e8948055e145415f701dd2dc1c21a063f
redis/redis,https://github.com/redis/redis,db4fc2a,Yuan Wang,2025-07-28T13:09:46Z,"Fix HINCRBYFLOAT removes field expiration on replica (#14224)  Fixes #14218  Before, we replicate HINCRBYFLOAT as an HSET command with the final value in order to make sure that differences in float precision or formatting will not create differences in replicas or after an AOF restart. However, on the replica side, if the field has an expiration time, HSET will remove it, even though the master retains it. This leads to inconsistencies between the master and the replica.  To address this, we now use the HSETEX command with the KEEPTTL flag instead of HSET, ensuring that the field’s TTL is preserved.  This bug was introduced in version 7.4, but the HSETEX command was only implemented from version 8.0. Therefore, this patch does not fix the issue in the 7.4 branch, a separate commit is needed to address it in 7.4.",https://github.com/redis/redis/commit/db4fc2a83309bf8b65e25deedfac0ff71d67e4b8
redis/redis,https://github.com/redis/redis,e9d2bf4,Ozan Tezcan,2025-07-28T09:55:09Z,Skip replication test due to flakiness under TSAN (#14225),https://github.com/redis/redis/commit/e9d2bf48e0960bd9f3199d3cb28bfb80d6e47f6e
redis/redis,https://github.com/redis/redis,3ed9460,Stav-Levi,2025-07-28T07:19:33Z,handle SET KEEPTTL in the optimization path  Fixed bug where SET key value after SET key value EX seconds would not remove the TTL as expected. The issue was in dbSetValue()'s optimization path which was missing TTL handling logic.,https://github.com/redis/redis/commit/3ed9460aa68e13c75e1d1afd4a0f2fb54ea972f8
redis/redis,https://github.com/redis/redis,8239671,Stav-Levi,2025-07-28T07:17:48Z,"Add API to allow Redis modules to unsubscribe from keyspace notifications  This API complements module subscribe by enabling modules to unsubscribe from specific keyspace event notifications when they are no longer needed. This helps reduce performance overhead and unnecessary callback invocations.  The function matches subscriptions based on event mask, callback pointer, and module identity. If a matching subscription is found, it is removed.  Returns REDISMODULE_OK if a subscription was successfully removed, otherwise REDISMODULE_ERR.",https://github.com/redis/redis/commit/82396716d05b3d63bc0d66a5820dfc604a6a2fd8
redis/redis,https://github.com/redis/redis,fe3f0aa,debing.sun,2025-07-28T02:53:57Z,"Fix some daily CI issues (#14217)  1) Fix the timeout of `Active defrag big keys: standalone` Using a pipe to write commands may cause the write to block if the read buffer becomes full.  2) Fix the failure of `Main db not affected when fail to diskless load` test If the master was killed in slow environment, then after `cluster-node-timeout` (3s in our test), running keyspace commands on the replica will get a CLUSTERDOWN error.  3) Fix the failure of `Test shutdown hook` test ASAN can intercept a signal, so I guess that when we send SIGCONT after SIGTERM to kill the server, it might start doing some work again, causing the process to close very slowly.",https://github.com/redis/redis/commit/fe3f0aa252158bd45eb0bd68e21b362bf865532a
redis/redis,https://github.com/redis/redis,ecd5e63,debing.sun,2025-07-24T07:20:04Z,"CLIENT UNBLOCK should't be able to unpause paused clients (#14164)  This PR is based on https://github.com/valkey-io/valkey/pull/2117  When a client is blocked by something like `CLIENT PAUSE`, we should not allow `CLIENT UNBLOCK timeout` to unblock it, since some blocking types does not has the timeout callback, it will trigger a panic in the core, people should use `CLIENT UNPAUSE` to unblock it.  Also using `CLIENT UNBLOCK error` is not right, it will return a UNBLOCKED error to the command, people don't expect a `SET` command to get an error.  So in this commit, in these cases, we will return 0 to `CLIENT UNBLOCK` to indicate the unblock is fail. The reason is that we assume that if a command doesn't expect to be timedout, it also doesn't expect to be unblocked by `CLIENT UNBLOCK`.  The old behavior of the following command will trigger panic in timeout and get UNBLOCKED error in error. Under the new behavior, client unblock will get the result of 0. ``` client 1> client pause 100000 write client 2> set x x  client 1> client unblock 2 timeout or client 1> client unblock 2 error ```  ---------  Signed-off-by: Binbin <binloveplay1314@qq.com> Co-authored-by: Binbin <binloveplay1314@qq.com>",https://github.com/redis/redis/commit/ecd5e639ed4d35f71c3e67889879aee0e8fb3919
redis/redis,https://github.com/redis/redis,7df3414,Slavomir Kaslev,2025-07-24T07:17:54Z,Prefer storing iterators on stack instead of the heap (#14200)  Refactor use of `dictGetIterator()/dictSafeGetIterator()/listGetIterator()` to `dictInitIterator()/dictInitSafeIterator()/listRewind()` respectively which don't allocate memory.,https://github.com/redis/redis/commit/7df34143c2cfa2416c2e37ca3520c77e9113f201
redis/redis,https://github.com/redis/redis,7ee748e,Ozan Tezcan,2025-05-14T08:02:30Z,"Retry accept() even if accepted connection reports an error (CVE-2025-48367)  In case of accept4() returns an error, we should check errno value and decide if we should retry accept4() without waiting next event loop iteration.",https://github.com/redis/redis/commit/7ee748eda691141b2150fdfec405be1afd3ea5df
redis/redis,https://github.com/redis/redis,d7992b7,debing.sun,2025-05-07T10:25:06Z,Fix out of bounds write in hyperloglog commands (CVE-2025-32023)  Co-authored-by: oranagra <oran@redislabs.com>,https://github.com/redis/redis/commit/d7992b7f78d00d456bc7ae426eceb71e2cb169a9
redis/redis,https://github.com/redis/redis,f6f1674,Lior Kogan,2025-07-22T13:48:03Z,"Update SECURITY.md and README.md (#14170)  For security vulnerability patches released under Redis Open Source 7.4 and thereafter, Redis permits users of earlier versions (7.2 and prior) to access patches under the BSD3 license noted in REDISCONTRIBUTIONS.txt instead of the full license requirements described in LICENSE.txt.",https://github.com/redis/redis/commit/f6f16746e1d4bc51960158d9a896e1aa0a2c7dbd
redis/redis,https://github.com/redis/redis,457089b,debing.sun,2025-07-22T12:44:12Z,"Create global data before test instead of module load for module defrag test (#13951)  After #13816, we added defragmentation support for moduleDict, which significantly increased global data size. As a result, the defragmentation tests for non-global data were affected. Now, we move the creation of global data to before the global data test to avoid it interfering with other tests. Fixed the simple key test failure due to forgetting to reset stats.",https://github.com/redis/redis/commit/457089b1fe265a7beae495da093c65c9c9941a27
redis/redis,https://github.com/redis/redis,45c8fcc,debing.sun,2025-07-21T12:05:47Z,"Only mark the client reprocessing flag when unblocked on keys (#14165)  This PR is based on https://github.com/valkey-io/valkey/pull/2109  When we refactored the blocking framework we introduced the client reprocessing infrastructure. In cases the client was blocked on keys, it will attempt to reprocess the command. One challenge was to keep track of the command timeout, since we are reprocessing and do not want to re-register the client with a fresh timeout each time. The solution was to consider the client reprocessing flag when the client is blockedOnKeys:  ```c     if (!(c->flags & CLIENT_REPROCESSING_COMMAND)) {         /* If the client is re-processing the command, we do not set the timeout          * because we need to retain the client's original timeout. */         c->bstate.timeout = timeout;     } ```  However, this introduced a new issue. There are cases where the client will consecutive blocking of different types for example: ``` CLIENT PAUSE 10000 ALL BZPOPMAX zset 1 ``` would have the client blocked on the zset endlessly if nothing will be written to it.  **Credits to @uriyage for locating this with his fuzzer testing**  The suggested solution is to only flag the client when it is specifically unblocked on keys.  Signed-off-by: Ran Shidlansik <ranshid@amazon.com> Co-authored-by: Ran Shidlansik <ranshid@amazon.com> Co-authored-by: Binbin <binloveplay1314@qq.com>",https://github.com/redis/redis/commit/45c8fcc9925b6347c3810323685bf1402d0fd9d8
redis/redis,https://github.com/redis/redis,a4ff8d6,Stav-Levi,2025-07-21T10:59:01Z,"Fix KSN for HSETEX command when FXX/FNX is used (#14150)  When HSETEX fails due to FXX/FNX, it may still expire some fields due to lazy expiry. Though, it does not send “hexpired” notification in this case.",https://github.com/redis/redis/commit/a4ff8d6ab68ba8473630d923fd9a1b75c187edf8
redis/redis,https://github.com/redis/redis,1e388d8,debing.sun,2025-07-18T10:24:28Z,"Fix peak memory time test (#14201)  Since INFO command can create a large amount of memory usage, it may further increase the peak memory. Therefore, we should get the peak memory after deleting the large string.",https://github.com/redis/redis/commit/1e388d8b9533f813f95d5b3912cc5a6d31cc99b0
redis/redis,https://github.com/redis/redis,b528788,Salvatore Sanfilippo,2025-07-18T10:19:14Z,"Fix vrand ping pong (#14183)  VRANDMEMBER had a bug when exactly two elements where present in the vector set: we selected a fixed number of random paths to take, and this will lead always to the same element. This PR should be kindly back-ported to Redis 8.x.",https://github.com/redis/redis/commit/b528788f6547e73afead8bd37cbc0c0bb373e871
redis/redis,https://github.com/redis/redis,d86cf66,Ali-Akber Saifee,2025-07-16T12:17:53Z,Move WITHATTRIBS token from vlinks to vsim (#14194)  # Description The `WITHATTRIBS` token was incorrectly documented under the `vlinks` command in #14065,https://github.com/redis/redis/commit/d86cf6610144249f846f3358ea8cc1cf6409b3e8
redis/redis,https://github.com/redis/redis,75cdc51,debing.sun,2025-07-15T02:34:27Z,Update debian buster sources to archive.debian.org (#14197)  http://deb.debian.org/debian no longer serves Debian 10 (buster); it has been moved to http://archive.debian.org/debian.,https://github.com/redis/redis/commit/75cdc51f86a7a306f9236311123c9f7f045c9af6
redis/redis,https://github.com/redis/redis,ebf19e4,Henry,2025-07-11T14:20:00Z,Fix memleak issues in redis-cli (#14186),https://github.com/redis/redis/commit/ebf19e4c92b7abc83c6e1e7cc445902779cefa48
redis/redis,https://github.com/redis/redis,b8382f0,show1999,2025-07-11T03:23:54Z,Fix misleading io_flags assertion comment (#14187),https://github.com/redis/redis/commit/b8382f0540cdb53c451775b38288c6fe5a8ec40a
redis/redis,https://github.com/redis/redis,b5d5486,Salvatore Sanfilippo,2025-07-10T02:08:59Z,"[Vector sets] Endianess fix and speedup of data loading (#14144)  Hello, this is a patch that improves vector sets in two ways:  1. It makes the RDB format compatible with big endian machines: yeah, they are non existent nowadays, but still it is better to be correct. The behavior remains unchanged in little endian systems, it only changes what happens in big endian systems in order for it to load and emit the exact same format produced by little endian. The implementation was *already largely safe* but for one detail.  2. More importantly, this PR saves nodes worst link score / index in a backward compatible way, introducing also versioning information for the serialized node encoding, that could be useful in the future. With this information, that in the past was not saved for a programming error (mine), there is no longer need to compute the worst link info at runtime when loading data. This results in a speed improvement of about 30% when loading data from disk / RESTORE. The saving performance is unaffected.  The patch was tested with care to be sure that data produced with old vector sets implementations are loaded without issues (that is, the backward compatibility was hand-tested). The new code is tested by the persistence test already in the test suite, so no new test was added.",https://github.com/redis/redis/commit/b5d54866ac9f17192b4e747dd27fe9db9022eb49
redis/redis,https://github.com/redis/redis,92e39ca,Ali-Akber Saifee,2025-07-08T08:32:00Z,Fix version for vector set commands.json (#14005)  # Description   Update `since` for all vector set commands from `1.0.0` to `8.0.0`,https://github.com/redis/redis/commit/92e39cac96a57e963db595ea49abfbadc911e05f
redis/redis,https://github.com/redis/redis,86c8be6,Mincho Paskalev,2025-07-07T10:29:14Z,"Add new KSN types - overwritten and type_changed (#14141)  ## What  Add new keyspace notification event types - OVERWRITTEN - emitted when the value of a key is completely overwritten - TYPE_CHANGED - when the value of a key's type changes  Used in Pub/Sub KSN mechanism. Also added module hooks for the new types.  ## Motivation  Many commands overwrite the value of a key. F.e SET completely overwrites the value of any key, even its type. Other commands that have the REPLACE parameter also do so.  This commit gives more granularity over following such events. Specific use-case at hand was module that is subscribed to string events for the sole purpose of checking if hash keys get converted to strings via the `SET` command. Subscribing to `type_changed` event not only removes the need to subscribe to string events but is also more correct as not only `SET` can change the type of a key.  ## List of commands emitting the new events  * SET * MSET * COPY * RESTORE * RENAME * BITOP  Each type with STORE operation: * SORT * S*STORE * Z*STORE * GEORADIUS * GEOSEARCHSTORE  ## Usage example  ### pub-sub  Emit overwritten and type-changed events... ``` $ redis-server --notify-keyspace-events KEoc ```  Generate an overwritten event that also changes the type of a key... ``` $ redis-cli 127.0.0.1:6379> lpush l 1 2 3 (integer) 3 127.0.0.1:6379> set l x OK ```  Subscribe to the events... ``` $ ./src/redis-cli 127.0.0.1:6379> psubscribe * 1) ""psubscribe"" 2) ""*"" 3) (integer) 1 1) ""pmessage"" 2) ""*"" 3) ""__keyspace@0__:l"" 4) ""overwritten"" 1) ""pmessage"" 2) ""*"" 3) ""__keyevent@0__:overwritten"" 4) ""l"" 1) ""pmessage"" 2) ""*"" 3) ""__keyspace@0__:l"" 4) ""type_changed"" 1) ""pmessage"" 2) ""*"" 3) ""__keyevent@0__:type_changed"" 4) ""l"" ```  ### Modules  As with any other KSN type subscribe to the appropriate events ``` RedisModule_SubscribeToKeyspaceEvents(       ctx,       REDISMODULE_NOTIFY_OVERWRITTEN | REDISMODULE_NOTIFY_TYPE_CHANGED | ...       notificationCallback ); ```  ## Implementation notes  Most of the cases are handled in `setKeyByLink` but for some commands overwriting had to be manually checked - specifically `RESTORE`, `COPY` and `RENAME` manually call `dbAddInternal`",https://github.com/redis/redis/commit/86c8be6368816c528062b50d84df3f7547f5d4f7
redis/redis,https://github.com/redis/redis,4322ceb,debing.sun,2025-07-07T02:12:51Z,"Ensure empty error tables in scripts don't crash (#14163)  This PR is based on: https://github.com/valkey-io/valkey/pull/2229  When calling the command `EVAL error{} 0`, Redis crashes with the following stack trace. This patch ensures we never leave the `err_info.msg` field null when we fail to extract a proper error message.  ---------  Signed-off-by: Fusl <fusl@meo.ws> Signed-off-by: Binbin <binloveplay1314@qq.com> Co-authored-by: Fusl <fusl@meo.ws> Co-authored-by: Binbin <binloveplay1314@qq.com>",https://github.com/redis/redis/commit/4322cebc1764d433b3fce3b3a108252648bf59e7
redis/redis,https://github.com/redis/redis,9ff8ade,adamiBs,2025-07-06T19:04:50Z,"Fix incorrect Rust 1.88.0 SHA256 checksums in modules/Makefile (#14176)  The SHA256 checksums for Rust 1.88.0 were incorrect, causing checksum verification failures during installation. Updated with the correct official checksums from https://static.rust-lang.org/dist/:  - x86_64-unknown-linux-gnu: 7b5437c1d18a174faae253a18eac22c32288dccfc09ff78d5ee99b7467e21bca - x86_64-unknown-linux-musl: 200bcf3b5d574caededba78c9ea9d27e7afc5c6df4154ed0551879859be328e1 - aarch64-unknown-linux-gnu: d5decc46123eb888f809f2ee3b118d13586a37ffad38afaefe56aa7139481d34 - aarch64-unknown-linux-musl: f8b3a158f9e5e8cc82e4d92500dd2738ac7d8b5e66e0f18330408856235dec35",https://github.com/redis/redis/commit/9ff8ade64a3ccd844473c584b967a768fadc5aef
redis/redis,https://github.com/redis/redis,17ffb1a,Jonas Kruckenberg,2025-07-06T11:44:40Z,chore: Rust version bump to 1.88 (#14167)  Simple version bump from Rust 1.87 to Rust 1.88 to make nightly build again.,https://github.com/redis/redis/commit/17ffb1ab80b8b39bcceead73b4496b5546f0c84a
redis/redis,https://github.com/redis/redis,ffd4154,show1999,2025-07-04T13:12:28Z,Add boundary assert to connTypeRegister (#14153)  Add boundary assertion to prevent array overflow in `connTypeRegister` when reaching **CONN_TYPE_MAX** limit.,https://github.com/redis/redis/commit/ffd41540f0005a8acfddc71cafd63a2bfb81e0ce
redis/redis,https://github.com/redis/redis,eb9337a,Mincho Paskalev,2025-07-04T09:26:24Z,Fix uninitializeed RMConfigIterator::is_glob causing MSan warnings (#14171)  Recent [PR](https://github.com/redis/redis/pull/14051) causes MSan to fail during daily CI with uninitialized value warning. It is not a bug per se as the uninitialized member is dependent on another member not being NULL. With this PR now MSan does not complain.,https://github.com/redis/redis/commit/eb9337abf89b2fdea95bdc95ffa38e01dfbdcb91
redis/redis,https://github.com/redis/redis,15706f2,Mincho Paskalev,2025-07-03T10:46:33Z,"Module set/get config API (#14051)  # Problem  Some redis modules need to call `CONFIG GET/SET` commands. Server may be ran with `rename-command CONFIG """"`(or something similar) which leads to the module being unable to access the config.  # Solution  Added new API functions for use by modules ``` RedisModuleConfigIterator* RedisModule_GetConfigIterator(RedisModuleCtx *ctx, const char *pattern); void RedisModule_ReleaseConfigIterator(RedisModuleCtx *ctx, RedisModuleConfigIterator *iter); const char *RedisModule_ConfigIteratorNext(RedisModuleConfigIterator *iter); int RedisModule_GetConfigType(const char *name, RedisModuleConfigType *res); int RedisModule_GetBoolConfig(RedisModuleCtx *ctx, const char *name, int *res); int RedisModule_GetConfig(RedisModuleCtx *ctx, const char *name, RedisModuleString **res); int RedisModule_GetEnumConfig(RedisModuleCtx *ctx, const char *name, RedisModuleString **res); int RedisModule_GetNumericConfig(RedisModuleCtx *ctx, const char *name, long long *res); int RedisModule_SetBoolConfig(RedisModuleCtx *ctx, const char *name, int value, RedisModuleString **err); int RedisModule_SetConfig(RedisModuleCtx *ctx, const char *name, RedisModuleString *value, RedisModuleString **err); int RedisModule_SetEnumConfig(RedisModuleCtx *ctx, const char *name, RedisModuleString *value, RedisModuleString **err); int RedisModule_SetNumericConfig(RedisModuleCtx *ctx, const char *name, long long value, RedisModuleString **err); ```  ## Implementation  The work is mostly done inside `config.c` as I didn't want to expose the config dict outside of it. That means each of these module functions has a corresponding method in `config.c` that actually does the job. F.e `RedisModule_SetEnumConfig` calls `moduleSetEnumConfig` which is implemented in `config.c`  ## Notes  Also, refactored `configSetCommand` and `restoreBackupConfig` functions for the following reasons: - code and logic is now way more clear in `configSetCommand`. Only caveat here is removal of an optimization that skipped running apply functions that already have ran in favour of code clarity. - Both functions needlessly separated logic for module configs and normal configs whereas no such separation is needed. This also had the side effect of removing some allocations. - `restoreBackupConfig` now has clearer interface and can be reused with ease. One of the places I reused it is for the individual `moduleSet*Config` functions, each of which needs the restoration functionality but for a single config only.  ## Future  Additionally, a couple considerations were made for potentially extending the API in the future - if need be an API for atomically setting multiple config values can be added - `RedisModule_SetConfigsTranscationStart/End` or similar that can be put around `RedisModule_Set*Config` calls. - if performance is an issue an API `RedisModule_GetConfigIteratorNextWithTypehint` or similar may be added in order not to incur the additional cost of calling `RedisModule_GetConfigType`.  ---------  Co-authored-by: Oran Agra <oran@redislabs.com>",https://github.com/redis/redis/commit/15706f2e82ab17692e6e049e3eed96f2e0ad5df8
redis/redis,https://github.com/redis/redis,5b7eec4,debing.sun,2025-07-03T06:41:26Z,"Fix crash due to incorrect event deletion of evport (#14162)  This PR fixes https://github.com/redis/redis/issues/14056#issuecomment-3026114590  ## Summary Because evport uses `eventLoop->events[fd].mask` to determine whether to remove the event, but in ae.c we call `aeApiDelEvent()` before updating `eventLoop->events[fd].mask`, this causes evport to always see the old value, and as a result, `port_dissociate()` is never called to remove the fd. This issue may not surface easily in a non-multithreaded, but since in the multi-threaded case we frequently reassign fds to different threads, it makes the crash much more likely to occur.",https://github.com/redis/redis/commit/5b7eec4c8172aa9decca219397af536e16ead8e2
redis/redis,https://github.com/redis/redis,a7d9114,Eran Hadad,2025-07-02T19:13:39Z,"Update Bloom, TS, and JSON Modules to Version 8.1.90 (#14160)",https://github.com/redis/redis/commit/a7d91145c092dff3475b88d7e121d6ea0aa1464d
redis/redis,https://github.com/redis/redis,2826fc8,alonre24,2025-07-02T19:10:42Z,"Update redisearch to 8.2 RC1 (#14159)  [#Q6329](https://github.com/RediSearch/RediSearch/pull/6329), [#Q6329](https://github.com/RediSearch/RediSearch/pull/6394) - Introducing the new SVS-VAMANA vector index type which supports vector compression (optimized on Intel machines)",https://github.com/redis/redis/commit/2826fc834aebda16eb504187f926c325cad2242a
redis/redis,https://github.com/redis/redis,ad8c7de,Mincho Paskalev,2025-07-02T08:55:57Z,"Fix assertion in updateClientMemUsageAndBucket (#14152)  ## Description  `updateClientMemUsageAndBucket` is called from the main thread to update memory usage and memory bucket of a client. That's why it has assertion that it's being called by the main thread.  But it may also be called from a thread spawned by a module. Specifically, when a module calls `RedisModule_Call` which in turn calls `call`->`replicationFeedMonitors`->`updateClientMemUsageAndBucket`. This is generally safe as module calls inside a spawned thread should be guarded by a call to `ThreadSafeContextLock`, i.e the module is holding the GIL at this point.  This commit fixes the assertion inside `updateClientMemUsageAndBucket` so that it encompasses that case also. Generally calls from module-spawned threads are safe to operate on clients that are not running on IO-threads when the module is holding the GIL.  ---------  Co-authored-by: Yuan Wang <wangyuancode@163.com> Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/ad8c7de3fe563ff6ccdce7d939c0d50e2a8f19fa
redis/redis,https://github.com/redis/redis,0d8e750,Slavomir Kaslev,2025-07-01T17:26:51Z,"Add CLUSTER SLOT-STATS command (#14039)  Add CLUSTER SLOT-STATS command for key count, cpu time and network IO per slot currently.  The command has the following syntax      CLUSTER SLOT-STATS SLOTSRANGE start-slot end-slot  or      CLUSTER SLOT-STATS ORDERBY metric [LIMIT limit] [ASC/DESC]  where metric can currently be one of the following      key-count -- Number of keys in a given slot cpu-usec -- Amount of CPU time (in microseconds) spent on a given slot network-bytes-in -- Amount of network ingress (in bytes) received for given slot network-bytes-out -- Amount of network egress (in bytes) sent out for given slot  This PR is based on:     valkey-io/valkey#351     valkey-io/valkey#709     valkey-io/valkey#710     valkey-io/valkey#720     valkey-io/valkey#840  Co-authored-by: Kyle Kim <kimkyle@amazon.com> Co-authored-by: Madelyn Olson <madelyneolson@gmail.com> Co-authored-by: Harkrishn Patro <harkrisp@amazon.com>  ---------  Co-authored-by: Kyle Kim <kimkyle@amazon.com> Co-authored-by: Madelyn Olson <madelyneolson@gmail.com>",https://github.com/redis/redis/commit/0d8e7508832086f1acab4c5b614462816b282412
redis/redis,https://github.com/redis/redis,fa040a7,debing.sun,2025-07-01T13:00:42Z,"Add XDELEX and XACKDEL commands for stream (#14130)  ## Summary and detailed design for new stream command  ## XDELEX  ### Syntax ``` XDELEX key [KEEPREF | DELREF | ACKED] IDS numids id [id ...] ```  ### Description The `XDELEX` command extends the Redis Streams `XDEL` command, offering enhanced control over message entry deletion with respect to consumer groups. It accepts optional `DELREF` or `ACKED` parameters to modify its behavior:  - **KEEPREF:** Deletes the specified entries from the stream, but preserves existing references to these entries in all consumer groups' PEL. This behavior is similar to XDEL. - **DELREF:** Deletes the specified entries from the stream and also removes all references to these entries from all consumer groups' pending entry lists, effectively cleaning up all traces of the messages. - **ACKED:** Only trims entries that were read and acknowledged by all consumer groups.  **Note:** The `IDS` block can appear at any position in the command, consistent with other commands.  ### Reply Array reply, for each `id`: - `-1`: No such `id` exists in the provided stream `key`. - `1`: Entry was deleted from the stream. - `2`: Entry was not deleted, but there are still dangling references. (ACKED option)  ## XACKDEL  ### Syntax ``` XACKDEL key group [KEEPREF | DELREF | ACKED] IDS numids id [id ...] ```  ### Description The `XACKDEL` command combines `XACK` and `XDEL` functionalities in Redis Streams. It acknowledges specified message IDs in the given consumer group and attempts to delete corresponding stream entries. It accepts optional `DELREF` or `ACKED` parameters:  - **KEEPREF:** Acknowledges the messages in the specified consumer group and deletes the entries from the stream, but preserves existing references to these entries in all consumer groups' PEL. - **DELREF:** Acknowledges the messages in the specified consumer group, deletes the entries from the stream, and also removes all references to these entries from all consumer groups' pending entry lists, effectively cleaning up all traces of the messages. - **ACKED:** Acknowledges the messages in the specified consumer group and only trims entries that were read and acknowledged by all consumer groups.   ### Reply Array reply, for each `id`: - `-1`: No such `id` exists in the provided stream `key`. - `1`: Entry was acknowledged and deleted from the stream. - `2`: Entry was acknowledged but not deleted, but there are still dangling references. (ACKED option)  # Redis Streams Commands Extension  ## XTRIM  ### Syntax ``` XTRIM key <MAXLEN | MINID> [= | ~] threshold [LIMIT count] [KEEPREF | DELREF | ACKED] ```  ### Description The `XTRIM` command trims a stream by removing entries based on specified criteria, extended to include optional `DELREF` or `ACKED` parameters for consumer group handling:  - **KEEPREF:** Trims the stream according to the specified strategy (MAXLEN or MINID) regardless of whether entries are referenced by any consumer groups, but preserves existing references to these entries in all consumer groups' PEL. - **DELREF:** Trims the stream according to the specified strategy and also removes all references to the trimmed entries from all consumer groups' PEL. - **ACKED:** Only trims entries that were read and acknowledged by all consumer groups.  ### Reply No change.  ## XADD  ### Syntax ``` XADD key [NOMKSTREAM] [<MAXLEN | MINID> [= | ~] threshold [LIMIT count]] [KEEPREF | DELREF | ACKED] <* | id> field value [field value ...] ```  ### Description The `XADD` command appends a new entry to a stream and optionally trims it in the same operation, extended to include optional `DELREF` or `ACKED` parameters for trimming behavior:  - **KEEPREF:** When trimming, removes entries from the stream according to the specified strategy (MAXLEN or MINID), regardless of whether they are referenced by any consumer groups, but preserves existing references to these entries in all consumer groups' PEL. - **DELREF:** When trimming, removes entries from the stream according to the specified strategy and also removes all references to these entries from all consumer groups' PEL. - **ACKED:** When trimming, only removes entries that were read and acknowledged by all consumer groups. Note that if the number of referenced entries is bigger than MAXLEN, we will still stop.  ### Reply No change.  ## Key implementation  Since we currently have no simple way to track the association between an entry and consumer groups without iterating over all groups, we introduce two mechanisms to establish this link. This allows us to determine whether an entry has been seen by all consumer groups, and to identify which groups are referencing it. With this links, we can break the association when the entry is either acknowledged or deleted.  1) Added reference tracking between stream messages and consumer groups using `cgroups_ref` The cgroups_ref is implemented as a rax that maps stream message IDs to lists of consumer groups that reference those messages, and streamNACK stores the corresponding nodes of this list, so that the corresponding groups can be deleted during `ACK`. In this way, we can determine whether an entry has been seen but not ack. 2) Store a cache minimum last_id in the stream structure. The reason for doing this is that there is a situation where an entry has never been seen by the consume group. In this case, we think this entry has not been consumed either. If there is an ""ACKED"" option, we cannot directly delete this entry either. When a consumer group updates its last_id, we don’t immediately update the cached minimum last_id. Instead, we check whether the group’s previous last_id was equal to the current minimum, or whether the new last_id is smaller than the current minimum (when using `XGROUP SETID`). If either is true, we mark the cached minimum last_id as invalid, and defer the actual update until the next time it’s needed.  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com> Co-authored-by: moticless <moticless@github.com> Co-authored-by: Ozan Tezcan <ozantezcan@gmail.com> Co-authored-by: Slavomir Kaslev <slavomir.kaslev@gmail.com> Co-authored-by: Yuan Wang <yuan.wang@redis.com>",https://github.com/redis/redis/commit/fa040a72c0720d9b0a833117b086e5bbafa6ddc8
redis/redis,https://github.com/redis/redis,5ff81f6,debing.sun,2025-07-01T09:35:09Z,"Fix XPENDING reply schema for empty reply (#14129)  When the PEL is empty, the reply of `XPENDING` without `start` option will be: ``` 1) (integer) 0 2) (nil) 3) (nil) 4) (nil) ```  It is not an empty array, so we need to create an individual reply schema for it.",https://github.com/redis/redis/commit/5ff81f68a3effdf9a41aabdb41ced5acc1689aed
redis/redis,https://github.com/redis/redis,64ae81d,itayTziv,2025-07-01T07:28:13Z,"New config: lazyexpire-nested-arbitrary-keys (#14149)  In this PR we added hidden config - `lazyexpire-nested-arbitrary-keys`, which can take: * yes - the default. produce and propagate lazy-expire DELs as usual. * no - avoid lazy-expire from commands that touch arbitrary keys (such as SCAN, RANDOMKEY), if generated within a transactions (MULTI/EXEC, LUA). This ensures such commands won't induce CROSSSLOT on remote proxy, as happened in when replicating one db into another (possibly sharded differently). Since the issue is relevant only in replicated servers (RE's replica-of mode or CRDT) - it was added to the core as a hidden config.  Please note that this config will always apply to read-only commands (see EXPIRE_FORCE_DELETE_EXPIRED flag). Since write commands may require key expiration to operate correctly.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/64ae81d37cf7fd7828c3c0c4da5184fb79458c21
redis/redis,https://github.com/redis/redis,9693066,Oran Agra,2025-06-30T13:56:17Z,"Make Active defrag big list test much faster (#14157)  it aims to create listpacks of 500k, but did that with 5 insertions of 100k each, instead do that in one insertion, reducing the need for listpack gradual growth, and reducing the number of commands we send. apparently there are some stalls reading the replies of the commands, specifically in GH actions, reducing the number of commands seems to eliminate that.",https://github.com/redis/redis/commit/96930663b4f9e2318a1fcdf6f7ea16cb1b3e239e
redis/redis,https://github.com/redis/redis,ca6145b,wclmxxs,2025-06-30T01:37:17Z,"Reduce the main thread blocking in clients cron (#13900)  The main thread needs to check clients in every cron iteration. During this check, the corresponding I/O threads must not operate on these clients to avoid data-race. As a result, the main thread is blocked until the I/O threads finish processing and are suspended, allowing the main thread to proceed with client checks.  Since the main thread's resources are more valuable than those of I/O threads in Redis, this blocking behavior should be avoided. To address this, the I/O threads check during their cron whether any of their maintained clients need to be inspected by the main thread. If so, the I/O threads send those clients to the main thread for processing, then the main thread runs cron jobs for these clients.  In addition, an always-active client might not be in thread->clients, so before processing the client’s command, we also check whether the client has skipped running its cron job for over 1 second. If it has, we run the cron job for the client.  The main thread does not need to actively pause the IO threads, thus avoiding potential blocking behavior, fixes https://github.com/redis/redis/issues/13885  Besides, this approach also can let all clients run cron task in a second, but before, we pause IO threads in multiple batches when there are more than 8 IO threads, that may cause some clients are not be processed in a second.  ---------  Co-authored-by: Yuan Wang <yuan.wang@redis.com>",https://github.com/redis/redis/commit/ca6145b18c35f405557197342f30fe24505b0e18
redis/redis,https://github.com/redis/redis,531b82d,Yi Deng,2025-06-27T01:07:33Z,"Fix replication lseek check (#14135)  In `sendBulkToSlave`, the `lseek()` call used to position the RDB file descriptor before reading the next data chunk was not checked for errors. If the `lseek()` system call were to fail, the file descriptor would remain at an incorrect position. The subsequent `read()` would then fetch the wrong data, leading to a corrupted RDB stream being sent to the replica. This could cause the replication to fail or result in data inconsistency. This patch introduces a check for the `lseek()` return value. On failure, it logs a detailed warning and aborts the replication by freeing the client, mirroring the existing error handling for `read()` and `write()` calls within the same function. This improves the robustness of the RDB transfer process.  ---------  Co-authored-by: Yuan Wang <wangyuancode@163.com>",https://github.com/redis/redis/commit/531b82df22d30f4b996351a86b29de8bb56c06bb
redis/redis,https://github.com/redis/redis,8948a5d,Salvatore Sanfilippo,2025-06-26T02:13:54Z,"[Vector Sets] IN operator for string/string operands (#14122)  This PR introduces ""IN"" overloading for strings in Vector Sets VSIM FILTER expressions. Now it is possible to do something like:      ""foo"" IN ""foobar""  IN continues to work as usually if the second operand is an array, checking for membership of the left operand.  Ping @rowantrollope that requested this feature. I'm evaluating if to add glob matching functionalities via the `=~` operator but I need to do an optimization round in our glob matching function probably. Glob matching can be slower, at the same time the complexity of the greedy search in the graph remains unchanged, so it may be a good idea to have it.  Case insensitive search will be likely not be added however, since this would require handling unicode that is kinda outside the scope of Redis filters. The user is still able to perform `""foo"" in ""foobar"" || ""FOO"" in ""foobar""` at least.",https://github.com/redis/redis/commit/8948a5d2b22472fd65857a4258b290ae22e5da96
redis/redis,https://github.com/redis/redis,a25f0a7,h.o.t. neglected,2025-06-26T01:47:00Z,"Fix generate-commands-json.py script (#14098) (#14111)  Close https://github.com/redis/redis/issues/14098 This is to handle some optional command docs fields, when the commands belong to group module.",https://github.com/redis/redis/commit/a25f0a715e58368451ce91d971dd1ae967d73816
redis/redis,https://github.com/redis/redis,a744411,Filipe Oliveira (Redis),2025-06-25T12:32:23Z,"Optimize Expiry Check in `scanCallback()` Using `kvobj` (#14140)  The current `scanCallback()` implementation performs expiry checks like this:  ```c robj kobj; sds keyname = kvobjGetKey(kv); initStaticStringObject(kobj, keyname); expireIfNeeded(db, &kobj, kv, 0); ```  This pattern introduces unnecessary temporary stack allocation for robj and additional memory traffic, confirmed by topdown analysis + perf  ``` sudo ./toplev.py --pid $(pgrep redis-server) --level 2 --run-sample -- sleep 30 # 5.01-full-perf on Intel(R) Xeon(R) Platinum 8488C [spr/sapphire_rapids] BE               Backend_Bound               % Slots                       49.6   BE/Mem           Backend_Bound.Memory_Bound  % Slots                       34.3  <== 	This metric represents fraction of slots the Memory 	subsystem within the Backend was a bottleneck... ```   From perf record -g sampling (hot path in scanCallback):  kvobjGetKey() + initStaticStringObject() account for ~12% frontend + backend bound stalls   These cycles are avoidable when kvobj is already available.  This PR extends expireIfNeeded to support key is none (using kvobj) , in order to avoid sds key copy + temporary robj on scanCallback when it's not needed.  ## Benchmarks  By running  ``` redis-benchmarks-spec-client-runner --tests-regexp "".*scan.*"" --flushall_on_every_test_start --flushall_on_every_test_end  --cpuset_start_pos 2 --override-memtier-test-time 30 --benchmark_local_install --override-test-runs 3 --db_server_port <...> --db_server_password <...> --db_server_host <...> ```  We see   Test Name | Metric | baseline redis Wed Jun 25 (4313d7ff23ce60d9d2aba4bb748d0dbc69516613) | comparison redis (3bb00b3a977fe131b189f3112dc1bf48284a9bcf) | Δ (%) -- | -- | -- | -- | -- generic-scan-count-500 | Ops/sec | 25126 | 26726 | 6.4% generic-scan-cursor-count-5000 | Ops/sec | 1416.18 | 1446 | 2.1% generic-scan-count-500 | p99 | 55.039 | 50.687 | 8.6% generic-scan-cursor-count-5000 | p99 | 1448.62 | 1064.959 | 36.0%  ---------  Co-authored-by: debing.sun <debing.sun@redis.com> Co-authored-by: Yuan Wang <yuan.wang@redis.com>",https://github.com/redis/redis/commit/a744411f271973157cd949079b847892c24764c0
redis/redis,https://github.com/redis/redis,4313d7f,Yuan Wang,2025-06-25T07:36:40Z,"Stabilize tests for IO threading (#14138)  - tests/unit/maxmemory.tcl If multithreaded, we need to let IO threads have chance to reply output buffer, to avoid next commands causing eviction. After eviction is performed, the next command becomes ready immediately in IO threads, and now we enqueue the client to be processed in main thread’s beforeSleep without notification. However, invalidation messages generated by eviction may not have been fully delivered by that time. As a result, executing the command in beforeSleep of the event loop (running eviction) can cause additional keys to be evicted. ``` Expected '73' to be between to '200' and '300' (context: type source line 473 file  redis/tests/unit/maxmemory.tcl cmd {assert_range [r dbsize] 200 300} proc ::test) ``` the reason why CI doesn't find this issue is that we skill this test `tsan:skip` as below `start_server {tags {""maxmemory external:skip tsan:skip""}} `,so remove this tag.  - tests/integration/aof.tcl Because IO and the main thread are working in better parallelism without notification, the main thread may haven't write AOF buffer into file, but the IO thread just writes the reply, so the clients receive the reply before AOF file is changed. We should use `appendfsync always` policy to make the command is written into AOF file when receiving reply. ``` Expected '0' to be equal to '54' (context: type source line 249 file redis/tests/integration/aof.tcl cmd {assert_equal $before $after} proc ::test) ```  #13969 makes these scenarios easy to appear.",https://github.com/redis/redis/commit/4313d7ff23ce60d9d2aba4bb748d0dbc69516613
redis/redis,https://github.com/redis/redis,03816c1,Ozan Tezcan,2025-06-23T04:41:30Z,"Fix short read of hfe key that causes exit() on replica (#14143)  If replica detects broken connection while reading min expiration time of hfe key, it calls exit(). Fixed it to handle the error gracefully without calling exit.   To reproduce the issue, the short-read test was modified to generate many small hfe keys, increasing the likelihood of a connection drop while reading min expiration time:  ```tcl for {set k 0} {$k < 50000} {incr k} {   for {set i 0} {$i < 1} {incr i} {     r hsetex ""$k hfe_small"" EX [expr {int(rand()*10)}] FIELDS 1 [string repeat A [expr {int(rand()*10)}]] 0[string repeat A [expr {int(rand()*10)}]]   } } ```  We can't have the test use only hfe keys, so a few were added alongside other data. I couldn't reproduce the issue this way but with the test's randomization, it should hit this scenario in one of the runs.",https://github.com/redis/redis/commit/03816c15f75962b8e4662cee22be6f7ba6d5d9dd
redis/redis,https://github.com/redis/redis,51239f7,Stav-Levi,2025-06-22T06:19:26Z,"Record the time a replica attempts to connect with master (#13990)  Merge fork counters with https://github.com/redis/redis/pull/12957 repl_current_sync_attempts - Total number of attempts to connect to a master since the last time we disconnected from a good connection (or a configuration change). any number greater than 1 (even if the link is currently up), indicates an issue. repl_total_sync_attempts - Number of times in current configuration, the replica attempted to sync to a master. (dosent reset on master reconnect.) repl_total_disconnect_time - Total cumulative time we've been disconnected as a replica, visible when the link is up too. master_link_up_since_seconds - Number of seconds since the link is down, just maintain symmetry with master_link_down_since_seconds.",https://github.com/redis/redis/commit/51239f75d024d02f468f8d43399b363b8ef70acf
redis/redis,https://github.com/redis/redis,2e1a17c,Filipe Oliveira (Redis),2025-06-21T02:28:51Z,"Optimize quicklistCompare with optional string2ll caching. (#14131)  ### Summary  This pull request improves the performance of quicklistCompare and lpCompare by avoiding repeated calls to string2ll when comparing many quicklist/listpack entries against the same string value. The optimization targets use cases like LREM, LPOS, LINSERT, and ZRANK where comparisons are made repeatedly in a loop.  By caching the result of string2ll during a single command execution, we avoid re-parsing the same input string thousands of times—resulting in up to **30% higher throughput and up to 25% lower p50 latency** in LREM LINSERT benchmarks, and **5% higher throughput** in ZRANK (listpack) command.  ### Changes  - Updated quicklistCompare and lpCompare to accept two optional parameters:   - `long long *cached_val`   - `int *cached_valid` - If caching parameters are provided, string2ll is invoked only once and its result is reused across comparisons. - listTypeEqual was updated to forward these parameters. - Commands such as LREM, LPOS, LINSERT, and ZRANK now use this optimization. - All internal tests and usage of quicklistCompare/lpCompare were updated accordingly.  ### Behavior  - If cached_valid is NULL, quicklistCompare/lpCompare behaves as before (no caching). - If cached_valid is non-NULL:   - 0 means uninitialized: string2ll is attempted.   - 1 means valid: cached_val is used.   - -1 means invalid: string2ll previously failed and is skipped.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/2e1a17c26ce6ef0749dedf34043fca51d72c754a
redis/redis,https://github.com/redis/redis,117424f,yzc-yzc,2025-06-20T05:51:52Z,Fix negative offset issue for ZRANGEBY[SCORE|LEX] command (#14043)  Fix #13952  This PR ensures that ZRANGE_SCORE/LEX command with a negative offset will return empty.,https://github.com/redis/redis/commit/117424f85c85cdb9bc560b8fd641ba213534cd50
redis/redis,https://github.com/redis/redis,61fa8bb,yzc-yzc,2025-06-20T05:49:20Z,Record peak memory time (#14067)  resolve #14049  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>,https://github.com/redis/redis/commit/61fa8bb06fe8a1991fb819a84966a6c00677b22f
redis/redis,https://github.com/redis/redis,ce6edd2,YaacovHazan,2025-06-19T08:25:32Z,Check length of AOF file name in redis-check-aof (CVE-2025-27151) (#14139)  Ensure that the length of the input file name does not exceed PATH_MAX,https://github.com/redis/redis/commit/ce6edd289a4403a2e6ba5a638e97d5cb3534b708
redis/redis,https://github.com/redis/redis,a95b94b,Yuan Wang,2025-06-18T14:29:13Z,"Fix command arity check in IO threads (#14134)  We need to check the command arity in IO threads, if it is not correct, we should reset it, as we may do memory prefetching according to the `iolookedcmd`. Accessing `argv` using the key positions returned by `getKeysFromCommand` is unsafe and must be avoided for invalid commands.  This bug starts to have an impact after #14017",https://github.com/redis/redis/commit/a95b94b3dd8948671926f356ae9238ead7f079cf
redis/redis,https://github.com/redis/redis,35dbfc4,Filipe Oliveira (Redis),2025-06-18T13:37:27Z,"Improve SCAN performance by only performing expiration checks on DBs with volatile keys  (#14121)  This PR optimizes scanGenericCommand by moving type filtering and expiration checks from post-processing (Step 3) to the scan callback, eliminating expensive `lookupKeyReadWithFlags()` calls and adding an optimization to skip expiration checks via dict lookup given we can now check the expiration with expiry flag in the kvobj (due to the move to the scanCallback).  Profiling data (https://pprof.me/1ac456b6d1a46b2184a5e2ef314aa0a2) showed that scanGenericCommand accounted for 2.8% of total CPU time and was a notable hotspot during SCAN-heavy workloads.  ## Key optimizations: 1. **Type filtering moved to scanCallback**: Uses existing `kvobj *kv` instead of expensive lookups 2. **Expiration check optimization**: Skips `expireIfNeeded()` calls when database has no volatile keys 3. **Better cache locality**: Processes filtering during iteration rather than post-processing",https://github.com/redis/redis/commit/35dbfc4ba8a1a924b5d7cf1bc44c89c791dc2fa4
redis/redis,https://github.com/redis/redis,f6d1fd0,Salvatore Sanfilippo,2025-06-18T02:23:22Z,"Vset tests improvements (#14089)  This changes improve a bit the Vector Sets tests:  * DB9 is used instead of the target DB. After a successful test the DB is left empty. * If the replica is not available, the replication tests are skipped without errors but just a warning. * Other refactoring stuff.",https://github.com/redis/redis/commit/f6d1fd08f9b03a90c4e6a18866f8709b86bae627
redis/redis,https://github.com/redis/redis,a8cde7d,lerman25,2025-06-17T13:50:35Z,"Fix alpine missing __STRING #define (#14133)  Alpine Linux doesn't provide the  __STRING macro, causing build failure: Adding conditional define the macro if not already available.",https://github.com/redis/redis/commit/a8cde7d19c51fd150fcfd40156e6d644c47058cb
redis/redis,https://github.com/redis/redis,9017871,Moti Cohen,2025-06-16T12:11:48Z,"Optimize COPY, RENAME and RESTORE commands with TTL (#14088)  * DEL optimized to call `kvstoreDictDelete(db->expires ...)` only when O(1) `kvobjGetExpire() != -1` * Combines the two funcitons `dbAdd()` and `setExpire()` into a single  efficient call: `dbAdd(..., long long expire)`. This optimization eliminates an extra lookup and also avoid reallocating a new object to store the TTL.  **Benchmarking RESTORE command with TTL:** ``` memtier_benchmark --command ""RESTORE bla 1000000 \""\\x00\\x03bbb\\x0c\\x00\\x8e\\x85\\xaf\\x9f\\x0e'#\\x00\"" REPLACE"" --command-key-pattern=P --data-size=1 --pipeline=1000 --key-maximum=10000000 -c 5 -t 20 --hide-histogram --requests=100000 ``` **Results Summary (Average of 3 Runs):**  Metric | unstable | optimize-setExpire | Δ (Improvement) -- | -- | -- | -- Throughput (ops/sec) | 1,614,176 | 1,737,198 | +7.6% P50 Latency (ms) | 61.25 | 57.30 | –6.5% P95 Latency (ms) | 64.77 | 59.99 | –7.4% P99 Latency (ms) | 73.73 | 70.83 | –3.9% Max Latency (ms) | 87.55 | 85.84 | –2.0%",https://github.com/redis/redis/commit/90178712f6eccf1e5b61daa677c5c103114bda3a
redis/redis,https://github.com/redis/redis,94aebb7,lerman25,2025-06-16T02:06:43Z,"Add config base to vector-sets and hnsw thread config (#14082)  This PR introduces the initial configuration infrastructure for vector-sets, along with a new option: `vset-force-single-threaded-execution`. When enabled, it applies the `NOTHREAD` flag to VSIM and disables the `CAS` option for VADD, thereby enforcing single-threaded execution. Note: This mode is not optimized for single-threaded performance.  ---------  Co-authored-by: GuyAv46 <47632673+GuyAv46@users.noreply.github.com> Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/94aebb7324c77e4acecaf6d9f08c23a658dc865f
redis/redis,https://github.com/redis/redis,66b3d2d,Filipe Oliveira (Redis),2025-06-13T08:59:34Z,"Add 2K software prefetch to improve BITCOUNT performance (#14103)  Adds a software prefetch with a 2K stride to the scalar popcount loop in redisPopcount(). Prefetching improved BITCOUNT throughput by up to 41.6%, reduced p50 latency by up to 43.9%, and significantly lowered L3 memory stalls, confirming effective mitigation of memory-bound bottlenecks, with no negative impact on L1/L2 usage or cache pollution (confirmed with HW counters).  Note: The 2K stride was the best starting from 128,256,512,1024,2048,4096. 4K gave the same outcome so it's best to avoid larger strides without reason.",https://github.com/redis/redis/commit/66b3d2d98e4f5d79cac467266f17a4319cda6f50
redis/redis,https://github.com/redis/redis,abaed0d,debing.sun,2025-06-12T14:31:26Z,"Reduce the overhead from malloc usable (#14074)  Currently, in the zmalloc and zfree family functions, we rely on `je_malloc_usable_size()` to obtain the usable size of a pointer for memory statistics or to return it to the caller. However, this function is relatively expensive, as it involves locking and rbtree lookups within jemalloc. Reducing the frequency of these calls can yield significant performance improvements.  ---------  Co-authored-by: oranagra <oran@redislabs.com> Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>",https://github.com/redis/redis/commit/abaed0d54c9a3b0fe2f2238e48280b60c6f47627
redis/redis,https://github.com/redis/redis,b1d202f,Alexander Dobrzhansky,2025-06-12T08:50:24Z,Bump rust version for INSTALL_RUST_TOOLCHAIN flag (#14118)  Updates the rust version that is getting installed if `INSTALL_RUST_TOOLCHAIN` is defined,https://github.com/redis/redis/commit/b1d202fb23a1eac483cd76aa91e55deea26f54e1
redis/redis,https://github.com/redis/redis,27dd3b7,Salvatore Sanfilippo,2025-06-10T13:55:09Z,"Vector Sets fixes against corrupted data in absence of checksum verification (#14102)  Vector Sets deserialization was not designed to resist corrupted data, assuming that a good checksum would mean everything is fine. However Redis allows the user to specify extra protection via a specific configuration option.  This commit makes the implementation more resistant, at the cost of some slowdown. This also fixes a serialization bug that is unrelated (and has no memory corruption effects) about the lack of the worst index / distance serialization, that could lower the quality of a graph after links are replaced. I'll address the serialization issues in a new PR that will focus on that aspect alone (already work in progress).  The net result is that loading vector sets is, when the serialization of worst index/distance is missing (always, for now) 100% slower, that is 2 times the loading time we had before. Instead when the info will be added it will be just 10/15% slower, that is, just making the new sanity checks.  It may be worth to export to modules if advanced sanity check if needed or not. Anyway most of the slowdown in this patch comes from having to recompute the worst neighbor, since duplicated and non reciprocal links detection was heavy optimized with probabilistic algorithms.  ---------  Co-authored-by: debing.sun <debing.sun@redis.com>",https://github.com/redis/redis/commit/27dd3b71ceb90f639b74253298ab1174e9b08613
redis/redis,https://github.com/redis/redis,2ba81b7,alonre24,2025-06-08T13:24:52Z,Bump search version to 8.1.00 (#14110),https://github.com/redis/redis/commit/2ba81b70957691a6a010e785225672e6657e53e8
redis/redis,https://github.com/redis/redis,38a8894,Eran Hadad,2025-06-08T13:24:19Z,"Update TS, JSON and Bloom Modules to 8.1.00 (#14109)",https://github.com/redis/redis/commit/38a8894b507e8f0c5bcd0cdc042fa4196d7d4349
