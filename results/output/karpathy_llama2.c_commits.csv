项目名称,项目URL,提交ID,提交者,提交时间,提交信息,提交链接
karpathy/llama2.c,https://github.com/karpathy/llama2.c,350e04f,Andrej,2024-05-29T17:01:04Z,Merge pull request #514 from jameswdelancey/patch-1  Add llama3.c port to README.md,https://github.com/karpathy/llama2.c/commit/350e04fe35433e6d2941dce5a1f53308f87058eb
karpathy/llama2.c,https://github.com/karpathy/llama2.c,2eb7430,Andrej,2024-05-29T17:00:48Z,Merge pull request #521 from code-cp/patch-1  Add another Rust port,https://github.com/karpathy/llama2.c/commit/2eb7430e1a5ea8c6ac2b0a70d55b869b1b14fc20
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c502b6b,code-cp,2024-05-12T01:47:36Z,Add another Rust port   Add my Rust port using Burn,https://github.com/karpathy/llama2.c/commit/c502b6ba4c416bee0f15ffef1b52d4c7f064c1f4
karpathy/llama2.c,https://github.com/karpathy/llama2.c,e6fd170,James Delancey,2024-04-29T08:48:20Z,"Add llama3.c port to README.md  A faithful clone of Karpathy's llama2.c (one file inference, zero dependency) but fully functional with LLaMA 3 8B base and instruct models.",https://github.com/karpathy/llama2.c/commit/e6fd170dc9431ca600e65fcf00a0100474ba2c2b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,b3c4b6c,Andrej,2024-02-13T01:23:39Z,Merge pull request #358 from micsh/patch-1  Adding F# Port to README.md,https://github.com/karpathy/llama2.c/commit/b3c4b6c3c4bbff42e5211293280307019368ccb5
karpathy/llama2.c,https://github.com/karpathy/llama2.c,b50a757,Andrej,2024-02-13T01:21:54Z,Merge pull request #440 from neoremind/master  Add a Java port information in README.,https://github.com/karpathy/llama2.c/commit/b50a7572b3856bcfd42495bc8d6dcb421daee879
karpathy/llama2.c,https://github.com/karpathy/llama2.c,2934c3d,Andrej,2024-02-13T01:21:34Z,Merge pull request #433 from mtb0x1/master  One web page demo of all rust ports in WASM.,https://github.com/karpathy/llama2.c/commit/2934c3db7382fbc34826cec6090163c53ee562d5
karpathy/llama2.c,https://github.com/karpathy/llama2.c,e0eb8b2,Andrej,2024-02-13T01:21:08Z,Merge pull request #444 from maxbbraun/patch-1  Fix typo in runq.c comment,https://github.com/karpathy/llama2.c/commit/e0eb8b29ab2ab137c43841a15ef5a0d373f83aa4
karpathy/llama2.c,https://github.com/karpathy/llama2.c,1fb5d15,Andrej,2024-02-13T01:20:45Z,Merge pull request #446 from kyoungje/kyoungje-patch-1  Update run.ipynb to use export.py and --meta-llama parameter,https://github.com/karpathy/llama2.c/commit/1fb5d150be7b85d0b9ac35ff8265f0bd8838e909
karpathy/llama2.c,https://github.com/karpathy/llama2.c,f8a501c,Andrej,2024-02-13T01:19:41Z,Merge branch 'master' into master,https://github.com/karpathy/llama2.c/commit/f8a501c522ed6a0484ad1414bf3cae99961576b1
karpathy/llama2.c,https://github.com/karpathy/llama2.c,6c8ac23,Andrej,2024-02-13T01:17:36Z,Merge pull request #441 from coldlarry/llama2_cpp_readme  [README] add a C++ port of this project,https://github.com/karpathy/llama2.c/commit/6c8ac230506b222eb5d042ea7d1d74443120c0f7
karpathy/llama2.c,https://github.com/karpathy/llama2.c,6f65538,Andrej,2024-02-13T01:16:58Z,Merge pull request #453 from digger-yu/patch1  fix some typo,https://github.com/karpathy/llama2.c/commit/6f65538c0030849ad05c89d0c91b57e1ba6d30f2
karpathy/llama2.c,https://github.com/karpathy/llama2.c,541ca8d,Andrej,2024-02-13T01:16:10Z,Merge pull request #455 from stepango/stepango/kotlin-multiplatform-readme  README: add Kotlin Multiplatform port,https://github.com/karpathy/llama2.c/commit/541ca8df22bfacd7ec10b266af6b5bda2b2d7237
karpathy/llama2.c,https://github.com/karpathy/llama2.c,fa3e06c,Andrej,2024-02-13T01:15:11Z,"Merge pull request #463 from jettjaniak/jettjaniak-multiquery-readme  remove ""multiquery not supported"" from README",https://github.com/karpathy/llama2.c/commit/fa3e06c3ef1235cac9d7a24488fa7bd543670364
karpathy/llama2.c,https://github.com/karpathy/llama2.c,e819c97,Andrej,2024-02-13T01:14:50Z,Merge pull request #466 from mikepapadim/master  [README] add Java implemenation with GPU acceleration,https://github.com/karpathy/llama2.c/commit/e819c9793a38abb80a8b6f6c84b56e55b7db4e37
karpathy/llama2.c,https://github.com/karpathy/llama2.c,289513a,Andrej,2024-02-13T01:10:23Z,Merge pull request #473 from chris-ch/patch-2  Added Haskell version of llama2.c,https://github.com/karpathy/llama2.c/commit/289513a625eb7df426f636f82349eb36fbd9ab67
karpathy/llama2.c,https://github.com/karpathy/llama2.c,bdaf7e0,Andrej,2024-02-13T01:09:51Z,Merge pull request #477 from dvshkn/master  [README] add link to Hare port,https://github.com/karpathy/llama2.c/commit/bdaf7e078d330d69a7ac2458a9e7722068b563d4
karpathy/llama2.c,https://github.com/karpathy/llama2.c,693dab3,David Knight,2024-02-01T04:56:39Z,[README] add link to Hare port,https://github.com/karpathy/llama2.c/commit/693dab33e410772c7bb98cbb873733234c9ac294
karpathy/llama2.c,https://github.com/karpathy/llama2.c,1eb43bd,Christophe,2024-01-22T10:46:35Z,"Added Haskell version of llama2.c  I noticed there was no Haskell version, so I implemented it for the sake of completeness",https://github.com/karpathy/llama2.c/commit/1eb43bdb09c0d01b1e5be1568a7f5645f8138bfc
karpathy/llama2.c,https://github.com/karpathy/llama2.c,cf2e76a,Michalis Papadimitriou,2023-12-21T18:19:06Z,Update README.md  with TornadoVM implemenation,https://github.com/karpathy/llama2.c/commit/cf2e76af65b3f91a08f1f50105810af0babcb8dc
karpathy/llama2.c,https://github.com/karpathy/llama2.c,9ee798d,Jett,2023-12-14T11:20:00Z,"remove ""multiquery not supported"" from README",https://github.com/karpathy/llama2.c/commit/9ee798d3f978d1d92ad621bbd40c07a5c892a3b6
karpathy/llama2.c,https://github.com/karpathy/llama2.c,a4aa93d,Stepan Goncharov,2023-12-03T08:35:32Z,README: add Kotlin Multiplatform port,https://github.com/karpathy/llama2.c/commit/a4aa93d23c65e7f4cbf14ef5ac2e5b2499b674e8
karpathy/llama2.c,https://github.com/karpathy/llama2.c,2fbf705,digger yu,2023-11-28T10:09:22Z,fix some typo,https://github.com/karpathy/llama2.c/commit/2fbf7059aab6f7e44047da1ff5c0ba53057a248e
karpathy/llama2.c,https://github.com/karpathy/llama2.c,49d2992,KyoungJe Oh,2023-11-14T08:30:52Z,"Update run.ipynb to use export.py and --meta-llama parameter  Already export_meta_llama_bin.py was replaced with export.py and README.md file was updated accordingly.  But this notebook wasn't updated yet.  The Python command is updated to use the export.py file.",https://github.com/karpathy/llama2.c/commit/49d299266e718327f249d05f04d3b696ef96cca2
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c760ae6,Max Braun,2023-11-12T03:00:00Z,Fix typo in runq.c comment,https://github.com/karpathy/llama2.c/commit/c760ae6171c70c81f765cb5c688a6d6d7c2565f9
karpathy/llama2.c,https://github.com/karpathy/llama2.c,39ae035,coldlarry,2023-11-07T17:27:35Z,[README] add a C++ port of this project,https://github.com/karpathy/llama2.c/commit/39ae0359b1532403b074048dbac8624ccae7d3d6
karpathy/llama2.c,https://github.com/karpathy/llama2.c,3fd7df5,neoremind,2023-11-03T16:18:13Z,Add a Java port information in README.,https://github.com/karpathy/llama2.c/commit/3fd7df56eebc65e9b5ddfe9a226849b351840c2e
karpathy/llama2.c,https://github.com/karpathy/llama2.c,dc431ac,mtb0x1,2023-10-23T12:10:53Z,"feat(readme): add of a llama2.rs.wasm, a WASM demo in one web page",https://github.com/karpathy/llama2.c/commit/dc431ac9e237830e1d8ec24ed6c8492ca21bdf60
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d986206,Andrej,2023-10-09T19:54:35Z,Merge pull request #364 from karpathy/feature/int8_try2  int8 quantization attempt #2,https://github.com/karpathy/llama2.c/commit/d9862069e7ef665fe6309e3c17398ded2f121bf5
karpathy/llama2.c,https://github.com/karpathy/llama2.c,b233b77,Andrej Karpathy,2023-10-09T16:35:51Z,add some docs for runq,https://github.com/karpathy/llama2.c/commit/b233b77058c926ef38c41129d461aae34fb8a0c8
karpathy/llama2.c,https://github.com/karpathy/llama2.c,1f8af82,Andrej,2023-10-09T15:34:43Z,Merge branch 'master' into feature/int8_try2,https://github.com/karpathy/llama2.c/commit/1f8af821300fda21fa34681f7bbcdf83fff287d6
karpathy/llama2.c,https://github.com/karpathy/llama2.c,46ded11,Andrej,2023-10-09T15:24:30Z,Merge pull request #420 from akshaytrikha/akshay/README_bugfix  README hugginface --> huggingface,https://github.com/karpathy/llama2.c/commit/46ded11926fc9d0cf9b6677fe68012f927a6129b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,3356bca,Andrej,2023-10-09T15:24:16Z,Merge pull request #415 from adarshxs/master  Update README.md,https://github.com/karpathy/llama2.c/commit/3356bca13bdc45bf208e9ea979d7edb068bb70c2
karpathy/llama2.c,https://github.com/karpathy/llama2.c,a359947,Andrej,2023-10-09T15:20:53Z,Merge pull request #395 from pesvut/master  Support export to HuggingFace,https://github.com/karpathy/llama2.c/commit/a359947aa20fa51f59c60a2100bfa8db3a35d44c
karpathy/llama2.c,https://github.com/karpathy/llama2.c,283c9c3,Andrej,2023-10-09T15:16:12Z,Merge pull request #402 from jackpeck/master  Add link to pure OCaml port,https://github.com/karpathy/llama2.c/commit/283c9c34ccf99be9551921731b335a1d63752683
karpathy/llama2.c,https://github.com/karpathy/llama2.c,5131084,Andrej,2023-10-09T15:15:22Z,Merge pull request #398 from kroggen/patch-7  readme: add another javascript port,https://github.com/karpathy/llama2.c/commit/5131084c6fb8695f90ad8da51fbd9539bc8e93b4
karpathy/llama2.c,https://github.com/karpathy/llama2.c,1fcdf04,Andrej,2023-10-09T15:13:47Z,Merge pull request #400 from kroggen/use-kv-cache  calculate key and value inside the kv cache,https://github.com/karpathy/llama2.c/commit/1fcdf04fbb81bd073262c317129017043ba2c97b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c97befa,Akshay Trikha,2023-10-07T03:33:44Z,remove accidental linting,https://github.com/karpathy/llama2.c/commit/c97befa7d27062315657560338bf09c300378052
karpathy/llama2.c,https://github.com/karpathy/llama2.c,2752ab6,Akshay Trikha,2023-10-07T03:27:24Z,hugginface --> huggingface,https://github.com/karpathy/llama2.c/commit/2752ab69499f96aa6f3636936bd47d633fa34162
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d0237ab,Andrej,2023-10-05T22:19:39Z,Bring back legendary tag line :D  hahaha,https://github.com/karpathy/llama2.c/commit/d0237abd32e553317a2bd80ecd5d4c621ddd307a
karpathy/llama2.c,https://github.com/karpathy/llama2.c,9fdb131,Adarsh Shirawalmath,2023-10-01T05:48:33Z,Update README.md,https://github.com/karpathy/llama2.c/commit/9fdb1316c7f63c3f3629f5f737ac7f1750528704
karpathy/llama2.c,https://github.com/karpathy/llama2.c,2dedad6,Nicky Pochinkov,2023-09-21T14:38:06Z,Added support for repeated kv weights,https://github.com/karpathy/llama2.c/commit/2dedad6ceaa68bb4b5d101cb69034ec7aa1ee6c5
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d3c25b1,Nicky Pochinkov,2023-09-21T14:36:36Z,Add checks/config for tied embedding weights,https://github.com/karpathy/llama2.c/commit/d3c25b10a6d0ca89c563a192267c24c04379ba27
karpathy/llama2.c,https://github.com/karpathy/llama2.c,ffea287,Nicky Pochinkov,2023-09-16T17:46:27Z,updated comment .pt -> .bin,https://github.com/karpathy/llama2.c/commit/ffea28751614cf161e513c09e2c2fd1635115a42
karpathy/llama2.c,https://github.com/karpathy/llama2.c,a61173d,Nicky Pochinkov,2023-09-16T17:32:31Z,Added CLI dtype code,https://github.com/karpathy/llama2.c/commit/a61173d6b9dd544631a73808ffa89592ef8fa6e9
karpathy/llama2.c,https://github.com/karpathy/llama2.c,19f40a2,Nicky Pochinkov,2023-09-16T17:32:21Z,Made default hf export torch.float32,https://github.com/karpathy/llama2.c/commit/19f40a2a717e6bff858a551b7fd8776b11edcbd7
karpathy/llama2.c,https://github.com/karpathy/llama2.c,fc11cc3,Nicky Pochinkov,2023-09-16T17:10:36Z,Changed code so that lm_head and token_embed are tied,https://github.com/karpathy/llama2.c/commit/fc11cc387b47efd98ca4ac0956f715d2e5451c41
karpathy/llama2.c,https://github.com/karpathy/llama2.c,f38055d,Nicky Pochinkov,2023-09-16T13:07:48Z,add option to set dtype for export,https://github.com/karpathy/llama2.c/commit/f38055dfb637c1d50b5f1ac2999a6d54cf8fa2ca
karpathy/llama2.c,https://github.com/karpathy/llama2.c,b259fb4,jackpeck,2023-09-16T12:43:10Z,Add link to pure OCaml port,https://github.com/karpathy/llama2.c/commit/b259fb44321caa1ca1a91dbb744a54ab36ba5863
karpathy/llama2.c,https://github.com/karpathy/llama2.c,411c5bd,Bernardo Ramos,2023-09-14T07:14:45Z,reorganize variables,https://github.com/karpathy/llama2.c/commit/411c5bd2db9a87e94e1bd1a6c7b7ca117adc4b01
karpathy/llama2.c,https://github.com/karpathy/llama2.c,593d846,Bernardo Ramos,2023-09-14T01:13:08Z,use key and value from kv cache,https://github.com/karpathy/llama2.c/commit/593d846bc3d9460c66925d7d3281e67c1b2df5d1
karpathy/llama2.c,https://github.com/karpathy/llama2.c,3da6cc1,Bernardo Ramos,2023-09-13T19:09:39Z,readme: add another javascript port,https://github.com/karpathy/llama2.c/commit/3da6cc1b21fabf6ac2ac44f3798bd1b9000b59ed
karpathy/llama2.c,https://github.com/karpathy/llama2.c,bf9a116,Nicky Pochinkov,2023-09-12T18:55:28Z,Added error handling for LlamaConfig import,https://github.com/karpathy/llama2.c/commit/bf9a1162e1bd2e1d492bd637eb26fd1c04d22d79
karpathy/llama2.c,https://github.com/karpathy/llama2.c,6360a53,Nicky Pochinkov,2023-09-12T18:53:26Z,fixed whitespace,https://github.com/karpathy/llama2.c/commit/6360a539013c7f371fbca1376663825f080d6f2b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c568f69,Nicky Pochinkov,2023-09-12T18:50:59Z,added option to export to huggingface format,https://github.com/karpathy/llama2.c/commit/c568f6952dbcc6d5c19fdec1a9c76dc649f14be4
karpathy/llama2.c,https://github.com/karpathy/llama2.c,766a30b,Andrej,2023-09-12T08:18:43Z,"Merge pull request #391 from karpathy/revert-389-realtime  Revert ""Minor fix: Use CLOCK_MONOTONIC instead of CLOCK_REALTIME""",https://github.com/karpathy/llama2.c/commit/766a30bc6e9a1c69ce007bb69caabf4c6062f0e9
karpathy/llama2.c,https://github.com/karpathy/llama2.c,38c58ac,Andrej,2023-09-12T08:17:04Z,"Revert ""Minor fix: Use CLOCK_MONOTONIC instead of CLOCK_REALTIME""",https://github.com/karpathy/llama2.c/commit/38c58ac336544ba08a814dea89135b0d07fd7450
karpathy/llama2.c,https://github.com/karpathy/llama2.c,0bf2981,Andrej,2023-09-12T08:13:28Z,Merge pull request #387 from flaneur2020/add-notable-forks  chore: add another rust fork to the notable forks section,https://github.com/karpathy/llama2.c/commit/0bf29817937bea04fc4e0f0a91f2bbfbcf7986fc
karpathy/llama2.c,https://github.com/karpathy/llama2.c,27e623c,Andrej,2023-09-12T08:12:31Z,Merge pull request #388 from tairov/patch  Add link to pure Mojo implementation of project,https://github.com/karpathy/llama2.c/commit/27e623c8ae02bad9cc52145c0ef64538260c04f6
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d15577b,Andrej,2023-09-12T08:11:58Z,Merge pull request #389 from jbochi/realtime  Minor fix: Use CLOCK_MONOTONIC instead of CLOCK_REALTIME,https://github.com/karpathy/llama2.c/commit/d15577b541375fcf69cf808e8ec6a0e49d6fa60e
karpathy/llama2.c,https://github.com/karpathy/llama2.c,9d73a37,Juarez Bochi,2023-09-11T18:05:37Z,Use CLOCK_MONOTONIC instead of realtime,https://github.com/karpathy/llama2.c/commit/9d73a377fb1aee70c0b8c335ce3d06eb12ae74c8
karpathy/llama2.c,https://github.com/karpathy/llama2.c,38011d0,Aydyn Tairov,2023-09-11T11:29:25Z,Add link to pure Mojo port of project,https://github.com/karpathy/llama2.c/commit/38011d070a2824297bc14634a5dcf8b5ab30a4a2
karpathy/llama2.c,https://github.com/karpathy/llama2.c,3ac6205,Li Yazhou,2023-09-10T13:22:05Z,add another rust implementation,https://github.com/karpathy/llama2.c/commit/3ac620572e962282e4f7c77248e1d30adfe000a1
karpathy/llama2.c,https://github.com/karpathy/llama2.c,35deb5e,Andrej,2023-09-06T21:55:53Z,Merge pull request #370 from dmarcos/llama2c-web  Add llama2.c-web to the list of projects in Readme.md,https://github.com/karpathy/llama2.c/commit/35deb5e0fa55f0a257040bcf1624ed8386e63dc7
karpathy/llama2.c,https://github.com/karpathy/llama2.c,5186b50,Andrej,2023-09-05T22:36:12Z,Merge pull request #383 from atamurad/int8_refactor  int8 refactor,https://github.com/karpathy/llama2.c/commit/5186b505f318496fe1def4ccea3c08fb6667bc51
karpathy/llama2.c,https://github.com/karpathy/llama2.c,3b0a6d3,Andrej,2023-09-05T10:54:18Z,Merge pull request #380 from dfurrer/master  Remove duplicate word in comment. EOM,https://github.com/karpathy/llama2.c/commit/3b0a6d39272f11ddee1ba143a3ada7bd67a11cec
karpathy/llama2.c,https://github.com/karpathy/llama2.c,358fe99,Andrej,2023-09-05T10:54:06Z,Merge pull request #381 from rbitr/edit_clones  added fortran clone,https://github.com/karpathy/llama2.c/commit/358fe991504d17d537274cfd76487127660fdd86
karpathy/llama2.c,https://github.com/karpathy/llama2.c,0b3a5e1,Andrew,2023-09-03T21:54:54Z,added fortran clone,https://github.com/karpathy/llama2.c/commit/0b3a5e17fd30ba7382c1d3a3258bc6994fd9430a
karpathy/llama2.c,https://github.com/karpathy/llama2.c,a69ee26,Daniel Furrer,2023-09-03T20:37:10Z,Update run.c  Remove duplicate word in comments.,https://github.com/karpathy/llama2.c/commit/a69ee269c5e7c4ed06c3fc8c56b66ef22438edb3
karpathy/llama2.c,https://github.com/karpathy/llama2.c,b9fb861,Andrej,2023-09-01T17:07:12Z,Merge pull request #367 from janimo/long-multiply  Do parameter count calculations in 64 bits to not overflow in case of…,https://github.com/karpathy/llama2.c/commit/b9fb86169f56bd787bb644c62a80bbab56f8dccc
karpathy/llama2.c,https://github.com/karpathy/llama2.c,0776f86,Andrej,2023-09-01T16:51:50Z,Merge pull request #374 from SpaceCowboy850/bugfix_train_vocab  Setting an encoding for tiny_file tokenizer file.,https://github.com/karpathy/llama2.c/commit/0776f8679309111a044b16c0194e7d6cf87b0c4b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,ab19aa0,Brandon Rowlett,2023-08-30T19:54:41Z,"Setting UTF encoding, otherwise windows breaks with UnicodeEncodeError: 'charmap' codec can't encode character '\u200b' in position 971: character maps to <undefined>",https://github.com/karpathy/llama2.c/commit/ab19aa08045f0f30db4291641ece301d7cc339f3
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c5ec6e2,Jani Monoses,2023-08-29T14:47:55Z,Use long long so it works with MSVC,https://github.com/karpathy/llama2.c/commit/c5ec6e21b8659d6d3500a2af3ac1dfe7f3e19ae1
karpathy/llama2.c,https://github.com/karpathy/llama2.c,fab753d,Diego Marcos Segura,2023-08-28T21:43:04Z,Add llama2.c-web to the list of projects in README.md,https://github.com/karpathy/llama2.c/commit/fab753db3a3dfdb55a8055600bba8177d82a127d
karpathy/llama2.c,https://github.com/karpathy/llama2.c,1ebb27f,Jani Monoses,2023-08-27T09:21:11Z,Do parameter count calculations in 64 bits to not overflow in case of very large models,https://github.com/karpathy/llama2.c/commit/1ebb27f090e10117964f1fa54a0be32d10a5a6e1
karpathy/llama2.c,https://github.com/karpathy/llama2.c,6e52df9,atamyrat,2023-08-27T05:18:03Z,properly handle token embeddings & shared classifier wcls,https://github.com/karpathy/llama2.c/commit/6e52df9b41439c4008dd816621b1bffd428792ad
karpathy/llama2.c,https://github.com/karpathy/llama2.c,06175b9,atamyrat,2023-08-27T03:47:03Z,free() quantizedtensors,https://github.com/karpathy/llama2.c/commit/06175b946b5cc82c09152bb32f9efe113a3aa7cb
karpathy/llama2.c,https://github.com/karpathy/llama2.c,7325bab,Andrej,2023-08-27T03:11:04Z,Merge pull request #365 from atamurad/patch-1  Update README.md - unclosed code block quotes,https://github.com/karpathy/llama2.c/commit/7325bab657406c427e7c1ca6575bace9a5982744
karpathy/llama2.c,https://github.com/karpathy/llama2.c,f850a97,atamyrat,2023-08-27T03:05:20Z,draft refactor to use QuantizedTensor in function arguments,https://github.com/karpathy/llama2.c/commit/f850a97c6ad94b15b8d41fadfa9aa196d615306c
karpathy/llama2.c,https://github.com/karpathy/llama2.c,37157bc,Atamurad Hezretkuliyev,2023-08-26T23:27:47Z,Update README.md  Fixed unclosed code block quotes,https://github.com/karpathy/llama2.c/commit/37157bc0a380994749b5325fd653fe21ec9439c2
karpathy/llama2.c,https://github.com/karpathy/llama2.c,df80471,Andrej Karpathy,2023-08-26T22:28:08Z,draft of int8 attempt number two,https://github.com/karpathy/llama2.c/commit/df804719146351141e1c870a0befbf07425e505d
karpathy/llama2.c,https://github.com/karpathy/llama2.c,f4b8a81,Andrej Karpathy,2023-08-26T21:22:28Z,Merge branch 'master' of github.com:karpathy/llama2.c,https://github.com/karpathy/llama2.c/commit/f4b8a817422f1bc4a8aadb1dd9d1045e42b679bc
karpathy/llama2.c,https://github.com/karpathy/llama2.c,91d57db,Andrej Karpathy,2023-08-26T21:22:19Z,add note on code llama being a bit wrong,https://github.com/karpathy/llama2.c/commit/91d57db9251d67c54caf483baebeb82e3d5de2e6
karpathy/llama2.c,https://github.com/karpathy/llama2.c,f856539,Andrej,2023-08-26T21:13:20Z,Merge pull request #363 from byte-6174/patch-1  fix tinyllamas url,https://github.com/karpathy/llama2.c/commit/f856539f412c6f0ec711b1e48b83e6158e6660a8
karpathy/llama2.c,https://github.com/karpathy/llama2.c,b5a0b65,byte-6174,2023-08-26T21:05:21Z,fix tinyllamas url,https://github.com/karpathy/llama2.c/commit/b5a0b65dbf5513f33473a072982eafe2a2705a26
karpathy/llama2.c,https://github.com/karpathy/llama2.c,7b0017c,Andrej,2023-08-26T21:03:31Z,Merge pull request #362 from byte-6174/upmaster  freeing tokenizer in test.c,https://github.com/karpathy/llama2.c/commit/7b0017c6cdb15fede07d60e1c2e877473131be88
karpathy/llama2.c,https://github.com/karpathy/llama2.c,50832e3,Andrej Karpathy,2023-08-26T21:02:23Z,move script into the new docs folder,https://github.com/karpathy/llama2.c/commit/50832e3dff94ebc37059c112810e8bcc0e37b8a4
karpathy/llama2.c,https://github.com/karpathy/llama2.c,1386edf,Andrej Karpathy,2023-08-26T20:52:49Z,add docs on stories260K,https://github.com/karpathy/llama2.c/commit/1386edfd9090e2ef1da6636f3ba3220362079024
karpathy/llama2.c,https://github.com/karpathy/llama2.c,32cecbf,Aniket,2023-08-26T20:35:50Z,freeing tokenizer in test.c,https://github.com/karpathy/llama2.c/commit/32cecbfe4adc5bcb60435c3baa5dcd84df6320e7
karpathy/llama2.c,https://github.com/karpathy/llama2.c,e47bacd,Andrej,2023-08-26T20:24:55Z,Merge pull request #355 from janimo/export-vocab-size  Export vocab size and Code Llama usage docs,https://github.com/karpathy/llama2.c/commit/e47bacdc62e1b1336188b6edd533624aaa04d07c
karpathy/llama2.c,https://github.com/karpathy/llama2.c,604d3c5,Jani Monoses,2023-08-26T19:36:09Z,Add Code Llama info,https://github.com/karpathy/llama2.c/commit/604d3c59c0caa8f73ef63045b24583717ff81d50
karpathy/llama2.c,https://github.com/karpathy/llama2.c,2c2b284,Jani Monoses,2023-08-26T19:35:55Z,Get vocab_size from token embeddings size,https://github.com/karpathy/llama2.c/commit/2c2b28498860c1dda49a473a34a60ebd693a58bb
karpathy/llama2.c,https://github.com/karpathy/llama2.c,7ec4f6a,Michael Shalala,2023-08-26T05:50:55Z,Update README.md - Adding F# Port  An F# port. Uses vectorization and parallelism and is really fast.,https://github.com/karpathy/llama2.c/commit/7ec4f6a1dfef15cc1c1f73050a467f3175c9791a
karpathy/llama2.c,https://github.com/karpathy/llama2.c,49daf18,Andrej,2023-08-25T15:00:11Z,Merge pull request #343 from karpathy/feature/chat  Add interactive loop to enable nice chat with a Llama 2 Chat model,https://github.com/karpathy/llama2.c/commit/49daf18f2f85cab239e80b8a452a2f78f295032f
karpathy/llama2.c,https://github.com/karpathy/llama2.c,4a7a62b,Andrej,2023-08-25T14:58:33Z,Merge branch 'master' into feature/chat,https://github.com/karpathy/llama2.c/commit/4a7a62bd21c6b06006f756368669e7b0b6a2e137
karpathy/llama2.c,https://github.com/karpathy/llama2.c,5c6427e,Andrej,2023-08-25T14:55:54Z,Merge pull request #352 from dmarcos/readmeTypo  Fix typo in README.md,https://github.com/karpathy/llama2.c/commit/5c6427e4d7a64e44f93b627ef7ce2037b4610360
karpathy/llama2.c,https://github.com/karpathy/llama2.c,cbc2488,Andrej,2023-08-25T14:55:26Z,Merge pull request #353 from photomz/master  Clearer WandB log step,https://github.com/karpathy/llama2.c/commit/cbc2488b8218cb63f0f63b15401435737cc1e32e
karpathy/llama2.c,https://github.com/karpathy/llama2.c,fbe324f,Andrej Karpathy,2023-08-25T14:54:05Z,adjust things a bit,https://github.com/karpathy/llama2.c/commit/fbe324fc5ab61eaab3b8f74694be5b3870d3d5ee
karpathy/llama2.c,https://github.com/karpathy/llama2.c,6def77d,Markus Zhang,2023-08-25T09:12:29Z,Correct WandB log step,https://github.com/karpathy/llama2.c/commit/6def77d4baef39b9878643001f473e49af405c7c
karpathy/llama2.c,https://github.com/karpathy/llama2.c,19cfbec,Diego Marcos Segura,2023-08-25T02:45:23Z,Fix typo in README.md,https://github.com/karpathy/llama2.c/commit/19cfbeca71f8d65d3b19a38e4dec9f4e37a731cf
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d7cd986,Andrej,2023-08-24T16:04:52Z,add todo item to add a PyTorch Engine,https://github.com/karpathy/llama2.c/commit/d7cd98633dcc50c9e58f4b39b105fe9f9494cf85
karpathy/llama2.c,https://github.com/karpathy/llama2.c,3d787b2,Andrej Karpathy,2023-08-24T04:31:06Z,"ok getting closer, and manually verified correctness of the schema matching python. still some weirdness in the printing to chase down, and also have to tune the buffer lengths and make them sensible and such",https://github.com/karpathy/llama2.c/commit/3d787b24635a7031f933ed42afc58e8117ee4504
karpathy/llama2.c,https://github.com/karpathy/llama2.c,40fb902,Andrej Karpathy,2023-08-24T03:33:44Z,fix chat format bug i think,https://github.com/karpathy/llama2.c/commit/40fb902cf07084d43a78b45b31d977c5e3659dea
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c7a2626,Andrej Karpathy,2023-08-24T03:10:18Z,Merge branch 'master' of github.com:karpathy/llama2.c,https://github.com/karpathy/llama2.c/commit/c7a26264a233c32f396b1c67be4ac019d2d8a659
karpathy/llama2.c,https://github.com/karpathy/llama2.c,446c1c0,Andrej Karpathy,2023-08-24T03:10:07Z,Merge branch 'janimo-train-vocab-python',https://github.com/karpathy/llama2.c/commit/446c1c0df34362d97e0ed3d7613dc88be574577d
karpathy/llama2.c,https://github.com/karpathy/llama2.c,096325b,Andrej Karpathy,2023-08-24T03:09:55Z,bring back num_threads,https://github.com/karpathy/llama2.c/commit/096325b66c2ab84095bd407cbab84d731edc65bc
karpathy/llama2.c,https://github.com/karpathy/llama2.c,90104db,Andrej,2023-08-24T02:57:01Z,Merge pull request #348 from nehzata/clip_steps  Clip steps maximum value,https://github.com/karpathy/llama2.c/commit/90104db7219b99061604f479c16154873835921d
karpathy/llama2.c,https://github.com/karpathy/llama2.c,9bc72ac,Ali Nehzat,2023-08-23T23:09:16Z,steps shouldn't exceed the model's seq_len either,https://github.com/karpathy/llama2.c/commit/9bc72acab041f471b655a763206a24513e8313d9
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c5e0e7f,Andrej Karpathy,2023-08-23T16:27:48Z,"attempt at chat function, but it was 8AM and I didn't have coffee yet. Seems to work but it's probably subtly broken or too complex. version 1 only, lots of hard-coded non-sensical buffer sizes. Have to go to work now",https://github.com/karpathy/llama2.c/commit/c5e0e7fce4f90f810450a278316dc9fc96298d25
karpathy/llama2.c,https://github.com/karpathy/llama2.c,fe9b9f2,Jani Monoses,2023-08-23T14:28:14Z,Train vocab in Python,https://github.com/karpathy/llama2.c/commit/fe9b9f2f15eda96507837f1b2584e98401a61930
karpathy/llama2.c,https://github.com/karpathy/llama2.c,7ac65cb,Andrej Karpathy,2023-08-23T01:08:31Z,make decode safer and fix issue with skipping bad byte tokens,https://github.com/karpathy/llama2.c/commit/7ac65cb2c2b169050747be92011b7bebdd1b4544
karpathy/llama2.c,https://github.com/karpathy/llama2.c,4b3e660,Andrej Karpathy,2023-08-23T00:26:47Z,lol text,https://github.com/karpathy/llama2.c/commit/4b3e66021a4ec050548f1d4daab5210f521d655b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d1eb18b,Andrej Karpathy,2023-08-23T00:08:22Z,"add BOS and EOS function to the Tokenizer as we start to converge closer to the Llama 2 code from Meta, and as we're about to add the Chat capability",https://github.com/karpathy/llama2.c/commit/d1eb18b8ecd4794a55f6ad8469d72f00c98a305f
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d26a499,Andrej Karpathy,2023-08-22T03:22:56Z,"absorb our rng state into the Sampler. I feel that this is correct because it makes our use of entropy very explicit and localized, and the sampler is now well-contained without any global state. Code is increasingly more beautiful.",https://github.com/karpathy/llama2.c/commit/d26a49920791e34a8cb1927ea1a46ff290683831
karpathy/llama2.c,https://github.com/karpathy/llama2.c,ac6cf8d,Andrej Karpathy,2023-08-22T02:48:51Z,tweak todo list,https://github.com/karpathy/llama2.c/commit/ac6cf8d6e81d1154163ef3b9ec0022d123f4d135
karpathy/llama2.c,https://github.com/karpathy/llama2.c,ad7a1ef,Andrej Karpathy,2023-08-22T02:32:21Z,clean up swiglu a little bit,https://github.com/karpathy/llama2.c/commit/ad7a1ef52547263debd8074ea30a4e3b550abe28
karpathy/llama2.c,https://github.com/karpathy/llama2.c,0e362f7,Andrej Karpathy,2023-08-22T02:22:36Z,and finallygit add run.c split off the generate function. alongside it will come a chat function. we are close,https://github.com/karpathy/llama2.c/commit/0e362f735f097a7c8306b67b56cc41c57ef9a091
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d73b917,Andrej Karpathy,2023-08-22T02:17:51Z,"hide temperature and topp into the sampler, it's a little bit less flexible but a little bit more cleaner",https://github.com/karpathy/llama2.c/commit/d73b917d3ba7a1d933c65b1f33ecb58fd8d78a92
karpathy/llama2.c,https://github.com/karpathy/llama2.c,379f083,Andrej Karpathy,2023-08-22T01:56:51Z,make sorted vocab a buffer of Tokenizer,https://github.com/karpathy/llama2.c/commit/379f083b85740ccabe80b61259e7346ad27ede3d
karpathy/llama2.c,https://github.com/karpathy/llama2.c,5eaca53,Andrej,2023-08-22T01:16:07Z,Merge pull request #335 from ozabluda/ozabluda-patch-5  Remove unneeded check of free(NULL),https://github.com/karpathy/llama2.c/commit/5eaca535cd8b725a6eef9c890c3b37d96258f4ae
karpathy/llama2.c,https://github.com/karpathy/llama2.c,83287ff,Andrej Karpathy,2023-08-22T01:15:00Z,fix steps=0 is max context,https://github.com/karpathy/llama2.c/commit/83287ff25456a2816986e12b1002e9d83802a981
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c2834c8,Oleg Zabluda,2023-08-21T17:54:53Z,Remove unneeded check of free(NULL)   Passing NULL to free() is totally allowed,https://github.com/karpathy/llama2.c/commit/c2834c8a1f016394ff15e0fbe8e2a50a9c3d29f1
karpathy/llama2.c,https://github.com/karpathy/llama2.c,ee95b1b,Andrej,2023-08-21T15:26:28Z,Merge pull request #315 from davidar/vocab_source  Fix vocab_source in sample.py,https://github.com/karpathy/llama2.c/commit/ee95b1bf2943f87bc3e2c845bf07a733aca3806b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d02e0c9,Andrej Karpathy,2023-08-21T15:17:37Z,Merge branch 'rdentato-patch-check-params',https://github.com/karpathy/llama2.c/commit/d02e0c90d826ccc1268b176fdc1ce4ebefb171d0
karpathy/llama2.c,https://github.com/karpathy/llama2.c,33d94f6,Andrej Karpathy,2023-08-21T15:17:14Z,parameter validation cleanup,https://github.com/karpathy/llama2.c/commit/33d94f60a5882729b3a3b793f8492f4088bdf35a
karpathy/llama2.c,https://github.com/karpathy/llama2.c,2d972f1,Remo Dentato,2023-08-21T15:02:42Z,Merge branch 'karpathy:master' into patch-check-params,https://github.com/karpathy/llama2.c/commit/2d972f17639b3c4361407a53c5e073cd10120565
karpathy/llama2.c,https://github.com/karpathy/llama2.c,8a3ea7b,Andrej,2023-08-21T14:34:32Z,Merge pull request #329 from atamurad/import_meta  Moved export_meta_llama_bin.py to new export.py,https://github.com/karpathy/llama2.c/commit/8a3ea7b433c909878cb09fdf1c2aa07b4a6faa7d
karpathy/llama2.c,https://github.com/karpathy/llama2.c,61c26d5,atamyrat,2023-08-21T11:24:01Z,Updated README to replace export_meta_llama_bin.py script with export.py,https://github.com/karpathy/llama2.c/commit/61c26d5392010495196a1525e104e4e7cdc7aadc
karpathy/llama2.c,https://github.com/karpathy/llama2.c,36a78af,atamyrat,2023-08-21T11:19:56Z,"tested load_meta_model() in export.py, deleting old export_meta_llama_bin.py file",https://github.com/karpathy/llama2.c/commit/36a78af5e16e68117d4e1235199938f024e2226c
karpathy/llama2.c,https://github.com/karpathy/llama2.c,de00547,atamyrat,2023-08-21T11:13:47Z,Added load_meta_model() to export.py,https://github.com/karpathy/llama2.c/commit/de005474d37d0cde1356739b8c79ebe7b42b5973
karpathy/llama2.c,https://github.com/karpathy/llama2.c,4444575,rdentato,2023-08-21T06:43:39Z,Added check of generation parameters.,https://github.com/karpathy/llama2.c/commit/4444575c4e9c72f8b8e3056713956853fa8d192d
karpathy/llama2.c,https://github.com/karpathy/llama2.c,dd61b13,Andrej Karpathy,2023-08-21T05:09:06Z,"delete the save_torchscript export file, but copy its content to the new export.py for the future maybe",https://github.com/karpathy/llama2.c/commit/dd61b13e578ef237c775ad05d8280d2a836c774b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,ea44f53,Andrej Karpathy,2023-08-21T04:58:19Z,"now that the export.py HF functionality is in master, we can delete this file, and update the readme",https://github.com/karpathy/llama2.c/commit/ea44f535682658f3c586719ef52fc985240461fe
karpathy/llama2.c,https://github.com/karpathy/llama2.c,801c68f,Andrej,2023-08-21T04:53:17Z,Merge pull request #326 from atamurad/import_hf  Added huggingface model loader/importer to export.py,https://github.com/karpathy/llama2.c/commit/801c68f5a1272480ff0a1ca609d0fe2ea80e101a
karpathy/llama2.c,https://github.com/karpathy/llama2.c,74a68ee,Andrej,2023-08-21T04:50:26Z,Merge pull request #325 from HarryGifford/users/hegi/update-readme-threading  Update readme with suggestion on number of threads to use,https://github.com/karpathy/llama2.c/commit/74a68eeb351034b4e6455d262699da2b557d8e1d
karpathy/llama2.c,https://github.com/karpathy/llama2.c,288b3ce,Andrej Karpathy,2023-08-21T04:47:49Z,remove dagger in the eyeball,https://github.com/karpathy/llama2.c/commit/288b3cec09ef7ed8e7d728c9632cd6bc3d62ae1e
karpathy/llama2.c,https://github.com/karpathy/llama2.c,14275bd,Andrej Karpathy,2023-08-21T04:43:24Z,minor clean. i think a lot of chaos has been reduced for today. we shall now rest.,https://github.com/karpathy/llama2.c/commit/14275bd623df8ebb9ef8628df460db624b7940fa
karpathy/llama2.c,https://github.com/karpathy/llama2.c,3868f73,Andrej Karpathy,2023-08-21T04:23:02Z,and finally refactor the Sampler. things are starting to look a lot cleaner I think,https://github.com/karpathy/llama2.c/commit/3868f732a43aed3290dc855fddea31f6d0e43ec1
karpathy/llama2.c,https://github.com/karpathy/llama2.c,8a377a1,Andrej Karpathy,2023-08-21T03:55:12Z,"refactor the Transformer (Config, Weights, RunState) into a single object, with build and free too",https://github.com/karpathy/llama2.c/commit/8a377a1d3110875ce3d6fdeda31a86489303b12a
karpathy/llama2.c,https://github.com/karpathy/llama2.c,ae2e4f8,Andrej Karpathy,2023-08-21T03:11:54Z,name the tokenizer methods cleaner: encode and decode,https://github.com/karpathy/llama2.c/commit/ae2e4f8d88366f3c01f66d553311ff23718500ef
karpathy/llama2.c,https://github.com/karpathy/llama2.c,0dd8215,atamyrat,2023-08-21T03:07:29Z,"removed transformers from requirements.txt, added error message",https://github.com/karpathy/llama2.c/commit/0dd82158f6b058409a6001647f88942d7a89e7b2
karpathy/llama2.c,https://github.com/karpathy/llama2.c,155475a,atamyrat,2023-08-21T02:16:11Z,Fix WQ and WK permutation in huggingface models,https://github.com/karpathy/llama2.c/commit/155475a5235a3ad492015f856f57f0f7a61f8686
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d7704bd,atamyrat,2023-08-21T00:40:34Z,mark ModelArgs.hidden_dim as optional and calculate as previously if not provided,https://github.com/karpathy/llama2.c/commit/d7704bdeaa7142300edeabf179e6b1e41c637608
karpathy/llama2.c,https://github.com/karpathy/llama2.c,09db52c,atamyrat,2023-08-20T23:53:50Z,Added huggingface model loader to export.py,https://github.com/karpathy/llama2.c/commit/09db52c69ec7dd485618f899f854b8183eb0e87b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,a72b3b0,Harry Gifford,2023-08-20T22:01:33Z,Update readme with suggestion on number of threads to use  Update the documentation to make suggestions on the number of threads. The performance difference can be very large. Also linked to the PyTorch docs which are relevant here.,https://github.com/karpathy/llama2.c/commit/a72b3b0206de4a05b483bc67d3a5149cb5d2fa00
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c74456f,Andrej Karpathy,2023-08-20T18:18:23Z,"refactor step 1. the tokenizer, and all the other abstractions, are a total mess, refactoring things a bit",https://github.com/karpathy/llama2.c/commit/c74456f3f084c73a2865f758e341f4cfe5b54a87
karpathy/llama2.c,https://github.com/karpathy/llama2.c,1e335a4,Andrej Karpathy,2023-08-20T17:26:43Z,remove freq_cis fields as they are not used anymore,https://github.com/karpathy/llama2.c/commit/1e335a41cfc1b34c37aed6ff5074ae6533dd7084
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c0511de,Andrej Karpathy,2023-08-20T17:18:06Z,probindex should never have been part of RunState. i apologize for this failure of abstraction,https://github.com/karpathy/llama2.c/commit/c0511de61716325b3dadc86b39ce69c12f2d8b22
karpathy/llama2.c,https://github.com/karpathy/llama2.c,8c93c7a,Andrej,2023-08-20T17:08:32Z,"Merge pull request #322 from karpathy/feature/export  New model export (the code remains ""dead"" and legacy version is still the default behavior, so no breaking changes are introduced). The major benefit is a new export.py file, which we can use to centralize work on formatting: both imports and exports.",https://github.com/karpathy/llama2.c/commit/8c93c7a30e360164847843e7bafb6fbb6c40e9b7
karpathy/llama2.c,https://github.com/karpathy/llama2.c,13dcee4,Andrej Karpathy,2023-08-20T17:02:22Z,todos update,https://github.com/karpathy/llama2.c/commit/13dcee493a727d60a8c4053d162ef085c08c2348
karpathy/llama2.c,https://github.com/karpathy/llama2.c,f3db92a,Andrej Karpathy,2023-08-20T16:51:35Z,use out_file.tell() instead of nbytes += arithmetic,https://github.com/karpathy/llama2.c/commit/f3db92a2dc8fc928f7877f7dac4c4c5d98a9f7ff
karpathy/llama2.c,https://github.com/karpathy/llama2.c,fa8dfd8,Andrej Karpathy,2023-08-19T19:21:12Z,"isolate read_checkpoint, because i'd like to now make it support both version 0 and version 1",https://github.com/karpathy/llama2.c/commit/fa8dfd854ebdd911e43bc6238217e343d5754796
karpathy/llama2.c,https://github.com/karpathy/llama2.c,4df5e2e,Andrej Karpathy,2023-08-19T18:51:32Z,make version 1 be the legacy export but with new header. version 2 will be Q8_0 export,https://github.com/karpathy/llama2.c/commit/4df5e2e939e214855fdccfa838dd419b37347ce6
karpathy/llama2.c,https://github.com/karpathy/llama2.c,4212bd6,Andrej Karpathy,2023-08-19T18:34:49Z,oops fix double indent on quantize def,https://github.com/karpathy/llama2.c/commit/4212bd6d4343ac8a13efaced5609af268e7f4730
karpathy/llama2.c,https://github.com/karpathy/llama2.c,7f551db,Andrej Karpathy,2023-08-19T18:25:20Z,new model export: versions 0 (legacy) and 1,https://github.com/karpathy/llama2.c/commit/7f551dbfd73e9d36342fa95d88ca1833f89ca2e6
karpathy/llama2.c,https://github.com/karpathy/llama2.c,6c5d78f,Andrej,2023-08-19T17:01:08Z,Merge pull request #317 from yiminghan/yhan/old  Add a link to Dart port in README,https://github.com/karpathy/llama2.c/commit/6c5d78fa41d35e57c510181309f89b189f6a536a
karpathy/llama2.c,https://github.com/karpathy/llama2.c,db1a722,Andrej,2023-08-19T17:00:56Z,Merge pull request #318 from rahoua/master  YARP - Yet Another Rust Port in README.md,https://github.com/karpathy/llama2.c/commit/db1a7228168f1d119a1d11aea2310e9f292745e7
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d2a546c,Andrej,2023-08-19T17:00:27Z,Merge pull request #319 from RahulSChand/warning  Give better error message in Tinystories data loader,https://github.com/karpathy/llama2.c/commit/d2a546c5777633d4ff79106d6e949f541550d3db
karpathy/llama2.c,https://github.com/karpathy/llama2.c,fbefeec,rahulschand,2023-08-19T07:35:26Z,add assert message to give better warning,https://github.com/karpathy/llama2.c/commit/fbefeec1b1e215206060079e3abe7b9bf95ea548
karpathy/llama2.c,https://github.com/karpathy/llama2.c,978c311,rahoua,2023-08-18T21:58:21Z,Add pecca-rs to README.md,https://github.com/karpathy/llama2.c/commit/978c311b3078e216d670e5b57fe4fee8419c60d5
karpathy/llama2.c,https://github.com/karpathy/llama2.c,882e480,YiMing Han,2023-08-18T19:18:29Z,update read me,https://github.com/karpathy/llama2.c/commit/882e480bc0a5abfaf2958a15e126d1e60df3b8cf
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d09ebbb,YiMing Han,2023-08-18T19:14:08Z,"Revert ""working one""  This reverts commit 8607b11ea1f287c2f0fdff6c40cd915a55dcd89b.",https://github.com/karpathy/llama2.c/commit/d09ebbb32ba62e2e6594b73c2520bd382d17f58f
karpathy/llama2.c,https://github.com/karpathy/llama2.c,bc7cb7d,YiMing Han,2023-08-18T19:13:59Z,"Revert ""only dart""  This reverts commit 01df3731d6747659ad4d8cf7d9f4bcb27eb6d5f0.",https://github.com/karpathy/llama2.c/commit/bc7cb7d0e87ac7cbaa67cd51cdcc52cbfcacce32
karpathy/llama2.c,https://github.com/karpathy/llama2.c,01df373,YiMing Han,2023-08-18T19:09:24Z,only dart,https://github.com/karpathy/llama2.c/commit/01df3731d6747659ad4d8cf7d9f4bcb27eb6d5f0
karpathy/llama2.c,https://github.com/karpathy/llama2.c,8607b11,YiMing Han,2023-08-18T19:07:41Z,working one,https://github.com/karpathy/llama2.c/commit/8607b11ea1f287c2f0fdff6c40cd915a55dcd89b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,52fe365,David A Roberts,2023-08-18T08:40:25Z,Fix vocab_source in sample.py,https://github.com/karpathy/llama2.c/commit/52fe3653e59d07f88dd934a4aadad6aef4055662
karpathy/llama2.c,https://github.com/karpathy/llama2.c,bd18228,Andrej Karpathy,2023-08-17T04:13:13Z,"calculate the freq_cis online, no need to write/read them to/from checkpoints",https://github.com/karpathy/llama2.c/commit/bd182289c596fa6059eb7b3b7c8ccd04b5c90fc3
karpathy/llama2.c,https://github.com/karpathy/llama2.c,b68a6d2,Andrej,2023-08-17T03:09:32Z,Merge pull request #307 from madroidmaq/master  Jupter Notebook: Add run Meta's Llama 2 models,https://github.com/karpathy/llama2.c/commit/b68a6d2ab5c634a588a7aedcfada8ebda75e55da
karpathy/llama2.c,https://github.com/karpathy/llama2.c,57bf0e9,Andrej,2023-08-16T16:51:11Z,Merge pull request #306 from rdentato/patch-utf8-no-validation  minimal protection against invalid UTF8 encoding.,https://github.com/karpathy/llama2.c/commit/57bf0e9ee4bbd61c98c4ad204b72f2b8881ac8cd
karpathy/llama2.c,https://github.com/karpathy/llama2.c,9fbe96f,madroid,2023-08-16T12:23:27Z,Jupter Notebook: Add run Meta's Llama 2 models,https://github.com/karpathy/llama2.c/commit/9fbe96fc2e80ffa38d08229e6062e38681b36d54
karpathy/llama2.c,https://github.com/karpathy/llama2.c,55e6074,rdentato,2023-08-16T07:58:07Z,Added space to str_buffer in case max_token_length is 1.,https://github.com/karpathy/llama2.c/commit/55e60740f5c94ec37f66212864242bb6ee910065
karpathy/llama2.c,https://github.com/karpathy/llama2.c,befe486,rdentato,2023-08-16T07:42:53Z,minimal protection against invalid UTF8 encoding.,https://github.com/karpathy/llama2.c/commit/befe4867b34723d0ba95d30784a42e4f522a4057
karpathy/llama2.c,https://github.com/karpathy/llama2.c,df6557a,Andrej,2023-08-16T02:26:34Z,Merge pull request #267 from krrishnarraj/master  Update readme for openmp on mac,https://github.com/karpathy/llama2.c/commit/df6557a10d95811f6153dca6e3141f03e04b98e2
karpathy/llama2.c,https://github.com/karpathy/llama2.c,65c8993,Andrej Karpathy,2023-08-16T02:22:26Z,Merge branch 'Majdoddin-ci-tiny-model',https://github.com/karpathy/llama2.c/commit/65c899314ca63449d4edb1128363d7485bdbbcdf
karpathy/llama2.c,https://github.com/karpathy/llama2.c,62a6d69,Andrej Karpathy,2023-08-16T02:22:13Z,style changes and remove spurious runc test call at the bottom,https://github.com/karpathy/llama2.c/commit/62a6d69d86670fca162473f866011ffb617e8ba4
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d47fc41,Andrej Karpathy,2023-08-16T02:20:34Z,Merge branch 'ci-tiny-model' of https://github.com/Majdoddin/llama2.c into Majdoddin-ci-tiny-model,https://github.com/karpathy/llama2.c/commit/d47fc41b6a3652db0285cc69b1663aebe7bb1e17
karpathy/llama2.c,https://github.com/karpathy/llama2.c,ca67253,Andrej Karpathy,2023-08-15T16:09:33Z,smallfix: not sure what the point of this indirection was,https://github.com/karpathy/llama2.c/commit/ca67253f28f95b11a8d3b76a3058eccd70c2b471
karpathy/llama2.c,https://github.com/karpathy/llama2.c,4c63c56,Andrej Karpathy,2023-08-15T16:07:48Z,shorten top comment on run.c file,https://github.com/karpathy/llama2.c/commit/4c63c5608d5f567dc62aa6a76e3754e743203812
karpathy/llama2.c,https://github.com/karpathy/llama2.c,a47f9b3,Andrej Karpathy,2023-08-15T16:03:11Z,collapsing copy paste code because it's driving my ocd crazy,https://github.com/karpathy/llama2.c/commit/a47f9b3969e9f2eb4e41eb177d8c39e33d45153b
karpathy/llama2.c,https://github.com/karpathy/llama2.c,87b11ed,Ruhollah Majdoddin,2023-08-15T16:01:53Z,modifiying test_all so it can safely run on windows,https://github.com/karpathy/llama2.c/commit/87b11edf270feefd9606662e73dd2a202c5b4b7a
karpathy/llama2.c,https://github.com/karpathy/llama2.c,66c9f5e,Ruhollah Majdoddin,2023-08-15T15:58:04Z,Adding pytest with the tiny model to macOS and  windows (except amd64_arm64) runners,https://github.com/karpathy/llama2.c/commit/66c9f5e6c82b592eaee5b3e9d50de57285952e0c
karpathy/llama2.c,https://github.com/karpathy/llama2.c,88eb238,Andrej Karpathy,2023-08-15T15:57:27Z,add tests into Makefile convenience,https://github.com/karpathy/llama2.c/commit/88eb238255a44536a7d8adfadbf49e2bfa093d64
karpathy/llama2.c,https://github.com/karpathy/llama2.c,600cedb,Andrej,2023-08-15T02:54:49Z,Merge pull request #297 from karpathy/feature/utf8  Add UTF-8 support to prompts,https://github.com/karpathy/llama2.c/commit/600cedb33ded8919f0c3e9293145edb27e3d90ac
karpathy/llama2.c,https://github.com/karpathy/llama2.c,fe2de68,Andrej Karpathy,2023-08-15T02:33:01Z,fix sample.py from tokenizer changes before,https://github.com/karpathy/llama2.c/commit/fe2de68688ec35502b566fcef227a94935a3f3b7
karpathy/llama2.c,https://github.com/karpathy/llama2.c,a9a0628,Andrej Karpathy,2023-08-15T02:18:49Z,thoroughly commented the UTF-8 byte reading code,https://github.com/karpathy/llama2.c/commit/a9a0628c9254c0efcc0249cdf3d5dc0b692201a6
karpathy/llama2.c,https://github.com/karpathy/llama2.c,d459fd4,Andrej Karpathy,2023-08-15T01:42:33Z,add back careful processing of the byte tokens,https://github.com/karpathy/llama2.c/commit/d459fd4243cddf5893231cbaa70da26e598cfa53
karpathy/llama2.c,https://github.com/karpathy/llama2.c,4bf36ec,Andrej Karpathy,2023-08-15T01:04:10Z,get rid of the special byte decoding logic,https://github.com/karpathy/llama2.c/commit/4bf36ecc1792ce2ed579d6c5718fc38b5a035677
karpathy/llama2.c,https://github.com/karpathy/llama2.c,8417cb4,Andrej Karpathy,2023-08-15T00:18:53Z,Merge branch 'utf8' of https://github.com/atamurad/llama2.c into feature/utf8,https://github.com/karpathy/llama2.c/commit/8417cb438da6dccb69bd39be261e00884bb08b6c
karpathy/llama2.c,https://github.com/karpathy/llama2.c,94a3a5e,Andrej Karpathy,2023-08-14T14:52:15Z,Merge branch 'master' of github.com:karpathy/llama2.c,https://github.com/karpathy/llama2.c/commit/94a3a5e0a5f63f06ffbfa7ec5452553eedafc215
karpathy/llama2.c,https://github.com/karpathy/llama2.c,32c1ff9,Andrej Karpathy,2023-08-14T14:52:07Z,"missed p->dim to kv_dim for k,v vectors. we're not doing anything wrong we're just being wasteful with memory. thanks @xefoci7612 for pointing out",https://github.com/karpathy/llama2.c/commit/32c1ff97fbe69a4d030e0bc05b156a3733da396c
karpathy/llama2.c,https://github.com/karpathy/llama2.c,013e012,Andrej,2023-08-14T14:46:39Z,Merge pull request #286 from Nick-infinity/master  [Feat]: Add support for meta llama hf model conversion,https://github.com/karpathy/llama2.c/commit/013e012b87f8584397624037f71c84843d2e02b3
karpathy/llama2.c,https://github.com/karpathy/llama2.c,50f970d,Andrej,2023-08-14T14:41:13Z,Merge pull request #289 from chenyangMl/update_readme  Update readme to introduce llama2.c-zh,https://github.com/karpathy/llama2.c/commit/50f970d17065ca731d0d878665b3ece2f7b135ea
karpathy/llama2.c,https://github.com/karpathy/llama2.c,2a9a4c4,chenyang,2023-08-14T07:12:30Z,update readme wiht a simple line to introduce llama2.c-zh,https://github.com/karpathy/llama2.c/commit/2a9a4c4e14d548a216503dae19f58c3af284f953
karpathy/llama2.c,https://github.com/karpathy/llama2.c,79900ff,chenyang,2023-08-14T07:00:33Z,update readme wiht a simple line to introduce llama2.c-zh,https://github.com/karpathy/llama2.c/commit/79900ff68ee662ddc72e5392843ba1e4c4bf860d
karpathy/llama2.c,https://github.com/karpathy/llama2.c,eec9ad5,Krishnaraj Bhat,2023-08-14T06:32:40Z,Merge remote-tracking branch 'upstream/master',https://github.com/karpathy/llama2.c/commit/eec9ad5a5be7f7df830112c390be69afe8d58e58
karpathy/llama2.c,https://github.com/karpathy/llama2.c,82ad2ba,Andrej Karpathy,2023-08-14T05:53:57Z,remove tiktoken as dependency,https://github.com/karpathy/llama2.c/commit/82ad2ba34ead544883ac84248c2dbd98a690c0aa
karpathy/llama2.c,https://github.com/karpathy/llama2.c,c39f19f,Nikhil Gupta,2023-08-14T04:48:51Z,[Feat]: Add support for meta llama hf model conversion  Description: Llama 2 hf models have weights stored with diff name  Signed-off-by: Nikhil Gupta <nikhilg.me@gmail.com>,https://github.com/karpathy/llama2.c/commit/c39f19f1a9e32f1d33eb47fa5f674d10dd2d382f
karpathy/llama2.c,https://github.com/karpathy/llama2.c,bae0bcf,Andrej,2023-08-14T03:03:00Z,Small tweaks to Readme intro,https://github.com/karpathy/llama2.c/commit/bae0bcf484493df65097a9fdae8b6157f338bf8d
karpathy/llama2.c,https://github.com/karpathy/llama2.c,45afa91,Andrej Karpathy,2023-08-14T02:54:27Z,"the accum function has been bothering me, there is no real need to add a function here, it does something trivial and is only used twice, scrap",https://github.com/karpathy/llama2.c/commit/45afa91dca8808f4d767d132210e7093c42f004c
karpathy/llama2.c,https://github.com/karpathy/llama2.c,854c97b,Andrej Karpathy,2023-08-14T00:12:45Z,turn topp 0.9 back on by default thanks to recent PR contributions truncating before quicksort,https://github.com/karpathy/llama2.c/commit/854c97b660fc8527a979ab5cf26436a6146f2ade
